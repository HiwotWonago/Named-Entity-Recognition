{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a190cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from telethon.sync import TelegramClient\n",
    "from telethon.tl.functions.messages import GetHistoryRequest\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "api_id = 22723598      \n",
    "api_hash = 'c2647da4d95bd4983ade135744857eda'\n",
    "phone_number = '+251912356409'     \n",
    "session_name = 'ethio_telegram_scraper'\n",
    "channel_username = [\n",
    "    'ZemenExpress',\n",
    "    'nevacomputer',\n",
    "    'meneshayeofficial',\n",
    "    'ethio_brand_collection',\n",
    "    'Leyueqa'\n",
    "]\n",
    "async def scrape_channels_to_csv():\n",
    "    all_messages = []\n",
    "\n",
    "    async with TelegramClient(session_name, api_id, api_hash) as client:\n",
    "        await client.start(phone=phone_number)\n",
    "\n",
    "        for username in channel_username:\n",
    "            try:\n",
    "                entity = await client.get_entity(username)\n",
    "                history = await client(GetHistoryRequest(\n",
    "                    peer=entity,\n",
    "                    limit=2000,\n",
    "                    offset_date=None,\n",
    "                    offset_id=0,\n",
    "                    max_id=0,\n",
    "                    min_id=0,\n",
    "                    add_offset=0,\n",
    "                    hash=0\n",
    "                ))\n",
    "\n",
    "                for message in history.messages:\n",
    "                    if message.message:\n",
    "                        all_messages.append({\n",
    "                            'channel': username,\n",
    "                            'message_id': message.id,\n",
    "                            'text': message.message,\n",
    "                            'date': str(message.date),\n",
    "                            'sender_id': message.from_id.user_id if message.from_id else None\n",
    "                        })\n",
    "\n",
    "                print(f\"✅ Scraped from {username}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to scrape {username}: {e}\")\n",
    "   \n",
    "    df = pd.DataFrame(all_messages)\n",
    "    df.to_csv('all_telegram_messages.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\" All messages saved to all_telegram_messages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d1f6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempt 1 at connecting failed: TimeoutError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped from ZemenExpress\n",
      "✅ Scraped from nevacomputer\n",
      "✅ Scraped from meneshayeofficial\n",
      "✅ Scraped from ethio_brand_collection\n",
      "✅ Scraped from Leyueqa\n",
      " All messages saved to all_telegram_messages.csv\n"
     ]
    }
   ],
   "source": [
    "await scrape_channels_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f08352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Hiwi\\\\Documents\\\\week4\\\\all_telegram_messages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6323d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  💥💥...................................💥💥\\n\\n📌Sa...   \n",
      "1  💥💥...................................💥💥\\n\\n3pc...   \n",
      "2  💥💥...................................💥💥\\n\\n3pc...   \n",
      "3  💥💥...................................💥💥\\n\\n📌1 ...   \n",
      "4  💥💥...................................💥💥\\n\\n📌1 ...   \n",
      "\n",
      "                                     cleaned_message  \\\n",
      "0  ................................... Saachi Ele...   \n",
      "1  ................................... 3pcs Bottl...   \n",
      "2  ................................... 3pcs Bottl...   \n",
      "3  ................................... 1 pairs Sn...   \n",
      "4  ................................... 1 pairs Sn...   \n",
      "\n",
      "                                 tokenized_sentences  \\\n",
      "0  [ዋጋ ብር ውስን ፍሬ ነው ያለው አድራሻ መገናኛመሰረትደፋርሞልሁለተኛፎቅ ...   \n",
      "1  [በማንኛውም ጠርሙስ ጫፍ የሚገጠም ለዘይት እና መሰል ነገሮች መቀነሻ የሚ...   \n",
      "2  [በማንኛውም ጠርሙስ ጫፍ የሚገጠም ለዘይት እና መሰል ነገሮች መቀነሻ የሚ...   \n",
      "3  [ዋጋ ብር ውስን ፍሬ ነው ያለው አድራሻ መገናኛመሰረትደፋርሞልሁለተኛፎቅ ...   \n",
      "4  [ዋጋ ብር ውስን ፍሬ ነው ያለው አድራሻ መገናኛመሰረትደፋርሞልሁለተኛፎቅ ...   \n",
      "\n",
      "                                         word_tokens  \\\n",
      "0  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...   \n",
      "1  [በማንኛውም, ጠርሙስ, ጫፍ, የሚገጠም, ለዘይት, እና, መሰል, ነገሮች,...   \n",
      "2  [በማንኛውም, ጠርሙስ, ጫፍ, የሚገጠም, ለዘይት, እና, መሰል, ነገሮች,...   \n",
      "3  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...   \n",
      "4  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...   \n",
      "\n",
      "                          location_normalized_tokens  \n",
      "0  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...  \n",
      "1  [በማንኛውም, ጠርሙስ, ጫፍ, የሚገጠም, ለዘይት, እና, መሰል, ነገሮች,...  \n",
      "2  [በማንኛውም, ጠርሙስ, ጫፍ, የሚገጠም, ለዘይት, እና, መሰል, ነገሮች,...  \n",
      "3  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...  \n",
      "4  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from etnltk import Amharic\n",
    "import unicodedata\n",
    "from etnltk.tokenize.am import sent_tokenize\n",
    "from etnltk.tokenize.am import word_tokenize\n",
    "# === Abbreviation and Location Mapping ===\n",
    "LOCATION_MAPPING = {\n",
    "    'addis ababa': ('አዲስ', 'አበባ'), 'አዲስ አበባ': ('አዲስ', 'አበባ'),\n",
    "    'bahir dar': ('ባህር', 'ዳር'), 'ባህር ዳር': ('ባህር', 'ዳር'),\n",
    "    'dire dawa': ('ድሬ', 'ዳዋ'), 'ድሬ ዳዋ': ('ድሬ', 'ዳዋ'),\n",
    "    'አዳማ': ('አዳማ',), 'ጅማ': ('ጅማ',), 'መቀሌ': ('መቀሌ',), 'ሀዋሳ': ('ሀዋሳ',),\n",
    "    'ቦሌ': ('ቦሌ',), 'ሰበር ምድር': ('ሰበር', 'ምድር'), 'አቃቂ': ('አቃቂ',),\n",
    "    'ጎንደር': ('ጎንደር',), 'ደሴ': ('ደሴ',), 'አርባ ምንጭ': ('አርባ', 'ምንጭ'),\n",
    "    'ነቀምት': ('ነቀምት',), 'ጂንማ': ('ጂንማ',)\n",
    "}\n",
    "\n",
    "ABBREVIATION_MAP = {\n",
    "    'ቢሮ': 'ቢሮዎች', 'ም/ቤት': 'ምክር ቤት', 'ወ/ሮ': 'ወይዘሮ', 'ወ/ሪት': 'ወይዘሪት',\n",
    "    'ኪ.ሜ': 'ኪሎ ሜትር', 'ቁ.ቁ': 'ቁጥር ቁጥር', 'ቁ': 'ቁጥር', 'ሜ': 'ሜትር',\n",
    "    'አ.አ': 'አዲስ አበባ', 'ሺ': 'ሺህ', 'ኪ.ግ': 'ኪሎ ግራም'\n",
    "}\n",
    "#  cleaning\n",
    "def clean_amharic_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # skip non-string entries\n",
    "    text = re.sub(r'[^\\w\\s\\u1200-\\u137F.,!?፡።፣]', '', text)\n",
    "    replacements = {\n",
    "        '::': '።',\n",
    "        ':': '፡',\n",
    "        ',': '፣'\n",
    "    }\n",
    "    for k, v in replacements.items():\n",
    "        text = text.replace(k, v)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    return text\n",
    "# Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub('', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "# 2. Tokenize Amharic sentences\n",
    "def tokenize_amharic(text):\n",
    "    return sent_tokenize(text)\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\Hiwi\\\\Documents\\\\week4\\\\all_telegram_messages.csv')  \n",
    "\n",
    "# Clean the 'message' column\n",
    "df['cleaned_message'] = df['text'].apply(clean_amharic_text)\n",
    "\n",
    "# Optional: Save the cleaned data to a new file\n",
    "df.to_csv('data/cleaned_telegram_ecommerce.csv', index=False)\n",
    "\n",
    "# 5. Apply tokenization\n",
    "df['tokenized_sentences'] = df['cleaned_message'].apply(tokenize_amharic)\n",
    "def expand_abbreviations(tokens):\n",
    "    return [ABBREVIATION_MAP.get(token, token) for token in tokens]\n",
    "\n",
    "# === Step 5: Normalize Location Names ===\n",
    "def normalize_locations(tokens):\n",
    "    normalized = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        found = False\n",
    "        for length in range(3, 0, -1):\n",
    "            if i + length <= len(tokens):\n",
    "                phrase = ' '.join(tokens[i:i+length]).lower()\n",
    "                if phrase in LOCATION_MAPPING:\n",
    "                    normalized.extend(LOCATION_MAPPING[phrase])\n",
    "                    i += length\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            normalized.append(tokens[i])\n",
    "            i += 1\n",
    "    return normalized\n",
    "\n",
    "# === Process Tokens ===\n",
    "df['word_tokens'] = df['cleaned_message'].apply(word_tokenize)\n",
    "df['expanded_tokens'] = df['word_tokens'].apply(expand_abbreviations)\n",
    "df['location_normalized_tokens'] = df['expanded_tokens'].apply(normalize_locations)\n",
    "\n",
    "# === Save Processed Data ===\n",
    "df.to_csv('data/processed_telegram_ecommerce.csv', index=False)\n",
    "\n",
    "\n",
    "# 7. Preview\n",
    "print(df[['text', 'cleaned_message', 'tokenized_sentences', 'word_tokens', 'location_normalized_tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8dfae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
