{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9851c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from telethon.sync import TelegramClient\n",
    "from telethon.tl.functions.messages import GetHistoryRequest\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "api_id = 22723598      \n",
    "api_hash = 'c2647da4d95bd4983ade135744857eda'\n",
    "phone_number = '+251912356409'     \n",
    "session_name = 'ethio_telegram_scraper'\n",
    "channel_username = [\n",
    "    'ZemenExpress',\n",
    "    'nevacomputer',\n",
    "    'meneshayeofficial',\n",
    "    'ethio_brand_collection',\n",
    "    'Leyueqa'\n",
    "]\n",
    "async def scrape_channels_to_csv():\n",
    "    all_messages = []\n",
    "\n",
    "    async with TelegramClient(session_name, api_id, api_hash) as client:\n",
    "        await client.start(phone=phone_number)\n",
    "\n",
    "        for username in channel_username:\n",
    "            try:\n",
    "                entity = await client.get_entity(username)\n",
    "                history = await client(GetHistoryRequest(\n",
    "                    peer=entity,\n",
    "                    limit=2000,\n",
    "                    offset_date=None,\n",
    "                    offset_id=0,\n",
    "                    max_id=0,\n",
    "                    min_id=0,\n",
    "                    add_offset=0,\n",
    "                    hash=0\n",
    "                ))\n",
    "\n",
    "                for message in history.messages:\n",
    "                    if message.message:\n",
    "                        all_messages.append({\n",
    "                            'channel': username,\n",
    "                            'message_id': message.id,\n",
    "                            'text': message.message,\n",
    "                            'date': str(message.date),\n",
    "                            'sender_id': message.from_id.user_id if message.from_id else None\n",
    "                        })\n",
    "\n",
    "                print(f\"✅ Scraped from {username}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to scrape {username}: {e}\")\n",
    "   \n",
    "    df = pd.DataFrame(all_messages)\n",
    "    df.to_csv('all_telegram_messages.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\" All messages saved to all_telegram_messages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8577e95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped from ZemenExpress\n",
      "✅ Scraped from nevacomputer\n",
      "✅ Scraped from meneshayeofficial\n",
      "✅ Scraped from ethio_brand_collection\n",
      "✅ Scraped from Leyueqa\n",
      " All messages saved to all_telegram_messages.csv\n"
     ]
    }
   ],
   "source": [
    "await scrape_channels_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8416c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Hiwi\\\\Documents\\\\week4\\\\all_telegram_messages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521a3e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  💥💥...................................💥💥\\n\\n📌Sa...   \n",
      "1  💥💥...................................💥💥\\n\\n3pc...   \n",
      "2  💥💥...................................💥💥\\n\\n3pc...   \n",
      "3  💥💥...................................💥💥\\n\\n📌1 ...   \n",
      "4  💥💥...................................💥💥\\n\\n📌1 ...   \n",
      "\n",
      "                                     cleaned_message  \\\n",
      "0  ................................... Saachi Ele...   \n",
      "1  ................................... 3pcs Bottl...   \n",
      "2  ................................... 3pcs Bottl...   \n",
      "3  ................................... 1 pairs Sn...   \n",
      "4  ................................... 1 pairs Sn...   \n",
      "\n",
      "                                 tokenized_sentences  \\\n",
      "0  [ዋጋ ብር ውስን ፍሬ ነው ያለው አድራሻ መገናኛመሰረትደፋርሞልሁለተኛፎቅ ...   \n",
      "1  [በማንኛውም ጠርሙስ ጫፍ የሚገጠም ለዘይት እና መሰል ነገሮች መቀነሻ የሚ...   \n",
      "2  [በማንኛውም ጠርሙስ ጫፍ የሚገጠም ለዘይት እና መሰል ነገሮች መቀነሻ የሚ...   \n",
      "3  [ዋጋ ብር ውስን ፍሬ ነው ያለው አድራሻ መገናኛመሰረትደፋርሞልሁለተኛፎቅ ...   \n",
      "4  [ዋጋ ብር ውስን ፍሬ ነው ያለው አድራሻ መገናኛመሰረትደፋርሞልሁለተኛፎቅ ...   \n",
      "\n",
      "                                         word_tokens  \\\n",
      "0  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...   \n",
      "1  [በማንኛውም, ጠርሙስ, ጫፍ, የሚገጠም, ለዘይት, እና, መሰል, ነገሮች,...   \n",
      "2  [በማንኛውም, ጠርሙስ, ጫፍ, የሚገጠም, ለዘይት, እና, መሰል, ነገሮች,...   \n",
      "3  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...   \n",
      "4  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...   \n",
      "\n",
      "                          location_normalized_tokens  \n",
      "0  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...  \n",
      "1  [በማንኛውም, ጠርሙስ, ጫፍ, የሚገጠም, ለዘይት, እና, መሰል, ነገሮች,...  \n",
      "2  [በማንኛውም, ጠርሙስ, ጫፍ, የሚገጠም, ለዘይት, እና, መሰል, ነገሮች,...  \n",
      "3  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...  \n",
      "4  [ዋጋ, ብር, ውስን, ፍሬ, ነው, ያለው, አድራሻ, መገናኛ, መሰረት, ደ...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from etnltk import Amharic\n",
    "import unicodedata\n",
    "from etnltk.tokenize.am import sent_tokenize\n",
    "from etnltk.tokenize.am import word_tokenize\n",
    "# === Abbreviation and Location Mapping ===\n",
    "LOCATION_MAPPING = {\n",
    "    'addis ababa': ('አዲስ', 'አበባ'), 'አዲስ አበባ': ('አዲስ', 'አበባ'),\n",
    "    'bahir dar': ('ባህር', 'ዳር'), 'ባህር ዳር': ('ባህር', 'ዳር'),\n",
    "    'dire dawa': ('ድሬ', 'ዳዋ'), 'ድሬ ዳዋ': ('ድሬ', 'ዳዋ'),\n",
    "    'አዳማ': ('አዳማ',), 'ጅማ': ('ጅማ',), 'መቀሌ': ('መቀሌ',), 'ሀዋሳ': ('ሀዋሳ',),\n",
    "    'ቦሌ': ('ቦሌ',), 'ሰበር ምድር': ('ሰበር', 'ምድር'), 'አቃቂ': ('አቃቂ',),\n",
    "    'ጎንደር': ('ጎንደር',), 'ደሴ': ('ደሴ',), 'አርባ ምንጭ': ('አርባ', 'ምንጭ'),\n",
    "    'ነቀምት': ('ነቀምት',), 'ጂንማ': ('ጂንማ',)\n",
    "}\n",
    "\n",
    "ABBREVIATION_MAP = {\n",
    "    'ቢሮ': 'ቢሮዎች', 'ም/ቤት': 'ምክር ቤት', 'ወ/ሮ': 'ወይዘሮ', 'ወ/ሪት': 'ወይዘሪት',\n",
    "    'ኪ.ሜ': 'ኪሎ ሜትር', 'ቁ.ቁ': 'ቁጥር ቁጥር', 'ቁ': 'ቁጥር', 'ሜ': 'ሜትር',\n",
    "    'አ.አ': 'አዲስ አበባ', 'ሺ': 'ሺህ', 'ኪ.ግ': 'ኪሎ ግራም'\n",
    "}\n",
    "#  cleaning\n",
    "def clean_amharic_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # skip non-string entries\n",
    "    text = re.sub(r'[^\\w\\s\\u1200-\\u137F.,!?፡።፣]', '', text)\n",
    "    replacements = {\n",
    "        '::': '።',\n",
    "        ':': '፡',\n",
    "        ',': '፣'\n",
    "    }\n",
    "    for k, v in replacements.items():\n",
    "        text = text.replace(k, v)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    return text\n",
    "# Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub('', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "# 2. Tokenize Amharic sentences\n",
    "def tokenize_amharic(text):\n",
    "    return sent_tokenize(text)\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\Hiwi\\\\Documents\\\\week4\\\\all_telegram_messages.csv')  \n",
    "\n",
    "# Clean the 'message' column\n",
    "df['cleaned_message'] = df['text'].apply(clean_amharic_text)\n",
    "\n",
    "# Optional: Save the cleaned data to a new file\n",
    "df.to_csv('data/cleaned_telegram_ecommerce.csv', index=False)\n",
    "\n",
    "# 5. Apply tokenization\n",
    "df['tokenized_sentences'] = df['cleaned_message'].apply(tokenize_amharic)\n",
    "def expand_abbreviations(tokens):\n",
    "    return [ABBREVIATION_MAP.get(token, token) for token in tokens]\n",
    "\n",
    "# === Step 5: Normalize Location Names ===\n",
    "def normalize_locations(tokens):\n",
    "    normalized = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        found = False\n",
    "        for length in range(3, 0, -1):\n",
    "            if i + length <= len(tokens):\n",
    "                phrase = ' '.join(tokens[i:i+length]).lower()\n",
    "                if phrase in LOCATION_MAPPING:\n",
    "                    normalized.extend(LOCATION_MAPPING[phrase])\n",
    "                    i += length\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            normalized.append(tokens[i])\n",
    "            i += 1\n",
    "    return normalized\n",
    "\n",
    "# === Process Tokens ===\n",
    "df['word_tokens'] = df['cleaned_message'].apply(word_tokenize)\n",
    "df['expanded_tokens'] = df['word_tokens'].apply(expand_abbreviations)\n",
    "df['location_normalized_tokens'] = df['expanded_tokens'].apply(normalize_locations)\n",
    "\n",
    "# === Save Processed Data ===\n",
    "df.to_csv('data/processed_telegram_ecommerce.csv', index=False)\n",
    "\n",
    "\n",
    "# 7. Preview\n",
    "print(df[['text', 'cleaned_message', 'tokenized_sentences', 'word_tokens', 'location_normalized_tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82087fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing message 1/10\n",
      "\n",
      "================================================================================\n",
      "Original Message: 💥💥...................................💥💥\n",
      "\n",
      "📌Saachi Electric Kettle\n",
      "\n",
      "👍Borosilicate Glass Body\n",
      "👍Overheat protection\n",
      "👍Automatic switch off\n",
      "👍2200w\n",
      "\n",
      "ዋጋ፦  💲🏷 2700  ብር ✅\n",
      "\n",
      "♦️ውስን ፍሬ ነው ያለው🔥🔥🔥\n",
      "\n",
      "🏢 አድራሻ👉\n",
      "\n",
      "📍♦️#መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ. S05/S06\n",
      "\n",
      "\n",
      "     💧💧💧💧\n",
      "\n",
      "\n",
      "    📲 0902660722\n",
      "    📲 0928460606 \n",
      "\n",
      "🔖\n",
      "💬በTelegram ለማዘዝ ⤵️ ይጠቀሙ🔽\n",
      "\n",
      "@zemencallcenter \n",
      "\n",
      "@zemenexpressadmin\n",
      "\n",
      "ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን⤵️\n",
      "https://telegram.me/zemenexpress\n",
      "\n",
      "💥💥...................................💥💥\n",
      "--------------------------------------------------------------------------------\n",
      "Tokens: ................................... Saachi Electric Kettle Borosilicate Glass Body Overheat protection Automatic switch off 2200w ዋጋ፦ 2700 ብር ውስን ፍሬ ነው ያለው አድራሻ መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ. S05S06 0902660722 0928460606 በTelegram ለማዘዝ ይጠቀሙ zemencallcenter zemenexpressadmin ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን httpstelegram.mezemenexpress ...................................\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . .\n",
      "Current Token (1/363): '.'\n",
      "Label: 1\n",
      "Invalid label! Please use one of: B-PROD, I-PROD, B-LOC, I-LOC, B-PRICE, I-PRICE, O\n",
      "Label: B-PROD\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . .\n",
      "Current Token (2/363): '.'\n",
      "Label: I-PROD\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . . .\n",
      "Current Token (3/363): '.'\n",
      "Label: B-LOC\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . . .\n",
      "Current Token (4/363): '.'\n",
      "Label: I-LOC\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . . .\n",
      "Current Token (5/363): '.'\n",
      "Label: B-PRICE\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . . .\n",
      "Current Token (6/363): '.'\n",
      "Label: I-PRICE\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . . .\n",
      "Current Token (7/363): '.'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load your scraped dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\Hiwi\\\\Documents\\\\week4\\\\all_telegram_messages.csv')\n",
    "\n",
    "# Define your labeling function\n",
    "def label_message(message, tokens):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Original Message: {message}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    \n",
    "    labels = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        valid_labels = ['B-PROD', 'I-PROD', 'B-LOC', 'I-LOC', 'B-PRICE', 'I-PRICE', 'O']\n",
    "        \n",
    "        # Show context tokens\n",
    "        context_start = max(0, i-2)\n",
    "        context_end = min(len(tokens), i+3)\n",
    "        context = tokens[context_start:context_end]\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(f\"Context: {' '.join(context)}\")\n",
    "        print(f\"Current Token ({i+1}/{len(tokens)}): '{token}'\")\n",
    "        \n",
    "        while True:\n",
    "            label = input(\"Label: \").strip().upper()\n",
    "            if label in valid_labels:\n",
    "                labels.append((token, label))\n",
    "                break\n",
    "            print(f\"Invalid label! Please use one of: {', '.join(valid_labels)}\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Define function to save to CoNLL format\n",
    "def save_to_conll(labeled_data, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for original_msg, annotations in labeled_data:\n",
    "            for token, label in annotations:\n",
    "                f.write(f\"{token}\\t{label}\\n\")\n",
    "            f.write(\"\\n\")  # Blank line between messages\n",
    "    print(f\"Saved {len(labeled_data)} messages to {output_path}\")\n",
    "\n",
    "# Precompute tokens for all messages\n",
    "df['tokens'] = df['text'].apply(clean_amharic_text)\n",
    "\n",
    "# Select messages to label (first 10)\n",
    "messages_to_label = df.head(10)\n",
    "\n",
    "labeled_data = []\n",
    "for idx, row in messages_to_label.iterrows():\n",
    "    message = row['text']\n",
    "    tokens = row['tokens']\n",
    "    \n",
    "    print(f\"\\nProcessing message {idx+1}/{len(messages_to_label)}\")\n",
    "    annotations = label_message(message, tokens)\n",
    "    labeled_data.append((message, annotations))\n",
    "    \n",
    "    print(f\"Message labeled! ({len(labeled_data)}/{len(messages_to_label)} done)\")\n",
    "\n",
    "print(f\"\\nSuccessfully labeled {len(labeled_data)} messages!\")\n",
    "\n",
    "# Now save to CoNLL format\n",
    "save_to_conll(labeled_data, \"my_labeled_dataset.conll\")\n",
    "\n",
    "# Verify the output\n",
    "print(\"\\nPreview of labeled data:\")\n",
    "with open(\"my_labeled_dataset.conll\", 'r', encoding='utf-8') as f:\n",
    "    print(f.read(500))  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ba325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
