{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9851c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from telethon.sync import TelegramClient\n",
    "from telethon.tl.functions.messages import GetHistoryRequest\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "api_id = 22723598      \n",
    "api_hash = 'c2647da4d95bd4983ade135744857eda'\n",
    "phone_number = '+251912356409'     \n",
    "session_name = 'ethio_telegram_scraper'\n",
    "channel_username = [\n",
    "    'ZemenExpress',\n",
    "    'nevacomputer',\n",
    "    'meneshayeofficial',\n",
    "    'ethio_brand_collection',\n",
    "    'Leyueqa'\n",
    "]\n",
    "async def scrape_channels_to_csv():\n",
    "    all_messages = []\n",
    "\n",
    "    async with TelegramClient(session_name, api_id, api_hash) as client:\n",
    "        await client.start(phone=phone_number)\n",
    "\n",
    "        for username in channel_username:\n",
    "            try:\n",
    "                entity = await client.get_entity(username)\n",
    "                history = await client(GetHistoryRequest(\n",
    "                    peer=entity,\n",
    "                    limit=2000,\n",
    "                    offset_date=None,\n",
    "                    offset_id=0,\n",
    "                    max_id=0,\n",
    "                    min_id=0,\n",
    "                    add_offset=0,\n",
    "                    hash=0\n",
    "                ))\n",
    "\n",
    "                for message in history.messages:\n",
    "                    if message.message:\n",
    "                        all_messages.append({\n",
    "                            'channel': username,\n",
    "                            'message_id': message.id,\n",
    "                            'text': message.message,\n",
    "                            'date': str(message.date),\n",
    "                            'sender_id': message.from_id.user_id if message.from_id else None\n",
    "                        })\n",
    "\n",
    "                print(f\"âœ… Scraped from {username}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Failed to scrape {username}: {e}\")\n",
    "   \n",
    "    df = pd.DataFrame(all_messages)\n",
    "    df.to_csv('all_telegram_messages.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\" All messages saved to all_telegram_messages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8577e95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scraped from ZemenExpress\n",
      "âœ… Scraped from nevacomputer\n",
      "âœ… Scraped from meneshayeofficial\n",
      "âœ… Scraped from ethio_brand_collection\n",
      "âœ… Scraped from Leyueqa\n",
      " All messages saved to all_telegram_messages.csv\n"
     ]
    }
   ],
   "source": [
    "await scrape_channels_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8416c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Hiwi\\\\Documents\\\\week4\\\\all_telegram_messages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521a3e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥\\n\\nğŸ“ŒSa...   \n",
      "1  ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥\\n\\n3pc...   \n",
      "2  ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥\\n\\n3pc...   \n",
      "3  ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥\\n\\nğŸ“Œ1 ...   \n",
      "4  ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥\\n\\nğŸ“Œ1 ...   \n",
      "\n",
      "                                     cleaned_message  \\\n",
      "0  ................................... Saachi Ele...   \n",
      "1  ................................... 3pcs Bottl...   \n",
      "2  ................................... 3pcs Bottl...   \n",
      "3  ................................... 1 pairs Sn...   \n",
      "4  ................................... 1 pairs Sn...   \n",
      "\n",
      "                                 tokenized_sentences  \\\n",
      "0  [á‹‹áŒ‹ á‰¥áˆ­ á‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹ áŠ á‹µáˆ«áˆ» áˆ˜áŒˆáŠ“áŠ›áˆ˜áˆ°áˆ¨á‰µá‹°á‹áˆ­áˆáˆáˆáˆˆá‰°áŠ›áá‰… ...   \n",
      "1  [á‰ áˆ›áŠ•áŠ›á‹áˆ áŒ áˆ­áˆ™áˆµ áŒ«á á‹¨áˆšáŒˆáŒ áˆ áˆˆá‹˜á‹­á‰µ áŠ¥áŠ“ áˆ˜áˆ°áˆ áŠáŒˆáˆ®á‰½ áˆ˜á‰€áŠáˆ» á‹¨áˆš...   \n",
      "2  [á‰ áˆ›áŠ•áŠ›á‹áˆ áŒ áˆ­áˆ™áˆµ áŒ«á á‹¨áˆšáŒˆáŒ áˆ áˆˆá‹˜á‹­á‰µ áŠ¥áŠ“ áˆ˜áˆ°áˆ áŠáŒˆáˆ®á‰½ áˆ˜á‰€áŠáˆ» á‹¨áˆš...   \n",
      "3  [á‹‹áŒ‹ á‰¥áˆ­ á‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹ áŠ á‹µáˆ«áˆ» áˆ˜áŒˆáŠ“áŠ›áˆ˜áˆ°áˆ¨á‰µá‹°á‹áˆ­áˆáˆáˆáˆˆá‰°áŠ›áá‰… ...   \n",
      "4  [á‹‹áŒ‹ á‰¥áˆ­ á‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹ áŠ á‹µáˆ«áˆ» áˆ˜áŒˆáŠ“áŠ›áˆ˜áˆ°áˆ¨á‰µá‹°á‹áˆ­áˆáˆáˆáˆˆá‰°áŠ›áá‰… ...   \n",
      "\n",
      "                                         word_tokens  \\\n",
      "0  [á‹‹áŒ‹, á‰¥áˆ­, á‹áˆµáŠ•, ááˆ¬, áŠá‹, á‹«áˆˆá‹, áŠ á‹µáˆ«áˆ», áˆ˜áŒˆáŠ“áŠ›, áˆ˜áˆ°áˆ¨á‰µ, á‹°...   \n",
      "1  [á‰ áˆ›áŠ•áŠ›á‹áˆ, áŒ áˆ­áˆ™áˆµ, áŒ«á, á‹¨áˆšáŒˆáŒ áˆ, áˆˆá‹˜á‹­á‰µ, áŠ¥áŠ“, áˆ˜áˆ°áˆ, áŠáŒˆáˆ®á‰½,...   \n",
      "2  [á‰ áˆ›áŠ•áŠ›á‹áˆ, áŒ áˆ­áˆ™áˆµ, áŒ«á, á‹¨áˆšáŒˆáŒ áˆ, áˆˆá‹˜á‹­á‰µ, áŠ¥áŠ“, áˆ˜áˆ°áˆ, áŠáŒˆáˆ®á‰½,...   \n",
      "3  [á‹‹áŒ‹, á‰¥áˆ­, á‹áˆµáŠ•, ááˆ¬, áŠá‹, á‹«áˆˆá‹, áŠ á‹µáˆ«áˆ», áˆ˜áŒˆáŠ“áŠ›, áˆ˜áˆ°áˆ¨á‰µ, á‹°...   \n",
      "4  [á‹‹áŒ‹, á‰¥áˆ­, á‹áˆµáŠ•, ááˆ¬, áŠá‹, á‹«áˆˆá‹, áŠ á‹µáˆ«áˆ», áˆ˜áŒˆáŠ“áŠ›, áˆ˜áˆ°áˆ¨á‰µ, á‹°...   \n",
      "\n",
      "                          location_normalized_tokens  \n",
      "0  [á‹‹áŒ‹, á‰¥áˆ­, á‹áˆµáŠ•, ááˆ¬, áŠá‹, á‹«áˆˆá‹, áŠ á‹µáˆ«áˆ», áˆ˜áŒˆáŠ“áŠ›, áˆ˜áˆ°áˆ¨á‰µ, á‹°...  \n",
      "1  [á‰ áˆ›áŠ•áŠ›á‹áˆ, áŒ áˆ­áˆ™áˆµ, áŒ«á, á‹¨áˆšáŒˆáŒ áˆ, áˆˆá‹˜á‹­á‰µ, áŠ¥áŠ“, áˆ˜áˆ°áˆ, áŠáŒˆáˆ®á‰½,...  \n",
      "2  [á‰ áˆ›áŠ•áŠ›á‹áˆ, áŒ áˆ­áˆ™áˆµ, áŒ«á, á‹¨áˆšáŒˆáŒ áˆ, áˆˆá‹˜á‹­á‰µ, áŠ¥áŠ“, áˆ˜áˆ°áˆ, áŠáŒˆáˆ®á‰½,...  \n",
      "3  [á‹‹áŒ‹, á‰¥áˆ­, á‹áˆµáŠ•, ááˆ¬, áŠá‹, á‹«áˆˆá‹, áŠ á‹µáˆ«áˆ», áˆ˜áŒˆáŠ“áŠ›, áˆ˜áˆ°áˆ¨á‰µ, á‹°...  \n",
      "4  [á‹‹áŒ‹, á‰¥áˆ­, á‹áˆµáŠ•, ááˆ¬, áŠá‹, á‹«áˆˆá‹, áŠ á‹µáˆ«áˆ», áˆ˜áŒˆáŠ“áŠ›, áˆ˜áˆ°áˆ¨á‰µ, á‹°...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from etnltk import Amharic\n",
    "import unicodedata\n",
    "from etnltk.tokenize.am import sent_tokenize\n",
    "from etnltk.tokenize.am import word_tokenize\n",
    "# === Abbreviation and Location Mapping ===\n",
    "LOCATION_MAPPING = {\n",
    "    'addis ababa': ('áŠ á‹²áˆµ', 'áŠ á‰ á‰£'), 'áŠ á‹²áˆµ áŠ á‰ á‰£': ('áŠ á‹²áˆµ', 'áŠ á‰ á‰£'),\n",
    "    'bahir dar': ('á‰£áˆ…áˆ­', 'á‹³áˆ­'), 'á‰£áˆ…áˆ­ á‹³áˆ­': ('á‰£áˆ…áˆ­', 'á‹³áˆ­'),\n",
    "    'dire dawa': ('á‹µáˆ¬', 'á‹³á‹‹'), 'á‹µáˆ¬ á‹³á‹‹': ('á‹µáˆ¬', 'á‹³á‹‹'),\n",
    "    'áŠ á‹³áˆ›': ('áŠ á‹³áˆ›',), 'áŒ…áˆ›': ('áŒ…áˆ›',), 'áˆ˜á‰€áˆŒ': ('áˆ˜á‰€áˆŒ',), 'áˆ€á‹‹áˆ³': ('áˆ€á‹‹áˆ³',),\n",
    "    'á‰¦áˆŒ': ('á‰¦áˆŒ',), 'áˆ°á‰ áˆ­ áˆá‹µáˆ­': ('áˆ°á‰ áˆ­', 'áˆá‹µáˆ­'), 'áŠ á‰ƒá‰‚': ('áŠ á‰ƒá‰‚',),\n",
    "    'áŒáŠ•á‹°áˆ­': ('áŒáŠ•á‹°áˆ­',), 'á‹°áˆ´': ('á‹°áˆ´',), 'áŠ áˆ­á‰£ áˆáŠ•áŒ­': ('áŠ áˆ­á‰£', 'áˆáŠ•áŒ­'),\n",
    "    'áŠá‰€áˆá‰µ': ('áŠá‰€áˆá‰µ',), 'áŒ‚áŠ•áˆ›': ('áŒ‚áŠ•áˆ›',)\n",
    "}\n",
    "\n",
    "ABBREVIATION_MAP = {\n",
    "    'á‰¢áˆ®': 'á‰¢áˆ®á‹á‰½', 'áˆ/á‰¤á‰µ': 'áˆáŠ­áˆ­ á‰¤á‰µ', 'á‹ˆ/áˆ®': 'á‹ˆá‹­á‹˜áˆ®', 'á‹ˆ/áˆªá‰µ': 'á‹ˆá‹­á‹˜áˆªá‰µ',\n",
    "    'áŠª.áˆœ': 'áŠªáˆ áˆœá‰µáˆ­', 'á‰.á‰': 'á‰áŒ¥áˆ­ á‰áŒ¥áˆ­', 'á‰': 'á‰áŒ¥áˆ­', 'áˆœ': 'áˆœá‰µáˆ­',\n",
    "    'áŠ .áŠ ': 'áŠ á‹²áˆµ áŠ á‰ á‰£', 'áˆº': 'áˆºáˆ…', 'áŠª.áŒ': 'áŠªáˆ áŒáˆ«áˆ'\n",
    "}\n",
    "#  cleaning\n",
    "def clean_amharic_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # skip non-string entries\n",
    "    text = re.sub(r'[^\\w\\s\\u1200-\\u137F.,!?á¡á¢á£]', '', text)\n",
    "    replacements = {\n",
    "        '::': 'á¢',\n",
    "        ':': 'á¡',\n",
    "        ',': 'á£'\n",
    "    }\n",
    "    for k, v in replacements.items():\n",
    "        text = text.replace(k, v)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    return text\n",
    "# Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub('', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "# 2. Tokenize Amharic sentences\n",
    "def tokenize_amharic(text):\n",
    "    return sent_tokenize(text)\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\Hiwi\\\\Documents\\\\week4\\\\all_telegram_messages.csv')  \n",
    "\n",
    "# Clean the 'message' column\n",
    "df['cleaned_message'] = df['text'].apply(clean_amharic_text)\n",
    "\n",
    "# Optional: Save the cleaned data to a new file\n",
    "df.to_csv('data/cleaned_telegram_ecommerce.csv', index=False)\n",
    "\n",
    "# 5. Apply tokenization\n",
    "df['tokenized_sentences'] = df['cleaned_message'].apply(tokenize_amharic)\n",
    "def expand_abbreviations(tokens):\n",
    "    return [ABBREVIATION_MAP.get(token, token) for token in tokens]\n",
    "\n",
    "# === Step 5: Normalize Location Names ===\n",
    "def normalize_locations(tokens):\n",
    "    normalized = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        found = False\n",
    "        for length in range(3, 0, -1):\n",
    "            if i + length <= len(tokens):\n",
    "                phrase = ' '.join(tokens[i:i+length]).lower()\n",
    "                if phrase in LOCATION_MAPPING:\n",
    "                    normalized.extend(LOCATION_MAPPING[phrase])\n",
    "                    i += length\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            normalized.append(tokens[i])\n",
    "            i += 1\n",
    "    return normalized\n",
    "\n",
    "# === Process Tokens ===\n",
    "df['word_tokens'] = df['cleaned_message'].apply(word_tokenize)\n",
    "df['expanded_tokens'] = df['word_tokens'].apply(expand_abbreviations)\n",
    "df['location_normalized_tokens'] = df['expanded_tokens'].apply(normalize_locations)\n",
    "\n",
    "# === Save Processed Data ===\n",
    "df.to_csv('data/processed_telegram_ecommerce.csv', index=False)\n",
    "\n",
    "\n",
    "# 7. Preview\n",
    "print(df[['text', 'cleaned_message', 'tokenized_sentences', 'word_tokens', 'location_normalized_tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82087fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing message 1/10\n",
      "\n",
      "================================================================================\n",
      "Original Message: ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥\n",
      "\n",
      "ğŸ“ŒSaachi Electric Kettle\n",
      "\n",
      "ğŸ‘Borosilicate Glass Body\n",
      "ğŸ‘Overheat protection\n",
      "ğŸ‘Automatic switch off\n",
      "ğŸ‘2200w\n",
      "\n",
      "á‹‹áŒ‹á¦Â  ğŸ’²ğŸ· 2700Â  á‰¥áˆ­ âœ…\n",
      "\n",
      "â™¦ï¸á‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹ğŸ”¥ğŸ”¥ğŸ”¥\n",
      "\n",
      "ğŸ¢ áŠ á‹µáˆ«áˆ»ğŸ‘‰\n",
      "\n",
      "ğŸ“â™¦ï¸#áˆ˜áŒˆáŠ“áŠ›_áˆ˜áˆ°áˆ¨á‰µ_á‹°á‹áˆ­_áˆáˆ_áˆáˆˆá‰°áŠ›_áá‰… á‰¢áˆ® á‰. S05/S06\n",
      "\n",
      "\n",
      "Â Â Â Â  ğŸ’§ğŸ’§ğŸ’§ğŸ’§\n",
      "\n",
      "\n",
      "Â Â Â  ğŸ“² 0902660722\n",
      "Â Â Â  ğŸ“² 0928460606 \n",
      "\n",
      "ğŸ”–\n",
      "ğŸ’¬á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™ğŸ”½\n",
      "\n",
      "@zemencallcenter \n",
      "\n",
      "@zemenexpressadmin\n",
      "\n",
      "áˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\n",
      "https://telegram.me/zemenexpress\n",
      "\n",
      "ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥\n",
      "--------------------------------------------------------------------------------\n",
      "Tokens: ................................... Saachi Electric Kettle Borosilicate Glass Body Overheat protection Automatic switch off 2200w á‹‹áŒ‹á¦ 2700 á‰¥áˆ­ á‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹ áŠ á‹µáˆ«áˆ» áˆ˜áŒˆáŠ“áŠ›_áˆ˜áˆ°áˆ¨á‰µ_á‹°á‹áˆ­_áˆáˆ_áˆáˆˆá‰°áŠ›_áá‰… á‰¢áˆ® á‰. S05S06 0902660722 0928460606 á‰ Telegram áˆˆáˆ›á‹˜á‹ á‹­áŒ á‰€áˆ™ zemencallcenter zemenexpressadmin áˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ• httpstelegram.mezemenexpress ...................................\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . .\n",
      "Current Token (1/363): '.'\n",
      "Label: 1\n",
      "Invalid label! Please use one of: B-PROD, I-PROD, B-LOC, I-LOC, B-PRICE, I-PRICE, O\n",
      "Label: B-PROD\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . .\n",
      "Current Token (2/363): '.'\n",
      "Label: I-PROD\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . . .\n",
      "Current Token (3/363): '.'\n",
      "Label: B-LOC\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . . .\n",
      "Current Token (4/363): '.'\n",
      "Label: I-LOC\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . . .\n",
      "Current Token (5/363): '.'\n",
      "Label: B-PRICE\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . . .\n",
      "Current Token (6/363): '.'\n",
      "Label: I-PRICE\n",
      "\n",
      "----------------------------------------\n",
      "Context: . . . . .\n",
      "Current Token (7/363): '.'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load your scraped dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\Hiwi\\\\Documents\\\\week4\\\\all_telegram_messages.csv')\n",
    "\n",
    "# Define your labeling function\n",
    "def label_message(message, tokens):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Original Message: {message}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    \n",
    "    labels = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        valid_labels = ['B-PROD', 'I-PROD', 'B-LOC', 'I-LOC', 'B-PRICE', 'I-PRICE', 'O']\n",
    "        \n",
    "        # Show context tokens\n",
    "        context_start = max(0, i-2)\n",
    "        context_end = min(len(tokens), i+3)\n",
    "        context = tokens[context_start:context_end]\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(f\"Context: {' '.join(context)}\")\n",
    "        print(f\"Current Token ({i+1}/{len(tokens)}): '{token}'\")\n",
    "        \n",
    "        while True:\n",
    "            label = input(\"Label: \").strip().upper()\n",
    "            if label in valid_labels:\n",
    "                labels.append((token, label))\n",
    "                break\n",
    "            print(f\"Invalid label! Please use one of: {', '.join(valid_labels)}\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Define function to save to CoNLL format\n",
    "def save_to_conll(labeled_data, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for original_msg, annotations in labeled_data:\n",
    "            for token, label in annotations:\n",
    "                f.write(f\"{token}\\t{label}\\n\")\n",
    "            f.write(\"\\n\")  # Blank line between messages\n",
    "    print(f\"Saved {len(labeled_data)} messages to {output_path}\")\n",
    "\n",
    "# Precompute tokens for all messages\n",
    "df['tokens'] = df['text'].apply(clean_amharic_text)\n",
    "\n",
    "# Select messages to label (first 10)\n",
    "messages_to_label = df.head(10)\n",
    "\n",
    "labeled_data = []\n",
    "for idx, row in messages_to_label.iterrows():\n",
    "    message = row['text']\n",
    "    tokens = row['tokens']\n",
    "    \n",
    "    print(f\"\\nProcessing message {idx+1}/{len(messages_to_label)}\")\n",
    "    annotations = label_message(message, tokens)\n",
    "    labeled_data.append((message, annotations))\n",
    "    \n",
    "    print(f\"Message labeled! ({len(labeled_data)}/{len(messages_to_label)} done)\")\n",
    "\n",
    "print(f\"\\nSuccessfully labeled {len(labeled_data)} messages!\")\n",
    "\n",
    "# Now save to CoNLL format\n",
    "save_to_conll(labeled_data, \"my_labeled_dataset.conll\")\n",
    "\n",
    "# Verify the output\n",
    "print(\"\\nPreview of labeled data:\")\n",
    "with open(\"my_labeled_dataset.conll\", 'r', encoding='utf-8') as f:\n",
    "    print(f.read(500))  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ba325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
