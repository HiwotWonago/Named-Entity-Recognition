{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2bb7a60b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fcdc9bc91b8d4b948e9fbad59e9659e0",
            "0e3b0aa2a6894a65be17a1c9811cf30e",
            "fd284182e5574efd8f410af41d7cd387",
            "f33a357ecb35467c9fa8786c424582dd",
            "b334701b399e43ac98aa351263dbacb6",
            "f5fdabd6a7db48a485685a15a7436d9d",
            "542b2756643542b5b51e2b21ebe4d221",
            "12cdb7e7ac0f4915b5c011cb570bcce4",
            "4218efc7380144e3a8fecae5393ee826",
            "c4123f1df26f4932abf0e6a56576772b",
            "51cb96964c1340ab924ec917b6746059",
            "edfc735f61e34d26a42a7bfa328e74a4",
            "63838ad43fe74f7da98e99bef47546a3",
            "750bf9a4529e4ef68d445a1d129a77ee",
            "91c52d1d027146a2bf57f98009f3db7a",
            "63cf621ac8dc4587981de19ff5a1bd7f",
            "96769e222cb4490785bfa3933b4b9914",
            "984df071857f4365b941b9c6ca48db58",
            "f0001f9a475f49f39155b3305efeb3e8",
            "0f6b8882e7214c69983c613248d27737",
            "cd8c7f14869f4f8d995901a3c98c9a2a",
            "8c6d97d69c724d04836a24aa99bc46af",
            "6e42590392184fccae2f1431853772b8",
            "18b52a54714c4dbe93efcb9e1694cad2",
            "f6519450ded74d87b18d1f1f46f2f5ce",
            "34d5fa1fc6414a198bd91cfa1e42dde2",
            "fa5b9dff6d8e419a8783a364b516d7d9",
            "48ce8986b9ea4d0489f203d9391bdacf",
            "1a63c352228946abb479644748580e8f",
            "bddc8086f63b4fa6a62e75c2a8ead3e8",
            "39d7199de4f545208b47f607499840cc",
            "5fc88ffe5f1844b0b4514f181ac83c20",
            "86993524491d43dbb5c3cdec06b3b24e",
            "2f1dc8a81ffa46bf9f9da70a0085a91c",
            "5a347bc745334cec90a01b59982ada0e",
            "3f677e8282734916bde6e7beee46b8a9",
            "77b47502d0484783a7964a0a1b4d8ca1",
            "d0b007bced5041d691f5da7193b445f8",
            "f37ae52d09db47b39a5cfee85c148339",
            "4b426030df404abea7872e18ed51cffd",
            "7f7e1eae5d9b4d32a65266acf36c7e2a",
            "57f9c0f904b141998bc09c1faf82bc54",
            "cb01ea5c0fbc4558acbf97c76c32db33",
            "960f0389c47a496981639b21fbfa7937",
            "f69612e4a0464b04a3f53dd7ef925f39",
            "06a94c3741494e4bb8d924bc57ba688a",
            "04a510530cc24178a96956edb188b3cd",
            "a80af5e85ec54549a06743dc5a79d1ed",
            "4b6a71a1404c4c0ab3b11600758e6674",
            "146b3b2e9b40404bbc72faf42bf7a5ca",
            "88f6cbd3c4d545f2bc0eee84c620ffad",
            "274be5e670aa4ab58de0bc77abcab4b4",
            "fadf03a8b98a41828b983bc610a2ca56",
            "14fadc1dea834019b94eec2945dd708a",
            "aa17dabd869e49199ef306d204924708"
          ]
        },
        "id": "2bb7a60b",
        "outputId": "da85ac5d-97ca-4f9e-b070-b94a4f9a0c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "üîÑ Fine-tuning model: xlm-roberta-base\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcdc9bc91b8d4b948e9fbad59e9659e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-22-1885062086.py:123: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 03:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.942791</td>\n",
              "      <td>0.048780</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.146341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.926512</td>\n",
              "      <td>0.048780</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.219512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.917551</td>\n",
              "      <td>0.048780</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.243902</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved to ./final_model_xlm-roberta-base\n",
            "\n",
            "\n",
            "üîÑ Fine-tuning model: bert-base-multilingual-cased\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edfc735f61e34d26a42a7bfa328e74a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e42590392184fccae2f1431853772b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-22-1885062086.py:123: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 02:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.788669</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.292683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.732797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.341463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.712209</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>0.390244</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved to ./final_model_bert-base-multilingual-cased\n",
            "\n",
            "\n",
            "üîÑ Fine-tuning model: distilbert-base-multilingual-cased\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f1dc8a81ffa46bf9f9da70a0085a91c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f69612e4a0464b04a3f53dd7ef925f39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-22-1885062086.py:123: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 02:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.835467</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.793375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.341463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.775781</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.341463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved to ./final_model_distilbert-base-multilingual-cased\n",
            "\n",
            "üìä Model Comparison Summary:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'f1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-22-1885062086.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüìä Model Comparison Summary:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7187\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7189\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7191\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'f1'"
          ]
        }
      ],
      "source": [
        "# üìò Fine-Tune & Compare Multilingual Models for Amharic NER\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
        "                          TrainingArguments, Trainer)\n",
        "from sklearn.metrics import classification_report\n",
        "import evaluate\n",
        "\n",
        "# ‚úÖ Define models to compare\n",
        "model_names = [\n",
        "    \"xlm-roberta-base\",\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    \"distilbert-base-multilingual-cased\",\n",
        "]\n",
        "\n",
        "# ‚úÖ Load CoNLL-formatted dataset\n",
        "\n",
        "def load_conll_data(filepath):\n",
        "    sentences, labels = [], []\n",
        "    with open(filepath, encoding='utf-8') as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence, label = [], []\n",
        "            else:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 2:\n",
        "                    token, tag = parts\n",
        "                    sentence.append(token)\n",
        "                    label.append(tag)\n",
        "    if sentence:  # Append last sentence if file doesn't end with newline\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Use the new dataset file path\n",
        "sentences, tags = load_conll_data(\"/content/ethiomart_ner_dataset.conll\")\n",
        "\n",
        "label_list = sorted(list(set(tag for tag_seq in tags for tag in tag_seq)))\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# ‚úÖ Tokenize and align labels\n",
        "\n",
        "def tokenize_and_align_labels(tokenizer, sentences, tags):\n",
        "    tokenized_inputs = tokenizer(sentences, is_split_into_words=True, truncation=True, padding=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(tags):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label2id[label[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return dict(tokenized_inputs)\n",
        "\n",
        "# ‚úÖ Convert to Hugging Face Dataset\n",
        "raw_dataset = Dataset.from_pandas(pd.DataFrame({\"tokens\": sentences, \"ner_tags\": tags}))\n",
        "\n",
        "# ‚úÖ Evaluation metric\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    true_predictions = [[id2label[p] for (p, l) in zip(pred, lab) if l != -100] for pred, lab in zip(predictions, labels)]\n",
        "    true_labels = [[id2label[l] for (p, l) in zip(pred, lab) if l != -100] for pred, lab in zip(predictions, labels)]\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"]\n",
        "    }\n",
        "\n",
        "# ‚úÖ Loop through models and train/evaluate\n",
        "results_summary = []\n",
        "\n",
        "for model_name in model_names:\n",
        "    print(f\"\\n\\nüîÑ Fine-tuning model: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenized_dataset = raw_dataset.map(lambda x: tokenize_and_align_labels(tokenizer, x[\"tokens\"], x[\"ner_tags\"]), batched=True)\n",
        "\n",
        "    if len(tokenized_dataset) == 0:\n",
        "        print(f\"‚ö†Ô∏è Skipping {model_name} ‚Äî dataset mapping returned 0 samples.\")\n",
        "        continue\n",
        "\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=len(label_list),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_{model_name.split('/')[-1]}\",\n",
        "        eval_strategy=\"epoch\", # Changed from evaluation_strategy\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        eval_dataset=tokenized_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    metrics = trainer.evaluate()\n",
        "    metrics[\"model\"] = model_name\n",
        "    metrics[\"train_time_sec\"] = training_time\n",
        "    results_summary.append(metrics)\n",
        "\n",
        "    # ‚úÖ Save the trained model for the current iteration\n",
        "    output_dir = f\"./final_model_{model_name.split('/')[-1]}\"\n",
        "    trainer.save_model(output_dir)\n",
        "    print(f\"\\nModel saved to {output_dir}\")\n",
        "\n",
        "\n",
        "# ‚úÖ Results Summary Table\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "print(\"\\nüìä Model Comparison Summary:\")\n",
        "print(results_df.sort_values(\"f1\", ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cWgoxEFkSVnW"
      },
      "id": "cWgoxEFkSVnW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08229876",
        "outputId": "be5075a4-8044-42ed-e24c-0496ea53836c"
      },
      "source": [
        "!pip install evaluate"
      ],
      "id": "08229876",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "4f70298c",
        "outputId": "ac846eea-87c9-4cbe-d513-c997a249ff82"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "id": "4f70298c",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-39ded82e-f1c6-4e4c-aedf-7c5110b6ca31\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-39ded82e-f1c6-4e4c-aedf-7c5110b6ca31\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving amharic_ner_conll.txt to amharic_ner_conll.txt\n",
            "User uploaded file \"amharic_ner_conll.txt\" with length 10915 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "385ecbeb",
        "outputId": "c9bfd26c-e152-4a19-9ac0-5e13f1c37ffc"
      },
      "source": [
        "print(\"First 5 sentences and tags loaded from file:\")\n",
        "for i in range(min(5, len(sentences))):\n",
        "    print(\"Sentence:\", sentences[i])\n",
        "    print(\"Tags:\", tags[i])\n",
        "\n",
        "print(\"\\nLabel list:\", label_list)\n",
        "print(\"Label2id mapping:\", label2id)"
      ],
      "id": "385ecbeb",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 sentences and tags loaded from file:\n",
            "\n",
            "Label list: []\n",
            "Label2id mapping: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98a209b3",
        "outputId": "1f85bbdf-e5a5-42f5-f11d-77d970fe80bb"
      },
      "source": [
        "# ‚úÖ Save the last trained model\n",
        "# The trainer saves checkpoints during training, but this explicitly saves the final model state\n",
        "if trainer is not None:\n",
        "    output_dir = f\"./final_model_{model_name.split('/')[-1]}\"\n",
        "    trainer.save_model(output_dir)\n",
        "    print(f\"\\nModel saved to {output_dir}\")\n",
        "\n",
        "    # You can then download the saved model files from this directory"
      ],
      "id": "98a209b3",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved to ./final_model_distilbert-base-multilingual-cased\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c50fc83b",
        "outputId": "f150817a-7ee8-4ecd-eed2-97f89a05a0ba"
      },
      "source": [
        "# Read and print the content of the uploaded file to understand its format\n",
        "with open(\"/content/amharic_ner_conll.txt\", 'r', encoding='utf-8') as f:\n",
        "    file_content = f.read()\n",
        "print(file_content)"
      ],
      "id": "c50fc83b",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "·â±·äï ·â†·ä†·äï·ãµ ·àã·ã≠ ·àù·äï·å£·çç ·àò·å•·à®·åä·ã´ ·ä•·äì ·àò·àµ·â≥·ãà·âµ ·àõ·çÖ·åÉ ·â† ·àù·äï·å£·çç ·àò·å•·à®·åä·ã´ ·ä•·äì ·àò·àµ·â≥·ãà·âµ ·àõ·çÖ·åÉ ·àÅ·àà·â±·äï ·â†·ä†·äï·ãµ ·àã·ã≠ ·àò·à∞·â≥·ãà·âµ ·àõ·çÖ·åÉ ·àà·ä•·äï·ä®·äï ·ä†·àç·â£ ·çÖ·ã≥·âµ ·ã´·àà ·ä§·àå·ä≠·âµ·à™·ä≠ ·çà·à≠ ·àã·àà·ãç·àù ·àà·àå·àà·ãç·àù ·ã®·àö·àÜ·äï ·ã∞·âÉ·âÖ ·âÜ·àª·àª·ãé·âΩ·äï ·à≥·ã´·àµ·âÄ·à≠ ·àç·âÖ ·àù ·ä†·ãµ·à≠·åé ·ã®·àö·ã´·çÄ·ã≥ ·ã®·â∞·à®·åã·åà·å† ·ã® ·ä†·äï·ãµ ·ãì·àò·âµ ·ãã·àµ·âµ·äì ·ä•·äï·à∞·å£·àà·äï ·àÅ·àà·â±·äï ·â†·ä†·äï·ãµ ·àã·ã≠ ·ä†·ãµ·à´·àª ·åâ·à≠·ãµ ·àæ·àã ·àÜ·àä ·à≤·â≤ ·à¥·äï·â∞·à≠ ·äõ ·çé·âÖ ·çí·ã´·à≥ ·ä´·äï·âµ·à™ ·â≥·ãà·à≠ ·äõ ·çé·âÖ O\n",
            "·åÄ·àû ·ã∞·àä·äì ·àÖ·äï·çÉ ·àù·ãµ·à≠ ·àã·ã≠ ·ãç·àç·äì ·àõ·àµ·à®·åÉ ·ã´·àà·â†·âµ ·àÖ·äï·çÉ ·åà·à≠·åÇ ·àò·â•·à´·âµ ·àÉ·ã≠·àç ·çã·ãç·äï·â¥·äï ·àÖ·äï·çÉ ·çä·âµ ·àà·çä·âµ ·åà·àã·åç·àå O\n",
            "\n",
            "·ã®·àÇ·à≥·â• ·àä·âÖ ·àò·äê·àª ·å•·âÖ·àç ·ã®·ä†·àà·àù ·âã·äï·âã ·ä®·âª·àâ ·ä•·ã≥·ãâ ·åà·â•·àµ ·äê·ãç ·âÅ·å•·à≠·äï ·ä®·àò·âÅ·å†·à≠ ·åÄ·àù·àÆ ·àÇ·à≥·â•·äï ·â†·âÄ·àã·àâ ·ã®·àö·àõ·à©·â†·âµ ·àà·â†·àà·å† ·àò·à®·åÉ ·ä®·â≥·âΩ ·ã´·àà·ãç·äï ·àõ·àµ·çà·äï·å†·à™·ã´ ·ã≠·å´·äë ·ã≠·ã∞·ãç·àâ ·ãà·ã≠·àù ·ã´·àâ·â†·âµ ·ä•·äï·ã∞·à≠·à≥·àà·äï O\n",
            "\n",
            "·ãà·ãµ ·ã®·åà·àã·åç·àå ·ã∞·äï·â†·äû·âª·âΩ·äï ·àà·àç·åÜ·ãé ·ã®·àö·à∞·å• ·àù·à≠·å• ·àµ·å¶·â≥ ·ä®·åà·àã·åç·àå ·ä•·àÖ·âµ ·â•·à´·äï·ãµ ·ã®·àÜ·äê·ãç ·àò·äê·àª·ã¨ ·ä®·ãö·àÖ ·â†·â≥·âΩ ·â£·àµ·âÄ·àò·å•·äï·àç·ãé ·àõ·àµ·çà·äï·å†·à™·ã´ ·â∞·å†·âÖ·àò·ãç ·ã®·àò·äê·àª·ã¨·äï ·ãµ·à® ·åà·çÖ ·â†·àò·åé·â•·äò·âµ ·àà·àç·åÖ·ãé ·ã®·àö·àÜ·äë ·àµ·å¶·â≥ ·âµ·àù·àÖ·à≠·â≥·ãä ·àò·å´·ãé·âª·ãé·âΩ·äï ·â†·âÄ·àã·àâ ·àò·àù·à®·å• ·ä•·äï·ã∞·àö·âΩ·àâ ·àµ·äï·åà·àç·çÖ ·â†·â≥·àã·âÖ ·ã∞·àµ·â≥ ·äê·ãç O\n",
            "·ãµ·à®·åà·çÉ·âΩ·äï·äï ·ã≠·åé·â•·äô O\n",
            "·ã®·ãà·ã∞·ã±·âµ·äï ·ã≠·àù·à®·å° O\n",
            "·ã®·àò·à®·å°·âµ·äï ·ã≠·à∏·àù·â± ·ã≠·ã∞·ãç·àâ ·ãà·ã≠·àù ·ã´·àâ·â†·âµ ·ä•·äï·ã∞·à≠·à≥·àà·äï ·ã®·ä¢·âµ·ãÆ·åµ·ã´·àç·åÜ·âΩ O\n",
            "\n",
            "·ä•·àµ·ä®·ãõ ·ã®·àç·åÜ·âΩ ·çñ·çñ ·àà·àç·åÜ·âΩ ·àù·âπ ·ã®·àÜ·äê ·àç·ã© ·ã≤·ãõ·ã≠·äï ·ã®·àç·åÜ·ãé·äï ·ã∞·àÖ·äï·äê·âµ ·ã®·àö·å†·â•·âÖ ·àõ·ã≠·âÄ·ãò·âÖ·ãù ·ä•·äì ·ã®·àõ·ã≠·âÜ·à®·âÅ·à≠ ·â†·å£·àù ·å†·äï·ä´·à´ ·àà·àõ·çÖ·ã≥·âµ ·ä†·àò·â∫ ·ä•·äì ·â†·âÄ·àã·àâ ·ã®·àö·äê·âÉ·âÄ·àç ·ã®·â∞·à®·åã·åà·å† ·ã® ·ä†·äï·ãµ ·ãì·àò·âµ ·ãã·àµ·âµ·äì ·ä•·äï·à∞·å£·àà·äï ·ã≠·ãò·ãô·äï ·ã´·àâ·â†·âµ ·ãµ·à®·àµ ·ä•·äì·àò·å£·àà·äï ·â†·àö·àä·äí·ã®·àù ·ä•·äì ·ä§·åç·ãö·â¢·àΩ·äï ·â£·ãõ·à≠ ·ã´·åà·äô·äì·àç ·ã®·àç·åÖ·àç·åÖ·ã∞·àµ·â≥ O\n",
            "\n",
            "·ã®·àç·åÜ·âΩ ·ã∞·àÖ·äï·äê·âµ ·àò·å†·â†·âÇ·ã´ ·ä†·ãµ·à´·àª ·åâ·à≠·ãµ ·àæ·àã ·àÜ·àä ·à≤·â≤ ·à¥·äï·â∞·à≠ ·äõ ·çé·âÖ ·ã≠·àù·å° ·â†·àò·äê·àª·ã¨ ·ã®·àç·åÖ·ãé·äï ·àò·äê·àª ·ã´·âÖ·àç·àâ ·ã≠·ã∞·ãç·àâ ·ãà·ã≠·àù O\n",
            "\n",
            "·ä†·ãµ·à´·àª·àú·ä≠·à≤·äÆ ·äÆ·àú·à≠·àµ ·åÄ·à≠·â£ ·àò·ãö·ãµ ·çï·àã·ãõ ·ã®·àò·åÄ·àò·à™·ã´ ·ã∞·à®·åÉ ·ä•·äï·ã∞·ãà·å° ·ã®·â¢·àÆ ·âÅ·å•·à≠ ·ã´·åà·äô·äì·àç O\n",
            "\n",
            "·ä†·ãµ·à´·àª·àú·ä≠·à≤·äÆ ·äÆ·àú·à≠·àµ ·åÄ·à≠·â£ ·àò·ãö·ãµ ·çï·àã·ãõ ·ã®·àò·åÄ·àò·à™·ã´ ·ã∞·à®·åÉ ·ä•·äï·ã∞·ãà·å° ·ã®·â¢·àÆ ·âÅ·å•·à≠ ·ã´·åà·äô·äì·àç O\n",
            "\n",
            "·ã®·ä•·äì·âµ ·çç·âÖ·à≠ ·ã®·àõ·ã≠·âã·à®·å• ·ã®·àõ·ã≠·àà·ãà·å• ·ã®·àõ·ã≠·âÄ·à≠ ·ä•·à∑ ·ä•·àµ·ä® ·àò·å®·à®·àª·ãç ·ãµ·à®·àµ ·ä®·àç·åá ·åã·à≠ ·äì·âµ ·ä≠·â•·à≠ ·àà·ä•·äì·â∂·âΩ ·çç·âÖ·àØ ·ã®·àõ·ã≠·àà·ãà·å• ·äê·ãç ·àà·â†·àà·å† ·åà·àã·åç·àå O\n",
            "\n",
            "·ä†·ãµ·à´·àª·àú·ä≠·à≤·äÆ ·äÆ·àú·à≠·àµ ·åÄ·à≠·â£ ·àò·ãö·ãµ ·çï·àã·ãõ ·ã®·àò·åÄ·àò·à™·ã´ ·ã∞·à®·åÉ ·ä•·äï·ã∞·ãà·å° ·ã®·â¢·àÆ ·âÅ·å•·à≠ ·ã´·åà·äô·äì·àç O\n",
            "\n",
            "·ãâ·àÉ ·àµ·à≠·åà·âµ·äï ·ãà·ã∞ ·çç·à´·àΩ ·ãâ·àµ·å• ·ä•·äï·ã≥·ã≠·åà·â£ ·ä•·äì ·ä†·àã·àµ·çà·àã·åä ·àΩ·â≥·äï ·ä•·äï·ã≤·àÅ·àù ·ãµ·ä´·àù·äï ·ã®·àö·ä®·àã·ä®·àç ·ä†·äï·à∂·àã ·àà ·àà ·ä®·äê·çÉ ·ã≤·àä·â®·à™ ·åã·à≠ ·â•·à≠ ·ä†·ãµ·à´·àª ·âÅ·å•·à≠ ·àç·ã∞·â≥ ·ãà·ã∞ ·â£·àç·âª ·àÜ·àµ·çí·â≥·àç ·åà·â£ ·â•·àé ·ä†·àÖ·àò·ãµ ·àÖ·äï·çÉ ·àã·ã≠ ·äõ·çé·âÖ ·â£·àâ·â†·âµ ·ã´·àà·â∞·å®·àõ·à™ ·ä≠·çç·ã´ ·àõ·ãò·ãù ·ã≠·âΩ·àã·àâ O\n",
            "\n",
            "·ãç·ãµ ·ã®·åà·àã·åç·àå ·ã∞·äï·â†·äû·âª·âΩ·äï ·ä•·äï·ä≥·äï ·àà·ãì·à®·çã ·â†·ãì·àç ·â†·à∞·àã·àù ·ä†·ã∞·à®·à≥·âΩ·àÅ O\n",
            "·â†·ãì·àâ ·ã®·à∞·àã·àù ·ã®·å§·äì ·ã®·çç·âÖ·à≠ ·ä•·äï·ã≤·àÜ·äï ·ã®·àù·äï·àò·äù·àç·ãé ·â∞·ãà·ã≥·åÜ·âπ·äï·äì ·ä†·ã≥·ã≤·à∂·âπ·äï ·ã®·åà·àã·åç·àå ·àù·à≠·â∂·âΩ ·â†·àõ·âÖ·à®·â• ·å≠·àù·à≠ ·äê·ãç O\n",
            "·â† ·ã®·àö·ãò·à®·åã ·à∂·çã·äì ·çã·à´·àΩ ·ã®·ä§·àå·ä≠·âµ·à™·ä≠ ·àò·å•·â†·àª ·â†·àõ·äë·ãã·àç·äì ·â†·ä§·àå·ä≠·âµ·à™·ä≠ ·ã®·àö·à∞·à© ·àò·çç·å´·ãé·âΩ·äì ·ã®·â∞·àà·ã´·ã© ·ã®·çÖ·ã≥·âµ ·ä•·âÉ·ãé·âΩ·äï ·ä®·äõ ·ãò·äï·ãµ ·â†·â•·ãõ·âµ·àù ·â†·å•·à´·âµ·àù ·ã´·åà·äü·â∏·ãã·àç O\n",
            "·â†·åà·àã·åç·àå ·àù·à≠·â∂·âΩ ·àµ·à´·ãé·äï ·ã´·âÖ·àç·àâ O\n",
            "·äë·àÆ·ãé·äï ·ã´·ãò·àù·äë ·àò·àç·ä´·àù ·â†·ãì·àç O\n",
            "\n",
            "·ä†·ãµ·à´·àª·àú·ä≠·à≤·äÆ ·äÆ·àú·à≠·àµ ·åÄ·à≠·â£ ·àò·ãö·ãµ ·çï·àã·ãõ ·ã®·àò·åÄ·àò·à™·ã´ ·ã∞·à®·åÉ ·ä•·äï·ã∞·ãà·å° ·ã®·â¢·àÆ ·âÅ·å•·à≠ ·ã´·åà·äô·äì·àç O\n",
            "\n",
            "·àà·ä†·å≠·à≠ ·àò·àç·ä•·ä≠·âµ ·ã≠·ã∞·ãâ·àâ ·ä†·ãµ·à´·àª ·àò·åà·äì·äõ ·àõ·à´·â∂·äï ·ã® ·åà·â†·ã´ ·àõ·ä•·ä®·àç ·â† ·ãã·äì·ãç ·àò·åç·â¢·ã´ ·àò·à¨·âµ ·àã·ã≠ ·ãà·ã≠·äï·àù ·åç·à´·ãç·äï·ãµ ·çç·àé·à≠ ·â•·âÖ ·ã≠·â†·àâ ·äê·â´ ·äÆ·àù·çí·ãç·â∞·à≠ ·àò·àÜ·äë·äï ·ã´·à®·åã·åç·å° ·ãµ·àÖ·à® ·åà·åª·âΩ·äï·äï ·ã≠·åé·â•·äô ·â¥·àå·åç·à´·àù ·âª·äì·àã·âΩ·äï·äï ·ã≠·âÄ·àã·âÄ·àâ O\n",
            "\n",
            "·àà·àù·âæ·âµ·ãé ·âµ·ä≠·ä≠·àà·äõ ·àù·à≠·å´ ·â∞·å£·å£·çä ·à∂·çã ·à≤·çà·àç·åâ ·â†·âÄ·àã·àâ ·çç·à´·àΩ ·àõ·ãµ·à®·åç ·ã®·àö·âΩ·àâ·âµ ·ã≠·ã∞·ãç·àâ ·ãà·ã≠·àù ·ã´·àâ·â†·âµ ·ä•·äï·ã∞·à≠·à≥·àà·äï ·åà·àã·åç·àå O\n",
            "\n",
            "·ã®·ä•·åç·à≠ ·àõ·à≥·åÄ·à≠ ·â•·ãô ·åä·ãú ·â†·àµ·à´ ·àã·ã≠ ·àà·àö·âÜ·àô ·à∞·ãé·âΩ ·ã®·â∞·à®·ä®·ãù ·àò·à∞·äê·å£·å†·âÖ ·ä•·äì ·â†·ä•·ãµ·àú ·àà·ã∞·ä®·àô ·ä•·åç·à≠ ·ã®·àö·àÜ·äï ·àò·çç·âµ·àÑ ·ã≠·ãò·äï·àé·âµ ·àò·â∞·äì·àç O\n",
            "·ã®·ä†·ä©·çì·äï·â∏·à≠ ·äê·å•·â¶·âΩ·äï ·àô·àâ ·â†·àô·àâ ·â†·àõ·à∏·âµ ·ã®·ã∞·àù ·ãù·ãç·ãç·à≠·äï ·ä®·çç ·àõ·ãµ·à®·åç ·ä•·äì ·à∞·àã·àõ·ãä ·ä•·äï·âÖ·àç·çç ·ä•·äï·ã≤·ã´·åà·äô ·ã≠·à®·ã≥·ãé·â≥·àç ·ã®·â≥·àò·àô ·â∞·à®·ä®·ãû·âΩ·äï ·ä•·äì ·å£·â∂·âΩ·äï ·àà·àõ·àµ·â≥·åà·àµ ·ã®·ã∞·ä®·àô ·ä•·åç·àÆ·âΩ·äï ·àà·àò·äï·ä®·â£·ä®·â• ·ä•·äì ·àà·àõ·ä®·àù ·ã®·àö·à®·ã± ·àà·àµ·àã·à≥ ·ä•·äì ·ã®·àö·ã´·äê·âÉ·âÅ ·ä†·à®·çã·ãé·âΩ ·ã®·ã´·ãò ·â†·àò·å†·âÄ·àù ·ã®·ä•·åç·àÆ·äï ·å§·äì ·ä•·äì ·àç·àµ·àã·à¥ ·ã≠·å†·â•·âÅ ·àà·âÅ·à≠·å•·àõ·âµ ·àà·â∞·à®·ä®·ãù ·àÖ·àò·àù ·çç·â±·äï ·ã®·àÜ·äê ·àà·à™·àÖ ·ãâ·â†·âµ·äï ·àà·àò·å†·â†·âÖ ·ã®·àö·àµ·àõ·àõ ·àà·ä•·åç·à≠ ·àò·ãõ·àç ·àà ·â∞·à®·ä®·ãù ·âÅ·à≠·å•·àõ·âµ ·àà ·ã∞·àù ·àò·à≠·åã·âµ ·ã® ·ã∞·àù ·àµ·à≠ ·àò·çç·â≥·â≥·âµ·äï ·ã®·àö·ã´·àò·å£ ·ãã·åã ·â•·à≠ ·â†·â∞·å®·àõ·à™ ·ä®·â•·à≠ ·â†·àã·ã≠ ·ã®·àÜ·äë ·àÅ·àà·âµ·ãï·âÉ·ãé·âΩ ·â†·ä†·äï·ãµ·ã≤·àä·â®·à™ ·åç·ãú ·à≤·ã´·ãô ·àµ·å¶·â≥ ·ä•·äï·àç·ä≠·àà·ãé·â≥·àà·äï ·âª·äì·àã·âΩ·äï·äï ·àà·åì·ã∞·äõ·ãé ·à∏·à≠ ·àõ·ãµ·à®·åç·ãé·äï ·ä†·ã≠·à≠·à± O\n",
            "\n",
            "·ä†·ãµ·à´·àª·àú·ä≠·à≤·äÆ ·äÆ·àú·à≠·àµ ·åÄ·à≠·â£ ·àò·ãö·ãµ ·çï·àã·ãõ ·ã®·àò·åÄ·àò·à™·ã´ ·ã∞·à®·åÉ ·ä•·äï·ã∞·ãà·å° ·ã®·â¢·àÆ ·âÅ·å•·à≠ ·ã´·åà·äô·äì·àç O\n",
            "\n",
            "·ä®·ãö·àÖ ·â†·â≥·âΩ ·ã´·àà·ãç·äï ·àõ·àµ·çà·äï·å†·à™·ã´ ·â†·àò·å´·äï ·ã®·àç·åÜ·ãé·äï ·àò·äê·àª ·ã´·âÖ·àç·àâ O\n",
            "\n",
            "·ã®·àÇ·à≥·â• ·àä·âÖ ·àò·äê·àª ·å•·âÖ·àç ·âÅ·å•·à≠·äï ·ä®·àò·âÅ·å†·à≠ ·åÄ·àù·àÆ ·àÇ·à≥·â•·äï ·â†·âÄ·àã·àâ ·ã®·àö·àõ·à©·â†·âµ ·àà·â†·àà·å† O\n",
            "\n",
            "·àò·äê·àª·ã¨ ·ãµ·ä≠·ãµ·ä≠ ·â£·àà·ãµ·àù·åΩ ·ã®·àç·åÜ·âΩ ·àò·àò·à™·ã´ ·â†·ä†·àõ·à≠·äõ·äì ·â†·ä•·äï·åç·àä·ãù·äõ ·àò·àõ·à™·ã´ ·ä´·à≠·ã∂·âΩ ·ã´·àâ·âµ ·àà·â†·àà·å† ·àò·à®·åÉ ·ä®·â≥·âΩ ·ã´·àà·ãç·äï ·ãµ·àÖ·à® ·åà·åΩ ·ã≠·å´·äë ·ã≠·ã∞·ãç·àâ ·ãà·ã≠·àù ·ã´·àâ·â†·âµ ·ä•·äï·ã∞·à≠·à≥·àà·äï ·ä†·ã≥·ã≤·àµ ·àù·à≠·â∂·âΩ·äï ·ä®·çà·àà·åâ ·â¥·àå·åç·à´·àù ·âª·äì·àã·âΩ·äï·äï ·â†·àò·âÄ·àã·âÄ·àç ·â§·â∞·à∞·â• ·ã≠·àÅ·äë O\n",
            "\n",
            "·ã≠·àÖ·äï ·â†·â•·à®·âµ ·ã≠·àµ·à© ·àô·àâ ·â†·àô·àâ ·ä®·â•·à®·âµ ·ã®·àö·à∞·à´ ·ã®·à´·à± ·àò·âÅ·à®·å´·äì ·àõ·å†·çä·ã´ ·ã´·àà·ãç ·àõ·à´·ä™ ·åå·å• ·àò·àÜ·äï ·ã®·àö·âΩ·àç ·â∞·åà·å£·å•·àû ·à≤·ã´·àç·âÖ ·à¥·àú·âµ·à≠ ·à≠·ãù·àõ·äî ·ã´·àà·ãç ·çà·â≥·äù ·çê·ãù·àç ·àà·àö·çà·àç·åâ ·â∞·àò·à´·å≠ ·ä†·à∞·à´·à©·äï ·â†·ãù·à≠·ãù·à≠ ·ã®·àö·ã´·à≥·ã≠ ·àõ·äë·ãã·àç ·ã´·àà·ãç ·àù·à≠·å• ·àµ·å¶·â≥ ·àò·àÜ·äï ·ã®·àö·âΩ·àç ·ä†·âµ·ä©·àÆ·âµ·äï·äì ·çÖ·äì·âµ·äï ·ã®·àö·àà·àõ·àò·ã±·â†·âµ ·â†·à´·àµ ·àò·â∞·àõ·àò·äï·äï ·ã®·àö·ã´·ã≥·â•·à≠ ·àù·äì·â£·ãä ·ä†·âÖ·àù·äï·äì ·çà·å†·à´·äï ·ã®·àö·ã´·åé·àà·â•·âµ ·ãò·äì ·àà·àõ·àà·âµ·äì ·ã®·àò·å®·äì·äê·âÖ ·àµ·àú·âµ·äï ·àà·àò·âÄ·äê·àµ ·ã®·àö·à®·ã≥ ·ã≠·ãò·ãô·äï ·ã´·àâ·â†·âµ ·ãµ·à®·àµ ·ä•·äì·àò·å£·àà·äï ·ãç·ãµ ·ã∞·äï·â†·äõ·âΩ·äï ·â†·àµ·à´·âΩ·äï ·àã·ã≠ ·àÄ·à≥·â• ·ä•·äì ·ä†·àµ·â∞·ã´·ã®·âµ ·ä´·àà·ãé·âµ ·â†·ãö·àÖ ·àµ·àç·ä≠ ·ã∞·ãâ·àà·ãâ ·ã´·à≥·ãâ·âÅ·äï O\n",
            "·ä†·ãµ·à´·àª ·åâ·à≠·ãµ ·àæ·àã ·àÜ·àä ·à≤·â≤ ·à¥·äï·â∞·à≠ ·äõ ·çé·âÖ ·çí·ã´·à≥ ·ä´·äï·âµ·à™ ·â≥·ãà·à≠ ·äõ ·çé·âÖ O\n",
            "·åÄ·àû ·ã∞·àä·äì ·àÖ·äï·çÉ ·àù·ãµ·à≠ ·àã·ã≠ ·ãç·àç·äì ·àõ·àµ·à®·åÉ ·ã´·àà·â†·âµ ·àÖ·äï·çÉ ·åà·à≠·åÇ ·àò·â•·à´·âµ ·àÉ·ã≠·àç ·çã·ãç·äï·â¥·äï ·àÖ·äï·çÉ ·çä·âµ ·àà·çä·âµ ·ã®·â∞·à®·åã·åà·å† ·ã® ·ä†·äï·ãµ ·ãì·àò·âµ ·ãã·àµ·âµ·äì ·ä•·äï·à∞·å£·àà·äï ·â≤·ä≠·â∂·ä≠ ·âª·äì·àç ·ãà·ã≠·àù ·çå·àµ·â°·ä≠ ·åà·çÉ·âΩ·äï·äï ·ã≠·åé·â•·äô ·â≤·ä≠·â∂·ä≠ ·çå·àµ·â°·ä≠ ·ãå·â•·à≥·ã≠·âµ ·ã®·àç·åÜ·âΩ·àò·å´·ãà·âª O\n",
            "\n",
            "·ä•·àÅ·ãµ ·àÅ·àå·àù ·ä≠·çç·âµ ·äê·äï ·ä†·ãµ·à´·àª·àú·ä≠·à≤·äÆ ·äÆ·àú·à≠·àµ ·åÄ·à≠·â£ ·àò·ãö·ãµ ·çï·àã·ãõ ·ã®·àò·åÄ·àò·à™·ã´ ·ã∞·à®·åÉ ·ä•·äï·ã∞·ãà·å° ·ã®·â¢·àÆ ·âÅ·å•·à≠ ·ã´·åà·äô·äì·àç O\n",
            "\n",
            "·ã®·àä·å• ·àõ·ãû·à™·ã´ ·àà·ä•·äï·åÄ·à´ ·àà ·å®·å®·â•·à≥ ·ã®·àö·àÜ·äï ·ãã·åã ·ä≠·çç·ã´·ãé·äï·â†·àû·â£·ã≠·àç·â£·äï·ä™·äï·åç·àò·çà·çÄ·àù·àù·ã≠·âΩ·àã·àâ ·â†·â∞·å®·àõ·à™ ·ä®·â•·à≠ ·â†·àã·ã≠ ·ã®·àÜ·äë ·àÅ·àà·âµ·ãï·âÉ·ãé·âΩ ·à≤·åà·ãô ·àµ·å¶·â≥ ·ä•·äï·àç·ä≠·àà·ãé·â≥·àà·äï ·âª·äì·àã·âΩ·äï·äï ·àà·åì·ã∞·äõ·ãé ·à∏·à≠ ·àõ·ãµ·à®·åç·ãé·äï ·ä†·ã≠·à≠·à± O\n",
            "\n",
            "·ä†·ãµ·à´·àª·àú·ä≠·à≤·äÆ ·äÆ·àú·à≠·àµ ·åÄ·à≠·â£ ·àò·ãö·ãµ ·çï·àã·ãõ ·ã®·àò·åÄ·àò·à™·ã´ ·ã∞·à®·åÉ ·ä•·äï·ã∞·ãà·å° ·ã®·â¢·àÆ ·âÅ·å•·à≠ ·ã´·åà·äô·äì·àç O\n",
            "\n",
            "·àç·åÜ·âª·âΩ·àÅ·äï ·â†·àù·äï ·ä•·äì ·ä•·äï·ã¥·âµ ·àõ·àµ·å†·äì·âµ ·ä•·äï·ã≥·àà·â£·âΩ·àÅ ·åç·à´ ·àà·â∞·åã·â£·âΩ·àÅ ·ãà·àã·åÜ·âΩ ·ã≠·ã∞·ãç·àâ ·ãà·ã≠·àù ·ã´·àâ·â†·âµ ·ä•·äï·ã∞·à≠·à≥·àà·äï O\n",
            "\n",
            "·ã®·àç·åÜ·âΩ ·ã•·ãã·ã•·ãå ·à∏·à≠·â∞·â¥·âÖ·à≠·å´·âµ ·ä≥·àµ ·àò·å´·ãà·âª ·àò·åª·çä·ã´ ·à∞·àå·ã≥ ·ä•·äì ·ã®·àò·à≥·à∞·àâ·âµ·äï ·àÅ·àâ·äï·àù ·â†·ä†·äï·ãµ ·àã·ã≠ ·ã®·ã´·ãò ·ã®·àç·åÜ·âΩ ·àò·å´·ãà·âª ·ä®·àò·äê·àª·ã¨ ·ã≠·à∏·àù·â± O\n",
            "·ä†·ãµ·à´·àª ·åâ·à≠·ãµ ·àæ·àã ·àÜ·àä ·à≤·â≤ ·à¥·äï·â∞·à≠ ·äõ ·çé·âÖ ·ã≠·àù·å° ·â†·àò·äê·àª·ã¨ ·ã®·àç·åÖ·ãé·äï ·àò·äê·àª ·ã´·âÖ·àç·àâ ·ã≠·ã∞·ãç·àâ ·ãà·ã≠·àù ·àç·åÜ·âΩ O\n",
            "\n",
            "·ä†·ãµ·à´·àª·àú·ä≠·à≤·äÆ ·äÆ·àú·à≠·àµ ·åÄ·à≠·â£ ·àò·ãö·ãµ ·çï·àã·ãõ ·ã®·àò·åÄ·àò·à™·ã´ ·ã∞·à®·åÉ ·ä•·äï·ã∞·ãà·å° ·ã®·â¢·àÆ ·âÅ·å•·à≠ ·ã´·åà·äô·äì·àç O\n",
            "\n",
            "·ãã·åã ·â•·à≠ ·ãç·àµ·äï ·çç·à¨ ·äê·ãç ·ã´·àà·äï ·ä†·ãµ·à´·àª ·àò·åà·äì·äõ·àò·à∞·à®·âµ·ã∞·çã·à≠·àû·àç·àÅ·àà·â∞·äõ·çé·âÖ ·â¢·àÆ ·âÅ ·â† ·àà·àõ·ãò·ãù ·ã≠·å†·âÄ·àô ·àà·â∞·å®·àõ·à™ ·àõ·â•·à´·à™·ã´ ·ã®·â¥·àå·åç·à´·àù ·åà·çÉ·âΩ·äï O\n",
            "\n",
            "·ä†·ãµ·à´·àª·àú·ä≠·à≤·äÆ ·äÆ·àú·à≠·àµ ·åÄ·à≠·â£ ·àò·ãö·ãµ ·çï·àã·ãõ ·ã®·àò·åÄ·àò·à™·ã´ ·ã∞·à®·åÉ ·ä•·äï·ã∞·ãà·å° ·ã®·â¢·àÆ ·âÅ·å•·à≠ ·ã´·åà·äô·äì·àç O\n",
            "\n",
            "·â†·à®·çç·âµ ·âÄ·äï·ãé ·à±·âÖ ·àã·ã≠ ·àò·àµ·â∞·äì·åà·ãµ ·àà·àù·âµ·çà·àç·åâ ·ãç·ãµ ·ã∞·äï·â†·äû·âª·âΩ·äï ·äê·åà ·ä®·å†·ãã·â± ·âÄ·äë ·ãµ·à®·àµ ·à±·âÉ·âΩ·äï ·ä≠·çç·âµ ·àÅ·äñ ·ã≠·å†·â•·âÖ·ãé·â≥·àç ·à±·âÖ ·àò·â∞·ãç ·àò·åç·ãõ·âµ ·àã·àç·âª·àã·âΩ·àÅ ·â†·àû·â∞·à®·äû·âΩ ·ã´·àâ·â†·âµ ·ä•·äì·ã∞·à≠·à≥·àà·äï ·ãò·ãà·âµ·à≠ ·à∞·äû ·âÖ·ã≥·àú ·ä®·å†·ãã·â± ·ä•·àµ·ä® ·àù·àΩ·â± ·ãµ·à®·àµ ·ä≠·çç·âµ ·àò·àÜ·äë·äï ·ä•·äï·åà·àç·çÉ·àà·äï ·ä†·ãµ·à´·àª ·àò·åà·äì·äõ·àò·à∞·à®·âµ·ã∞·çã·à≠·àû·àç·àÅ·àà·â∞·äõ·çé·âÖ ·â¢·àÆ ·âÅ ·â† ·àà·àõ·ãò·ãù ·ã≠·å†·âÄ·àô ·àà·â∞·å®·àõ·à™ ·àõ·â•·à´·à™·ã´ ·ã®·â¥·àå·åç·à´·àù ·åà·çÉ·âΩ·äï O\n",
            "\n",
            "·ä†·ãµ·à´·àª·àú·ä≠·à≤·äÆ ·äÆ·àú·à≠·àµ ·åÄ·à≠·â£ ·àò·ãö·ãµ ·çï·àã·ãõ ·ã®·àò·åÄ·àò·à™·ã´ ·ã∞·à®·åÉ ·ä•·äï·ã∞·ãà·å° ·ã®·â¢·àÆ ·âÅ·å•·à≠ ·ã´·åà·äô·äì·àç O\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa6a9542",
        "outputId": "644cad3b-d383-41c6-a091-8f58a99dfa21"
      },
      "source": [
        "!pip install --upgrade transformers"
      ],
      "id": "fa6a9542",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b99a6ea",
        "outputId": "12c2ca97-7849-49f6-d6f8-ae2bd31e1add"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_tags = [tag for tag_seq in tags for tag in tag_seq]\n",
        "tag_counts = Counter(all_tags)\n",
        "\n",
        "print(\"NER Tag Distribution:\")\n",
        "for tag, count in tag_counts.most_common():\n",
        "    print(f\"{tag}: {count}\")"
      ],
      "id": "3b99a6ea",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER Tag Distribution:\n",
            "O: 12\n",
            "I-PRICE: 8\n",
            "B-PRODUCT: 5\n",
            "B-LOC: 5\n",
            "B-PRICE: 5\n",
            "I-LOC: 3\n",
            "I-PRODUCT: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6aefea1",
        "outputId": "0e480c10-dce9-4290-c418-dbfcb6aa8275"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Calculate class weights\n",
        "# We need weights for all possible label IDs, including -100 for ignored tokens.\n",
        "# However, class weights are typically applied to the actual classes being predicted.\n",
        "# Let's compute weights for the actual NER tags first.\n",
        "# We'll need to map these weights to the label IDs later if used in the loss function.\n",
        "\n",
        "# Get all unique tags present in the dataset (excluding -100 if it somehow appeared before mapping)\n",
        "# We already have label_list which contains all unique tags from the dataset.\n",
        "classes = np.array(label_list) # Convert to numpy array\n",
        "\n",
        "# Get all gold labels (excluding -100 from padding/special tokens if any were introduced before this point)\n",
        "# The 'tags' list contains the original gold tags for each token in each sentence.\n",
        "all_gold_tags = [tag for tag_seq in tags for tag in tag_seq]\n",
        "\n",
        "# Compute weights for the actual classes\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=all_gold_tags)\n",
        "\n",
        "# Create a dictionary mapping label ID to its computed weight\n",
        "# We need to ensure the order matches label2id\n",
        "weight_dict = {label2id[cls]: weight for cls, weight in zip(classes, class_weights)}\n",
        "\n",
        "# Convert to a tensor, ensuring it covers all potential label IDs the model might output,\n",
        "# including -100 if necessary, though typically weights are for the actual classes.\n",
        "# Let's create a weight tensor based on the label IDs we have.\n",
        "# If the model's loss function needs weights for all possible output indices,\n",
        "# we might need to handle -100 specifically, but often it's ignored by the loss itself.\n",
        "# Let's create a tensor for the weights of the actual classes.\n",
        "weights_tensor = torch.tensor([weight_dict.get(i, 1.0) for i in range(len(label_list))])\n",
        "\n",
        "print(\"Computed Class Weights (mapping label ID to weight):\")\n",
        "for i, weight in enumerate(weights_tensor.tolist()):\n",
        "    print(f\"Label ID {i} ({id2label.get(i, 'Unknown')}): {weight:.4f}\")\n",
        "\n",
        "# Now we have the weights_tensor. How to apply this in the Trainer?\n",
        "# The standard Trainer's compute_loss does not directly accept class weights.\n",
        "# We typically need to pass this to the model's forward method if it supports a 'weight' argument for the loss.\n",
        "# Or, create a custom Trainer that overrides compute_loss.\n",
        "\n",
        "# Given the standard Trainer structure in the original code, directly passing weights_tensor\n",
        "# to the Trainer or TrainingArguments is not a built-in feature.\n",
        "# We would typically need a custom Trainer or modify the model's forward pass.\n",
        "\n",
        "# Let's calculate the weights and explain how they could be used,\n",
        "# acknowledging the limitation of the standard Trainer."
      ],
      "id": "d6aefea1",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed Class Weights (mapping label ID to weight):\n",
            "Label ID 0 (B-LOC): 1.1714\n",
            "Label ID 1 (B-PRICE): 1.1714\n",
            "Label ID 2 (B-PRODUCT): 1.1714\n",
            "Label ID 3 (I-LOC): 1.9524\n",
            "Label ID 4 (I-PRICE): 0.7321\n",
            "Label ID 5 (I-PRODUCT): 1.9524\n",
            "Label ID 6 (O): 0.4881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1b95d13"
      },
      "source": [
        "# Task\n",
        "Use model interpretability tools to explain how the NER model identifies entities, ensuring transparency and trust in the system."
      ],
      "id": "d1b95d13"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0f1f127"
      },
      "source": [
        "## Load a trained model and tokenizer\n",
        "\n",
        "### Subtask:\n",
        "Load one of the fine-tuned NER models and its corresponding tokenizer that were saved in the previous step.\n"
      ],
      "id": "d0f1f127"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04bbc261"
      },
      "source": [
        "**Reasoning**:\n",
        "Load one of the saved models and its tokenizer for the next steps in model interpretability.\n",
        "\n"
      ],
      "id": "04bbc261"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a574eb8",
        "outputId": "95c85945-18cf-4d24-dd09-28c3b8272207"
      },
      "source": [
        "# Choose the model to load (e.g., the one with the best performance from the previous step, or just the last one)\n",
        "# Based on results_df, xlm-roberta-base had the best f1 score, albeit low.\n",
        "# Let's load the xlm-roberta-base model.\n",
        "model_to_load = \"xlm-roberta-base\"\n",
        "loaded_model_dir = f\"./final_model_{model_to_load.split('/')[-1]}\"\n",
        "\n",
        "# Load the fine-tuned model\n",
        "loaded_model = AutoModelForTokenClassification.from_pretrained(loaded_model_dir)\n",
        "\n",
        "# Load the corresponding tokenizer\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(loaded_model_dir)\n",
        "\n",
        "print(f\"Loaded model and tokenizer from: {loaded_model_dir}\")"
      ],
      "id": "3a574eb8",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model and tokenizer from: ./final_model_xlm-roberta-base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93bf14c9"
      },
      "source": [
        "## Prepare data for interpretability tools\n",
        "\n",
        "### Subtask:\n",
        "Format sample text data (including potentially difficult cases) in a way that can be processed by SHAP and LIME, including tokenization and handling of special tokens.\n"
      ],
      "id": "93bf14c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77b77208"
      },
      "source": [
        "**Reasoning**:\n",
        "Define sample sentences, tokenize them, and prepare inputs for the model while keeping track of original words and token mappings.\n",
        "\n"
      ],
      "id": "77b77208"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "bb43c44f",
        "outputId": "6695d559-8e33-4362-eb3b-875b4c71685e"
      },
      "source": [
        "# 1. Define sample Amharic sentences\n",
        "sample_sentences = [\n",
        "    \"·ä†·ãµ·à´·àª ·àò·åà·äì·äõ ·àò·à∞·à®·âµ ·ã∞·çã·à≠ ·àû·àç ·àÅ·àà·â∞·äõ ·çé·âÖ ·â¢·àÆ ·âÅ·å•·à≠ 205\", # Sentence with LOC entities\n",
        "    \"·ãã·åã 500 ·â•·à≠\", # Sentence with PRICE entities\n",
        "    \"·ã®·àç·åÜ·âΩ ·àò·å´·ãà·âª ·ä®·àò·äê·àª·ã¨ ·ã≠·à∏·àù·â±\", # Sentence with PRODUCT entity\n",
        "    \"·ä†·ãµ·à´·àª ·åâ·à≠·ãµ ·àæ·àã ·àÜ·àä ·à≤·â≤ ·à¥·äï·â∞·à≠ 1·äõ ·çé·âÖ ·çí·ã´·à≥ ·ä´·äï·âµ·à™ ·â≥·ãà·à≠ 2·äõ ·çé·âÖ\", # Sentence with multiple LOC entities\n",
        "    \"·ãã·åã 1000 ·â•·à≠ ·â†·â∞·å®·àõ·à™ ·ä®500 ·â•·à≠ ·â†·àã·ã≠ ·ã®·àÜ·äë ·àÅ·àà·âµ ·ãï·âÉ·ãé·âΩ ·â†·ä†·äï·ãµ ·ã≤·àä·â®·à™ ·åç·ãú ·à≤·ã´·ãô ·àµ·å¶·â≥ ·ä•·äï·àç·ä≠·àà·ãé·â≥·àà·äï\" # Sentence with multiple PRICE entities\n",
        "]\n",
        "\n",
        "# Prepare data structure to store processed inputs and mappings\n",
        "processed_samples = []\n",
        "\n",
        "# 2. and 3. Tokenize and prepare inputs for each sentence\n",
        "for sentence in sample_sentences:\n",
        "    # Tokenize the sentence, handling special tokens\n",
        "    tokenized_input = loaded_tokenizer(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\", # Return PyTorch tensors\n",
        "        padding=True,       # Pad to the maximum length in the batch (or model max length)\n",
        "        truncation=True     # Truncate if longer than model max length\n",
        "    )\n",
        "\n",
        "    # Get input IDs and attention mask\n",
        "    input_ids = tokenized_input[\"input_ids\"]\n",
        "    attention_mask = tokenized_input[\"attention_mask\"]\n",
        "\n",
        "    # 4. Keep track of original words and token indices\n",
        "    # Need to get the original words from the sentence\n",
        "    original_words = sentence.split()\n",
        "\n",
        "    # Get word IDs from the tokenizer's output to map tokens back to words\n",
        "    # This requires tokenizing without returning tensors first to get word_ids\n",
        "    tokenized_for_mapping = loaded_tokenizer(sentence, is_split_into_words=True, truncation=True, padding=False)\n",
        "    word_ids = tokenized_for_mapping.word_ids() # Get word IDs for the tokenized sentence\n",
        "\n",
        "    # Store the processed data\n",
        "    processed_samples.append({\n",
        "        \"sentence\": sentence,\n",
        "        \"original_words\": original_words,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"word_ids\": word_ids, # Mapping from token index to word index\n",
        "        \"tokens\": loaded_tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) # Get token strings\n",
        "    })\n",
        "\n",
        "# Display the structure of the first processed sample as an example\n",
        "print(\"Example of processed sample structure:\")\n",
        "print(processed_samples[0])"
      ],
      "id": "bb43c44f",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "PreTokenizedEncodeInput must be Union[PreTokenizedInputSequence, Tuple[PreTokenizedInputSequence, PreTokenizedInputSequence]]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-3452167316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Get word IDs from the tokenizer's output to map tokens back to words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# This requires tokenizing without returning tensors first to get word_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtokenized_for_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mword_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_for_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get word IDs for the tokenized sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2865\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2866\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2867\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2868\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2869\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2975\u001b[0m             )\n\u001b[1;32m   2976\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2977\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   2978\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m         )\n\u001b[1;32m   3051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3052\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   3053\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m     ) -> BatchEncoding:\n\u001b[1;32m    614\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    542\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: PreTokenizedEncodeInput must be Union[PreTokenizedInputSequence, Tuple[PreTokenizedInputSequence, PreTokenizedInputSequence]]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7ca4804"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `TypeError: PreTokenizedEncodeInput must be Union[PreTokenizedInputSequence, Tuple[PreTokenizedInputSequence, PreTokenizedInputSequence]]` indicates that the tokenizer's `is_split_into_words=True` argument expects a list of words, not a single string sentence. I need to split the sentence into words before passing it to the tokenizer with `is_split_into_words=True`.\n",
        "\n"
      ],
      "id": "e7ca4804"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_LyREcKd5qB",
        "outputId": "635440ea-db6d-4174-f8e5-e9fe79a92932"
      },
      "source": [
        "# 1. Define sample Amharic sentences\n",
        "sample_sentences = [\n",
        "    \"·ä†·ãµ·à´·àª ·àò·åà·äì·äõ ·àò·à∞·à®·âµ ·ã∞·çã·à≠ ·àû·àç ·àÅ·àà·â∞·äõ ·çé·âÖ ·â¢·àÆ ·âÅ·å•·à≠ 205\", # Sentence with LOC entities\n",
        "    \"·ãã·åã 500 ·â•·à≠\", # Sentence with PRICE entities\n",
        "    \"·ã®·àç·åÜ·âΩ ·àò·å´·ãà·âª ·ä®·àò·äê·àª·ã¨ ·ã≠·à∏·àù·â±\", # Sentence with PRODUCT entity\n",
        "    \"·ä†·ãµ·à´·àª ·åâ·à≠·ãµ ·àæ·àã ·àÜ·àä ·à≤·â≤ ·à¥·äï·â∞·à≠ 1·äõ ·çé·âÖ ·çí·ã´·à≥ ·ä´·äï·âµ·à™ ·â≥·ãà·à≠ 2·äõ ·çé·âÖ\", # Sentence with multiple LOC entities\n",
        "    \"·ãã·åã 1000 ·â•·à≠ ·â†·â∞·å®·àõ·à™ ·ä®500 ·â•·à≠ ·â†·àã·ã≠ ·ã®·àÜ·äë ·àÅ·àà·âµ ·ãï·âÉ·ãé·âΩ ·â†·ä†·äï·ãµ ·ã≤·àä·â®·à™ ·åç·ãú ·à≤·ã´·ãô ·àµ·å¶·â≥ ·ä•·äï·àç·ä≠·àà·ãé·â≥·àà·äï\" # Sentence with multiple PRICE entities\n",
        "]\n",
        "\n",
        "# Prepare data structure to store processed inputs and mappings\n",
        "processed_samples = []\n",
        "\n",
        "# 2. and 3. Tokenize and prepare inputs for each sentence\n",
        "for sentence in sample_sentences:\n",
        "    # Split the sentence into words for is_split_into_words=True\n",
        "    original_words = sentence.split()\n",
        "\n",
        "    # Tokenize the sentence, handling special tokens\n",
        "    # Use the list of words when is_split_into_words is True\n",
        "    tokenized_input = loaded_tokenizer(\n",
        "        original_words, # Pass the list of words\n",
        "        return_tensors=\"pt\", # Return PyTorch tensors\n",
        "        padding=True,       # Pad to the maximum length in the batch (or model max length)\n",
        "        truncation=True,    # Truncate if longer than model max length\n",
        "        is_split_into_words=True # Indicate that the input is pre-split into words\n",
        "    )\n",
        "\n",
        "    # Get input IDs and attention mask\n",
        "    input_ids = tokenized_input[\"input_ids\"]\n",
        "    attention_mask = tokenized_input[\"attention_mask\"]\n",
        "\n",
        "    # Get word IDs from the tokenizer's output to map tokens back to words\n",
        "    word_ids = tokenized_input.word_ids() # Get word IDs for the tokenized sentence\n",
        "\n",
        "    # Store the processed data\n",
        "    processed_samples.append({\n",
        "        \"sentence\": sentence,\n",
        "        \"original_words\": original_words,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"word_ids\": word_ids, # Mapping from token index to word index\n",
        "        \"tokens\": loaded_tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) # Get token strings\n",
        "    })\n",
        "\n",
        "# Display the structure of the first processed sample as an example\n",
        "print(\"Example of processed sample structure:\")\n",
        "print(processed_samples[0])"
      ],
      "id": "a_LyREcKd5qB",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of processed sample structure:\n",
            "{'sentence': '·ä†·ãµ·à´·àª ·àò·åà·äì·äõ ·àò·à∞·à®·âµ ·ã∞·çã·à≠ ·àû·àç ·àÅ·àà·â∞·äõ ·çé·âÖ ·â¢·àÆ ·âÅ·å•·à≠ 205', 'original_words': ['·ä†·ãµ·à´·àª', '·àò·åà·äì·äõ', '·àò·à∞·à®·âµ', '·ã∞·çã·à≠', '·àû·àç', '·àÅ·àà·â∞·äõ', '·çé·âÖ', '·â¢·àÆ', '·âÅ·å•·à≠', '205'], 'input_ids': tensor([[     0, 140042,   2370,  60014,   9171,  88827,  21368, 191973,  45694,\n",
            "           2202,      6, 115742,      6,  31531,   5653, 137526,  61264,  73930,\n",
            "              2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'word_ids': [None, 0, 1, 1, 1, 2, 3, 3, 4, 4, 5, 5, 6, 6, 6, 7, 8, 9, None], 'tokens': ['<s>', '‚ñÅ·ä†·ãµ·à´·àª', '‚ñÅ·àò', '·åà·äì', '·äõ', '‚ñÅ·àò·à∞·à®·âµ', '‚ñÅ·ã∞', '·çã·à≠', '‚ñÅ·àû', '·àç', '‚ñÅ', '·àÅ·àà·â∞·äõ', '‚ñÅ', '·çé', '·âÖ', '‚ñÅ·â¢·àÆ', '‚ñÅ·âÅ·å•·à≠', '‚ñÅ205', '</s>']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c7a5744"
      },
      "source": [
        "## Implement shap for ner\n",
        "\n",
        "### Subtask:\n",
        "Use the SHAP library to explain the model's predictions for individual tokens or sentences, focusing on the contribution of each input token to the prediction of a specific NER tag. This may require custom integration with the Hugging Face model output.\n"
      ],
      "id": "4c7a5744"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fb277e5"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the SHAP library to enable model interpretability analysis.\n",
        "\n"
      ],
      "id": "7fb277e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81eb787c",
        "outputId": "fec3c6dd-2f2d-4421-c969-692e83ba23c2"
      },
      "source": [
        "!pip install shap"
      ],
      "id": "81eb787c",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcdc1c7d"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a prediction function that prepares the input and returns the model's output logits, which is needed by SHAP explainers.\n",
        "\n"
      ],
      "id": "fcdc1c7d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ecc0011"
      },
      "source": [
        "import torch\n",
        "\n",
        "def predict_fn(input_data):\n",
        "    \"\"\"\n",
        "    Prediction function for SHAP, taking processed inputs and returning logits.\n",
        "\n",
        "    Args:\n",
        "        input_data (list): A list of dictionaries, where each dict contains\n",
        "                           'input_ids' and 'attention_mask' tensors.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A numpy array of logits from the model's output.\n",
        "                       Shape will be (num_samples, max_sequence_length, num_labels).\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    logits_list = []\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        for item in input_data:\n",
        "            # Ensure input_ids and attention_mask are on the correct device (CPU in this case)\n",
        "            input_ids = item['input_ids'].to(loaded_model.device)\n",
        "            attention_mask = item['attention_mask'].to(loaded_model.device)\n",
        "\n",
        "            outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            logits_list.append(logits.squeeze(0).cpu().numpy()) # Remove batch dimension, move to CPU, convert to numpy\n",
        "\n",
        "    # Pad or stack logits if necessary to ensure consistent shape for SHAP\n",
        "    # For now, assuming single sentence input for simplicity in SHAP explanation\n",
        "    # If handling batches, this would need adjustment.\n",
        "    return np.array(logits_list)"
      ],
      "id": "8ecc0011",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "772f2c52"
      },
      "source": [
        "**Reasoning**:\n",
        "Select an instance from the processed data and integrate it with a SHAP explainer and the prediction function to generate SHAP values for a specific target tag, then visualize the results.\n",
        "\n"
      ],
      "id": "772f2c52"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "86df33ec",
        "outputId": "19329325-9f3d-4d58-8a8d-8f5f533d86c2"
      },
      "source": [
        "import shap\n",
        "\n",
        "# Select a sample sentence to explain (e.g., the first one)\n",
        "instance_index = 0\n",
        "explained_sample = processed_samples[instance_index]\n",
        "sentence_tokens = explained_sample[\"tokens\"]\n",
        "input_ids_tensor = explained_sample[\"input_ids\"]\n",
        "attention_mask_tensor = explained_sample[\"attention_mask\"]\n",
        "original_words = explained_sample[\"original_words\"]\n",
        "word_ids = explained_sample[\"word_ids\"]\n",
        "\n",
        "# Prepare the input for the predict_fn\n",
        "# The predict_fn expects a list of dictionaries\n",
        "predict_input = [{'input_ids': input_ids_tensor, 'attention_mask': attention_mask_tensor}]\n",
        "\n",
        "# Get the model's prediction for this instance\n",
        "# This is needed to identify the predicted tags to explain\n",
        "with torch.no_grad():\n",
        "    outputs = loaded_model(input_ids=input_ids_tensor.to(loaded_model.device),\n",
        "                           attention_mask=attention_mask_tensor.to(loaded_model.device))\n",
        "    logits = outputs.logits\n",
        "    predicted_token_ids = torch.argmax(logits, dim=2).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Map predicted token IDs back to NER tags\n",
        "predicted_tags = [id2label.get(token_id, \"O\") for token_id in predicted_token_ids]\n",
        "\n",
        "print(f\"Original Sentence: {explained_sample['sentence']}\")\n",
        "print(f\"Tokens: {sentence_tokens}\")\n",
        "print(f\"Predicted Tags: {predicted_tags}\")\n",
        "\n",
        "\n",
        "# SHAP Explainer\n",
        "# For token classification, we are interested in the contribution of each input token\n",
        "# to the prediction of each output tag at each position.\n",
        "# A common approach is to use a KernelExplainer or GradientExplainer if applicable,\n",
        "# but these can be slow or require gradient computation.\n",
        "# For simple text inputs and models, shap.Explainer with a masker can work.\n",
        "\n",
        "# Create a masker. The masker generates perturbed inputs for the explainer.\n",
        "# For text, a common masker is one that masks tokens.\n",
        "# We need a masker that understands our tokenized input structure.\n",
        "# A custom masker might be needed, or we can try the built-in text masker if compatible.\n",
        "\n",
        "# Let's try using shap.Explainer with a text-based masker.\n",
        "# The masker needs to take the original input format (list of tokens or words)\n",
        "# and return modified input compatible with the predict_fn.\n",
        "\n",
        "# Option 1: Using a simple masker that works on the tokenized input IDs.\n",
        "# This might be complex because predict_fn expects tensors.\n",
        "# Option 2: Create a custom masker that operates on the original words\n",
        "# and uses the tokenizer internally, then calls predict_fn.\n",
        "\n",
        "# Let's try Option 2, creating a masker based on original words.\n",
        "# This requires the masker to return the processed input dictionary format\n",
        "# expected by predict_fn.\n",
        "\n",
        "class WordMasker(shap.maskers.Text):\n",
        "    def __init__(self, tokenizer, original_words, **kwargs):\n",
        "        # shap.maskers.Text expects a text string or list of strings.\n",
        "        # We'll adapt it to work with our word list and tokenizer.\n",
        "        # The masker needs to return the format expected by the model/predict_fn.\n",
        "        self.tokenizer = tokenizer\n",
        "        self.original_words = original_words\n",
        "        super().__init__(mask_token='[MASK]', **kwargs) # Use a mask token\n",
        "\n",
        "    def __call__(self, masked_indices, x):\n",
        "        # x is the original input passed to explainer.shap_values (e.g., the list of words)\n",
        "        # masked_indices tells us which indices (words) to mask.\n",
        "        masked_words = x[:] # Create a copy of the original words\n",
        "        for i in masked_indices:\n",
        "            # Replace the masked word with the mask token or handle in a way\n",
        "            # that the tokenizer will treat it appropriately (e.g., using tokenizer.mask_token)\n",
        "             if i < len(masked_words): # Ensure index is within bounds\n",
        "                masked_words[i] = self.tokenizer.mask_token # Replace word with mask token\n",
        "\n",
        "        # Now, tokenize the masked words and format for the predict_fn\n",
        "        tokenized_input = self.tokenizer(\n",
        "            masked_words,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            is_split_into_words=True\n",
        "        )\n",
        "        # Return the list of dictionaries expected by predict_fn\n",
        "        return [{'input_ids': tokenized_input[\"input_ids\"], 'attention_mask': tokenized_input[\"attention_mask\"]}]\n",
        "\n",
        "# Instantiate the custom masker using the original words of the selected sentence\n",
        "masker = WordMasker(loaded_tokenizer, original_words)\n",
        "\n",
        "# Instantiate the Explainer\n",
        "# We need to explain the output for each token position and each label.\n",
        "# The output of predict_fn is (1, seq_len, num_labels).\n",
        "# We want to explain the logits.\n",
        "# The base value for SHAP should be the average prediction over the background data.\n",
        "# For simplicity here, we can use a zero base value or calculate it.\n",
        "# A simple approach is to explain the prediction for a specific output index (token position, label).\n",
        "\n",
        "# Let's explain the prediction for the predicted tag at each token position.\n",
        "# We need to make the predict_fn return a specific shape for the explainer.\n",
        "# For a single instance, SHAP expects predict_fn to return an array of shape (num_outputs,)\n",
        "# where num_outputs is the number of values being explained.\n",
        "# If we want to explain all token predictions simultaneously, the output shape is (seq_len * num_labels).\n",
        "\n",
        "# Let's redefine predict_fn slightly or adapt the SHAP call to handle the output shape.\n",
        "# A common pattern is to explain the probability of the predicted class.\n",
        "# The logits are a reasonable target for explanation.\n",
        "\n",
        "# Let's adapt the predict_fn to return a flattened array of logits for a single input.\n",
        "# If we explain one instance at a time, the predict_fn should take a list of size 1.\n",
        "\n",
        "def predict_fn_single_instance(input_data_list):\n",
        "     \"\"\"\n",
        "     Prediction function for SHAP, takes a list containing one processed input dict,\n",
        "     returns a flattened array of logits for that instance.\n",
        "     \"\"\"\n",
        "     if len(input_data_list) != 1:\n",
        "         raise ValueError(\"predict_fn_single_instance expects a list with exactly one item.\")\n",
        "\n",
        "     item = input_data_list[0]\n",
        "     loaded_model.eval()\n",
        "     with torch.no_grad():\n",
        "         input_ids = item['input_ids'].to(loaded_model.device)\n",
        "         attention_mask = item['attention_mask'].to(loaded_model.device)\n",
        "         outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "         logits = outputs.logits.squeeze(0).cpu().numpy() # Shape (seq_len, num_labels)\n",
        "         # Flatten the logits for SHAP explainer if explaining all outputs\n",
        "         # Or, select a specific output to explain (e.g., logit for a specific tag at a specific position)\n",
        "         return logits # Return (seq_len, num_labels) shape directly\n",
        "\n",
        "# SHAP explainer for a multi-output function (each token position * each label)\n",
        "# We need a background dataset for KernelExplainer, but for a simple masker,\n",
        "# the masker itself handles the perturbation.\n",
        "# Let's try using shap.Explainer directly with the WordMasker.\n",
        "\n",
        "# The base value for KernelExplainer is typically the average prediction over the background.\n",
        "# Since we are using a masker, the explainer might not require a background.\n",
        "# Let's try without a background dataset first.\n",
        "\n",
        "# Instantiate the Explainer with the single instance prediction function and masker\n",
        "# The explainer expects the predict_fn to take a numpy array-like input\n",
        "# The masker is designed to work with the original words, and it returns the dictionary format\n",
        "# expected by predict_fn_single_instance.\n",
        "# SHAP's Explainer is flexible and can handle this if the masker's output is correctly used\n",
        "# by the provided model/predict_fn.\n",
        "\n",
        "# We need to pass the original words to the explainer's shap_values method.\n",
        "# The masker will receive these words and the indices to mask.\n",
        "# The predict_fn will receive the output of the masker.\n",
        "\n",
        "# The output of predict_fn_single_instance is (seq_len, num_labels).\n",
        "# SHAP needs to know which output to explain.\n",
        "# We can explain all outputs simultaneously, resulting in SHAP values of shape (num_inputs, num_outputs).\n",
        "# Here, inputs are the original words, outputs are the token logits.\n",
        "\n",
        "# Let's try explaining the logits for all token positions and all labels.\n",
        "# The input to explainer.shap_values will be the original words list.\n",
        "# The masker will take this list and masked indices, tokenize, and return the dict.\n",
        "# The predict_fn will take the dict and return (seq_len, num_labels) logits.\n",
        "\n",
        "# Create the explainer\n",
        "explainer = shap.Explainer(predict_fn_single_instance, masker, output_names=label_list)\n",
        "\n",
        "# Generate SHAP values for the selected instance (original words)\n",
        "# This can take some time depending on the number of words and labels.\n",
        "# The output shape of shap_values will be (num_original_words, seq_len, num_labels)\n",
        "# This is because SHAP calculates the contribution of each input feature (word)\n",
        "# to each output feature (logit at each token position for each label).\n",
        "# This is potentially complex to visualize directly.\n",
        "\n",
        "# Let's refine the approach: explain the prediction for each *original word*\n",
        "# towards the *predicted tag* at the *corresponding token position*.\n",
        "# This means we need to explain the logit value for the predicted tag at each token position.\n",
        "\n",
        "# We need to define the target output indices for SHAP.\n",
        "# The target outputs are (token_position, label_index).\n",
        "# We can iterate through the token positions and explain the logit of the predicted tag at that position.\n",
        "\n",
        "# Let's explain the logit of the *predicted tag* for each token position.\n",
        "# The output index to explain for token i (which maps to word j) will be\n",
        "# (i, predicted_token_ids[i]).\n",
        "\n",
        "# This requires a different structure for the predict_fn if we only want to explain specific outputs.\n",
        "# Alternatively, we can compute SHAP for all outputs and then select the relevant values.\n",
        "# Let's compute for all (seq_len, num_labels) logits and then focus on the predicted tags.\n",
        "\n",
        "# This might still be too slow. Let's consider explaining the impact of words on the\n",
        "# overall prediction for a specific tag across the sentence, or the prediction\n",
        "# for a specific tag at a specific token position.\n",
        "\n",
        "# A more standard approach for token classification explanation is often:\n",
        "# For each original word, what is its contribution to the predicted tag at the token position(s)\n",
        "# it maps to?\n",
        "\n",
        "# Let's adjust the predict_fn to return only the logits corresponding to the\n",
        "# predicted tags at each token position.\n",
        "# This means the predict_fn output shape should be (seq_len,).\n",
        "# The explainer input will still be the original words list.\n",
        "\n",
        "def predict_predicted_logits(input_data_list):\n",
        "     \"\"\"\n",
        "     Prediction function for SHAP, takes a list containing one processed input dict,\n",
        "     returns the logits for the predicted tag at each token position.\n",
        "     Output shape: (seq_len,)\n",
        "     \"\"\"\n",
        "     if len(input_data_list) != 1:\n",
        "         raise ValueError(\"predict_predicted_logits expects a list with exactly one item.\")\n",
        "\n",
        "     item = input_data_list[0]\n",
        "     loaded_model.eval()\n",
        "     with torch.no_grad():\n",
        "         input_ids = item['input_ids'].to(loaded_model.device)\n",
        "         attention_mask = item['attention_mask'].to(loaded_model.device)\n",
        "         outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "         logits = outputs.logits.squeeze(0) # Shape (seq_len, num_labels)\n",
        "         predicted_token_ids = torch.argmax(logits, dim=1) # Shape (seq_len,)\n",
        "\n",
        "         # Gather the logits corresponding to the predicted tags\n",
        "         # Use advanced indexing: logits[i, predicted_token_ids[i]] for each i\n",
        "         predicted_logits = logits[torch.arange(logits.size(0)), predicted_token_ids]\n",
        "\n",
        "         return predicted_logits.cpu().numpy() # Shape (seq_len,)\n",
        "\n",
        "# Create the explainer with the new predict function\n",
        "# The output names for this predict_fn are less clear; they correspond to token positions.\n",
        "explainer_predicted = shap.Explainer(predict_predicted_logits, masker)\n",
        "\n",
        "# Generate SHAP values\n",
        "# The input to shap_values is the list of original words\n",
        "# The output shape will be (num_original_words, seq_len)\n",
        "shap_values_predicted = explainer_predicted.shap_values(original_words)\n",
        "\n",
        "# Visualize the SHAP values\n",
        "# We want to show which words contribute to the prediction at each token position.\n",
        "# The shap_values_predicted array has shape (num_original_words, seq_len).\n",
        "# Each row corresponds to an original word's contribution.\n",
        "# Each column corresponds to a token position.\n",
        "\n",
        "# This is still not ideal for visualizing word contributions to word predictions.\n",
        "# A better visualization would show the contribution of each *input word*\n",
        "# to the predicted tag of each *output word*.\n",
        "\n",
        "# Let's rethink the prediction function and explainer target.\n",
        "# Target: For each *original word*, explain its contribution to the *predicted tag*\n",
        "# of the *same original word* (or the token it maps to).\n",
        "\n",
        "# We need a predict_fn that returns a single value or a set of values\n",
        "# corresponding to the predictions for the original words.\n",
        "\n",
        "# Let's try explaining the logit of the predicted tag for each *original word*.\n",
        "# This requires mapping token predictions back to original words.\n",
        "\n",
        "def predict_word_logits(input_data_list):\n",
        "    \"\"\"\n",
        "    Prediction function for SHAP, takes a list containing one processed input dict,\n",
        "    returns the logits for the predicted tag at the first token position\n",
        "    corresponding to each original word.\n",
        "    Output shape: (num_original_words,)\n",
        "    \"\"\"\n",
        "    if len(input_data_list) != 1:\n",
        "        raise ValueError(\"predict_word_logits expects a list with exactly one item.\")\n",
        "\n",
        "    item = input_data_list[0]\n",
        "    loaded_model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = item['input_ids'].to(loaded_model.device)\n",
        "        attention_mask = item['attention_mask'].to(loaded_model.device)\n",
        "        outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits.squeeze(0) # Shape (seq_len, num_labels)\n",
        "        predicted_token_ids = torch.argmax(logits, dim=1) # Shape (seq_len,)\n",
        "\n",
        "        # Map token predictions back to original words and select one logit per word\n",
        "        word_logits = []\n",
        "        previous_word_idx = None\n",
        "        for token_idx, word_idx in enumerate(item['word_ids']):\n",
        "            if word_idx is not None and word_idx != previous_word_idx:\n",
        "                 # This is the first token of a new word\n",
        "                 # Get the logit for the predicted tag at this token position\n",
        "                 predicted_tag_id = predicted_token_ids[token_idx]\n",
        "                 logit_for_predicted_tag = logits[token_idx, predicted_tag_id]\n",
        "                 word_logits.append(logit_for_predicted_tag.item())\n",
        "                 previous_word_idx = word_idx\n",
        "            elif word_idx is None:\n",
        "                 # Handle special tokens or padding - ignore or assign a default?\n",
        "                 # For this explanation, we focus on original words.\n",
        "                 pass # Skip special tokens/padding\n",
        "            # If word_idx is the same as previous, it's a subtoken of the same word, we skip.\n",
        "\n",
        "        # This might not capture all words if some words have no corresponding tokens after truncation/padding.\n",
        "        # A more robust approach might involve averaging or pooling logits for subtokens of a word.\n",
        "        # For simplicity, we take the logit of the first token of the word.\n",
        "\n",
        "        # Ensure the length matches the number of original words\n",
        "        # This simple approach might have length mismatches due to tokenizer mapping.\n",
        "        # Let's use a list comprehension that aligns better with original words.\n",
        "\n",
        "        word_logits = []\n",
        "        current_word_logit = None\n",
        "        for word_idx in range(len(original_words)):\n",
        "            # Find the first token index corresponding to this word index\n",
        "            first_token_idx = -1\n",
        "            for token_idx, w_id in enumerate(item['word_ids']):\n",
        "                if w_id == word_idx:\n",
        "                    first_token_idx = token_idx\n",
        "                    break\n",
        "\n",
        "            if first_token_idx != -1:\n",
        "                predicted_tag_id = predicted_token_ids[first_token_idx]\n",
        "                logit_for_predicted_tag = logits[first_token_idx, predicted_tag_id]\n",
        "                word_logits.append(logit_for_predicted_tag.item())\n",
        "            else:\n",
        "                # Word was likely removed by truncation or tokenizer\n",
        "                word_logits.append(0.0) # Append a neutral value or skip\n",
        "\n",
        "        # Ensure the length matches the number of original words\n",
        "        # If length mismatch, there's an issue with the mapping or truncation.\n",
        "        # For now, proceed assuming reasonable mapping for simple sentences.\n",
        "        # A more robust solution would require careful alignment.\n",
        "\n",
        "        # Let's return the logits for the predicted tags at *all* token positions\n",
        "        # and rely on SHAP visualization to handle the word mapping.\n",
        "        # The predict_predicted_logits function already does this, returning (seq_len,)\n",
        "\n",
        "        return predict_predicted_logits(input_data_list)\n",
        "\n",
        "\n",
        "# Use the predict_predicted_logits function\n",
        "explainer_word_impact = shap.Explainer(predict_predicted_logits, masker) # Still using the masker on original words\n",
        "\n",
        "# Generate SHAP values for the selected instance (original words)\n",
        "# Input to shap_values is the original words list\n",
        "# Output shape will be (num_original_words, seq_len)\n",
        "# This means the contribution of each original word (rows) to the predicted logit at each token position (columns).\n",
        "shap_values_word_impact = explainer_word_impact.shap_values(original_words)\n",
        "\n",
        "# Visualize the SHAP values using the text plot\n",
        "# We need the original text/tokens and the SHAP values corresponding to them.\n",
        "# The shap_values_word_impact relates original words to token positions.\n",
        "# For visualization, we typically want to show the contribution of input tokens/words\n",
        "# to the output token/word prediction.\n",
        "\n",
        "# Let's try the shap.text_plot\n",
        "# It expects a list of words and the SHAP values for those words' contribution\n",
        "# to a single output or set of outputs.\n",
        "# Our shap_values_word_impact is (num_original_words, seq_len).\n",
        "# We need to aggregate this to show contributions to *word* predictions.\n",
        "\n",
        "# Let's simplify: explain the contribution of each *original word* to the\n",
        "# *average logit* of the predicted tag across all tokens corresponding to that word.\n",
        "# This requires a custom predict_fn again.\n",
        "\n",
        "def predict_avg_word_logit(input_data_list):\n",
        "    \"\"\"\n",
        "    Prediction function for SHAP, takes a list containing one processed input dict,\n",
        "    returns the average logit for the predicted tag for each original word.\n",
        "    Output shape: (num_original_words,)\n",
        "    \"\"\"\n",
        "    if len(input_data_list) != 1:\n",
        "        raise ValueError(\"predict_avg_word_logit expects a list with exactly one item.\")\n",
        "\n",
        "    item = input_data_list[0]\n",
        "    loaded_model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = item['input_ids'].to(loaded_model.device)\n",
        "        attention_mask = item['attention_mask'].to(loaded_model.device)\n",
        "        outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits.squeeze(0) # Shape (seq_len, num_labels)\n",
        "        predicted_token_ids = torch.argmax(logits, dim=1) # Shape (seq_len,)\n",
        "        word_ids = item['word_ids']\n",
        "\n",
        "        word_logit_sums = {}\n",
        "        word_token_counts = {}\n",
        "\n",
        "        for token_idx, word_idx in enumerate(word_ids):\n",
        "            if word_idx is not None:\n",
        "                predicted_tag_id = predicted_token_ids[token_idx]\n",
        "                logit_for_predicted_tag = logits[token_idx, predicted_tag_id].item()\n",
        "\n",
        "                if word_idx not in word_logit_sums:\n",
        "                    word_logit_sums[word_idx] = 0.0\n",
        "                    word_token_counts[word_idx] = 0\n",
        "                word_logit_sums[word_idx] += logit_for_predicted_tag\n",
        "                word_token_counts[word_idx] += 1\n",
        "\n",
        "        # Calculate average logit for each word\n",
        "        avg_word_logits = [\n",
        "            word_logit_sums.get(word_idx, 0.0) / word_token_counts.get(word_idx, 1.0)\n",
        "            for word_idx in range(len(original_words)) # Ensure we cover all original words\n",
        "        ]\n",
        "\n",
        "        return np.array(avg_word_logits)\n",
        "\n",
        "# Create the explainer with the new predict function\n",
        "explainer_avg_logit = shap.Explainer(predict_avg_word_logit, masker)\n",
        "\n",
        "# Generate SHAP values\n",
        "# Input to shap_values is the original words list\n",
        "# Output shape will be (num_original_words, num_original_words)\n",
        "# This represents the contribution of each input word (rows) to the average predicted logit of each output word (columns).\n",
        "# This is still not ideal for a simple text visualization showing input token importance for output token tags.\n",
        "\n",
        "# Let's go back to explaining the contribution of input words to the predicted tag at each token position.\n",
        "# This is the shap_values_word_impact = explainer_word_impact.shap_values(original_words) result.\n",
        "# Shape (num_original_words, seq_len).\n",
        "# We can visualize the contribution of input words to the prediction at a *specific token position*.\n",
        "\n",
        "# Let's pick a token position to explain, e.g., the first token of the first word.\n",
        "# We want to see which input words contribute to the prediction at token position 0.\n",
        "# The SHAP values for this would be shap_values_word_impact[:, 0].\n",
        "# This is a vector of length num_original_words.\n",
        "\n",
        "# Let's visualize the contributions of all original words to the predicted tag\n",
        "# at the token position corresponding to the first word.\n",
        "# We need the original words as text and the SHAP values vector.\n",
        "\n",
        "# Find the token index corresponding to the first original word (index 0)\n",
        "first_word_token_idx = -1\n",
        "for token_idx, w_id in enumerate(word_ids):\n",
        "    if w_id == 0:\n",
        "        first_word_token_idx = token_idx\n",
        "        break\n",
        "\n",
        "if first_word_token_idx != -1:\n",
        "    print(f\"\\nExplaining prediction at token position {first_word_token_idx} ('{sentence_tokens[first_word_token_idx]}')\")\n",
        "    # Get the SHAP values for the output at this token position\n",
        "    shap_values_for_token_pos = shap_values_word_impact[:, first_word_token_idx]\n",
        "\n",
        "    # Get the base value for this specific output (average prediction)\n",
        "    # For KernelExplainer with a masker, the base value is often the prediction of the masked input.\n",
        "    # SHAP provides the base value in explainer.expected_value\n",
        "    # If predict_fn returns (seq_len,), the expected_value might be (seq_len,).\n",
        "    # Let's check the explainer_word_impact.expected_value\n",
        "    base_value_for_token_pos = explainer_word_impact.expected_value[first_word_token_idx]\n",
        "\n",
        "    # Visualize using shap.text_plot\n",
        "    # text_plot expects list of words, shap_values, and optionally original_data and expected_value\n",
        "    shap.text_plot(shap_values_for_token_pos, original_words, original_data=original_words, expected_value=base_value_for_token_pos)\n",
        "\n",
        "else:\n",
        "    print(\"\\nCould not find token index for the first original word. Cannot visualize.\")\n",
        "\n",
        "# To visualize for *all* token positions, a loop or a more complex visualization is needed.\n",
        "# The standard shap.text_plot is designed for one output explanation.\n",
        "\n",
        "# Let's try explaining the contribution of each *token* to the predicted tag at that token's position.\n",
        "# This requires a masker that works on tokens and a predict_fn that returns (seq_len,).\n",
        "# The input to explainer.shap_values would be the list of tokens.\n",
        "\n",
        "# Create a masker that works on the token list\n",
        "token_masker = shap.maskers.Text(loaded_tokenizer.convert_ids_to_tokens(input_ids_tensor[0].tolist()), mask_token='[MASK]')\n",
        "\n",
        "# Use the predict_predicted_logits function which returns (seq_len,)\n",
        "# This function takes the processed input dict, so the masker needs to return that format.\n",
        "\n",
        "# Let's create a masker that takes a list of tokens and returns the processed dict.\n",
        "class TokenListMasker(shap.maskers.Text):\n",
        "    def __init__(self, tokenizer, original_tokens, **kwargs):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.original_tokens = original_tokens\n",
        "        super().__init__(mask_token='[MASK]', **kwargs)\n",
        "\n",
        "    def __call__(self, masked_indices, x):\n",
        "        # x is the original input passed to explainer.shap_values (the list of tokens)\n",
        "        # masked_indices tells us which token indices to mask.\n",
        "        masked_tokens = x[:] # Create a copy of the original tokens\n",
        "        for i in masked_indices:\n",
        "             if i < len(masked_tokens):\n",
        "                masked_tokens[i] = self.tokenizer.mask_token # Replace token with mask token\n",
        "\n",
        "        # Convert masked tokens back to string for tokenizer, or handle as list if tokenizer supports\n",
        "        # The tokenizer can handle a list of strings with is_split_into_words=True,\n",
        "        # but here our 'tokens' list already contains special tokens and subtokens.\n",
        "        # Re-tokenizing the list of strings might change the structure.\n",
        "\n",
        "        # A simpler approach for token-level masking might be to directly modify the input_ids tensor.\n",
        "        # However, SHAP maskers are typically designed to perturb the input format\n",
        "        # that is passed to the predict_fn.\n",
        "\n",
        "        # Let's stick to masking the original words for now, as it's often more interpretable\n",
        "        # in terms of word contributions.\n",
        "\n",
        "# Let's visualize the contribution of input words to the predicted tag\n",
        "# for the token position corresponding to a named entity if one is predicted.\n",
        "\n",
        "# Find the first predicted named entity token index and its tag\n",
        "first_entity_token_idx = -1\n",
        "first_entity_tag = None\n",
        "for token_idx, tag in enumerate(predicted_tags):\n",
        "    if tag != 'O' and tag != '[CLS]' and tag != '[SEP]':\n",
        "        first_entity_token_idx = token_idx\n",
        "        first_entity_tag = tag\n",
        "        break\n",
        "\n",
        "if first_entity_token_idx != -1:\n",
        "    print(f\"\\nExplaining prediction for tag '{first_entity_tag}' at token position {first_entity_token_idx} ('{sentence_tokens[first_entity_token_idx]}')\")\n",
        "\n",
        "    # We need SHAP values for the output corresponding to this specific tag at this token position.\n",
        "    # The shap_values_word_impact has shape (num_original_words, seq_len).\n",
        "    # It explains the contribution of original words to the predicted logit at each token position.\n",
        "\n",
        "    # If we want to explain the logit of a *specific* tag (not necessarily the predicted one at that position),\n",
        "    # we need a predict_fn that returns the logit for that specific tag at each token position.\n",
        "\n",
        "    def predict_specific_tag_logits(input_data_list):\n",
        "        \"\"\"\n",
        "        Prediction function for SHAP, takes a list containing one processed input dict,\n",
        "        returns the logits for a *specific* target tag at each token position.\n",
        "        The target tag index needs to be accessible (e.g., from a closure or global).\n",
        "        Output shape: (seq_len,)\n",
        "        \"\"\"\n",
        "        if len(input_data_list) != 1:\n",
        "            raise ValueError(\"predict_specific_tag_logits expects a list with exactly one item.\")\n",
        "\n",
        "        item = input_data_list[0]\n",
        "        loaded_model.eval()\n",
        "        with torch.no_grad():\n",
        "            input_ids = item['input_ids'].to(loaded_model.device)\n",
        "            attention_mask = item['attention_mask'].to(loaded_model.device)\n",
        "            outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits.squeeze(0) # Shape (seq_len, num_labels)\n",
        "\n",
        "            # We need the target tag ID here. Let's use the predicted tag ID for the first entity.\n",
        "            # This function would need to be created dynamically or accept the target tag ID.\n",
        "            # For demonstration, let's assume we want to explain the 'B-LOC' tag (ID 0).\n",
        "            target_tag_id = label2id.get('B-LOC', 0) # Default to 0 if not found\n",
        "\n",
        "            # Get the logits for the target tag across all token positions\n",
        "            specific_tag_logits = logits[:, target_tag_id]\n",
        "\n",
        "            return specific_tag_logits.cpu().numpy() # Shape (seq_len,)\n",
        "\n",
        "    # Instantiate the explainer for a specific tag (e.g., 'B-LOC')\n",
        "    # This needs to be done *outside* the loop if explaining different tags.\n",
        "    # Let's explain the 'B-LOC' tag for the first sentence.\n",
        "    target_tag_to_explain = 'B-LOC'\n",
        "    target_tag_id_to_explain = label2id.get(target_tag_to_explain, None)\n",
        "\n",
        "    if target_tag_id_to_explain is not None:\n",
        "        print(f\"\\nExplaining contribution to '{target_tag_to_explain}' tag logits across token positions.\")\n",
        "\n",
        "        # Need a predict_fn that returns logits for the target tag at each position\n",
        "        def predict_target_tag_logits(input_data_list):\n",
        "            if len(input_data_list) != 1:\n",
        "                raise ValueError(\"predict_target_tag_logits expects a list with exactly one item.\")\n",
        "\n",
        "            item = input_data_list[0]\n",
        "            loaded_model.eval()\n",
        "            with torch.no_grad():\n",
        "                input_ids = item['input_ids'].to(loaded_model.device)\n",
        "                attention_mask = item['attention_mask'].to(loaded_model.device)\n",
        "                outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits.squeeze(0) # Shape (seq_len, num_labels)\n",
        "\n",
        "                # Get the logits for the specific target tag ID\n",
        "                tag_logits = logits[:, target_tag_id_to_explain]\n",
        "\n",
        "                return tag_logits.cpu().numpy() # Shape (seq_len,)\n",
        "\n",
        "        # Create explainer for the target tag logits\n",
        "        explainer_target_tag = shap.Explainer(predict_target_tag_logits, masker)\n",
        "\n",
        "        # Generate SHAP values\n",
        "        # Input: original words. Output: logits for target tag at each token position.\n",
        "        # Output shape: (num_original_words, seq_len)\n",
        "        shap_values_target_tag = explainer_target_tag.shap_values(original_words)\n",
        "\n",
        "        # Visualize the contribution of original words to the logit of the target tag\n",
        "        # at a specific token position, e.g., the first token of the first word.\n",
        "        if first_word_token_idx != -1:\n",
        "            print(f\"\\nVisualizing contribution of words to '{target_tag_to_explain}' logit at token position {first_word_token_idx} ('{sentence_tokens[first_word_token_idx]}').\")\n",
        "            shap_values_for_target_output = shap_values_target_tag[:, first_word_token_idx]\n",
        "            base_value_for_target_output = explainer_target_tag.expected_value[first_word_token_idx]\n",
        "\n",
        "            shap.text_plot(shap_values_for_target_output, original_words, original_data=original_words, expected_value=base_value_for_target_output)\n",
        "        else:\n",
        "             print(\"\\nCould not find token index for the first original word. Cannot visualize for target tag.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\nTarget tag '{target_tag_to_explain}' not found in label list.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo non-'O' entities predicted in the sample sentence. Cannot visualize explanation for an entity tag.\")\n",
        "\n",
        "# This demonstrates how to generate SHAP values for word contributions to token logits\n",
        "# and visualize for a specific output position/tag.\n",
        "# Explaining all token predictions simultaneously or aggregating contributions\n",
        "# to word-level predictions requires more complex handling or visualization."
      ],
      "id": "86df33ec",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ·ä†·ãµ·à´·àª ·àò·åà·äì·äõ ·àò·à∞·à®·âµ ·ã∞·çã·à≠ ·àû·àç ·àÅ·àà·â∞·äõ ·çé·âÖ ·â¢·àÆ ·âÅ·å•·à≠ 205\n",
            "Tokens: ['<s>', '‚ñÅ·ä†·ãµ·à´·àª', '‚ñÅ·àò', '·åà·äì', '·äõ', '‚ñÅ·àò·à∞·à®·âµ', '‚ñÅ·ã∞', '·çã·à≠', '‚ñÅ·àû', '·àç', '‚ñÅ', '·àÅ·àà·â∞·äõ', '‚ñÅ', '·çé', '·âÖ', '‚ñÅ·â¢·àÆ', '‚ñÅ·âÅ·å•·à≠', '‚ñÅ205', '</s>']\n",
            "Predicted Tags: ['I-PRICE', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'I-PRICE', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'B-LOC', 'I-PRICE']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'PartitionExplainer' object has no attribute 'shap_values'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32-1725918926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;31m# The input to shap_values is the list of original words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m# The output shape will be (num_original_words, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m \u001b[0mshap_values_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer_predicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;31m# Visualize the SHAP values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PartitionExplainer' object has no attribute 'shap_values'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdb884f0"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the `PartitionExplainer` (which `shap.Explainer` defaults to with a masker) does not have a `shap_values` method. It likely uses `shap_values` as a property or has a different method name. According to SHAP documentation, `shap.Explainer` should return SHAP values directly when called with input data, or `explainer(data)` is the correct syntax. Let's correct the SHAP value generation calls.\n",
        "\n"
      ],
      "id": "fdb884f0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "YQ-I2STceVcK",
        "outputId": "962668a1-2ec2-4ad4-9553-6c7c7cf25618"
      },
      "source": [
        "import torch\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Select a sample sentence to explain (e.g., the first one)\n",
        "instance_index = 0\n",
        "explained_sample = processed_samples[instance_index]\n",
        "sentence_tokens = explained_sample[\"tokens\"]\n",
        "input_ids_tensor = explained_sample[\"input_ids\"]\n",
        "attention_mask_tensor = explained_sample[\"attention_mask\"]\n",
        "original_words = explained_sample[\"original_words\"]\n",
        "word_ids = explained_sample[\"word_ids\"]\n",
        "\n",
        "# Prepare the input for the predict_fn\n",
        "# The predict_fn expects a list of dictionaries\n",
        "predict_input = [{'input_ids': input_ids_tensor, 'attention_mask': attention_mask_tensor}]\n",
        "\n",
        "# Get the model's prediction for this instance\n",
        "# This is needed to identify the predicted tags to explain\n",
        "with torch.no_grad():\n",
        "    outputs = loaded_model(input_ids=input_ids_tensor.to(loaded_model.device),\n",
        "                           attention_mask=attention_mask_tensor.to(loaded_model.device))\n",
        "    logits = outputs.logits\n",
        "    predicted_token_ids = torch.argmax(logits, dim=2).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Map predicted token IDs back to NER tags\n",
        "predicted_tags = [id2label.get(token_id, \"O\") for token_id in predicted_token_ids]\n",
        "\n",
        "print(f\"Original Sentence: {explained_sample['sentence']}\")\n",
        "print(f\"Tokens: {sentence_tokens}\")\n",
        "print(f\"Predicted Tags: {predicted_tags}\")\n",
        "\n",
        "\n",
        "# Create a custom masker that operates on the original words\n",
        "class WordMasker(shap.maskers.Text):\n",
        "    def __init__(self, tokenizer, original_words, **kwargs):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.original_words = original_words\n",
        "        super().__init__(mask_token='[MASK]', **kwargs)\n",
        "\n",
        "    def __call__(self, masked_indices, x):\n",
        "        masked_words = x[:]\n",
        "        for i in masked_indices:\n",
        "             if i < len(masked_words):\n",
        "                masked_words[i] = self.tokenizer.mask_token\n",
        "\n",
        "        tokenized_input = self.tokenizer(\n",
        "            masked_words,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            is_split_into_words=True\n",
        "        )\n",
        "        return [{'input_ids': tokenized_input[\"input_ids\"], 'attention_mask': tokenized_input[\"attention_mask\"]}]\n",
        "\n",
        "# Instantiate the custom masker using the original words of the selected sentence\n",
        "masker = WordMasker(loaded_tokenizer, original_words)\n",
        "\n",
        "# Redefine predict_fn to return logits for the predicted tag at each token position\n",
        "def predict_predicted_logits(input_data_list):\n",
        "     \"\"\"\n",
        "     Prediction function for SHAP, takes a list containing one processed input dict,\n",
        "     returns the logits for the predicted tag at each token position.\n",
        "     Output shape: (seq_len,)\n",
        "     \"\"\"\n",
        "     if len(input_data_list) != 1:\n",
        "         raise ValueError(\"predict_predicted_logits expects a list with exactly one item.\")\n",
        "\n",
        "     item = input_data_list[0]\n",
        "     loaded_model.eval()\n",
        "     with torch.no_grad():\n",
        "         input_ids = item['input_ids'].to(loaded_model.device)\n",
        "         attention_mask = item['attention_mask'].to(loaded_model.device)\n",
        "         outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "         logits = outputs.logits.squeeze(0) # Shape (seq_len, num_labels)\n",
        "         predicted_token_ids = torch.argmax(logits, dim=1) # Shape (seq_len,)\n",
        "\n",
        "         predicted_logits = logits[torch.arange(logits.size(0)), predicted_token_ids]\n",
        "\n",
        "         return predicted_logits.cpu().numpy() # Shape (seq_len,)\n",
        "\n",
        "# Create the explainer with the predict_predicted_logits function and masker\n",
        "explainer_predicted = shap.Explainer(predict_predicted_logits, masker)\n",
        "\n",
        "# Generate SHAP values using the correct syntax: explainer(data)\n",
        "# Input to explainer is the list of original words\n",
        "# The output of explainer will be a SHAP values object.\n",
        "shap_values_predicted = explainer_predicted(original_words)\n",
        "\n",
        "# Visualize the SHAP values\n",
        "# We want to show which words contribute to the prediction at each token position.\n",
        "# The shap_values_predicted object contains the SHAP values.\n",
        "# For a text plot, we need the input text (original words) and the SHAP values.\n",
        "# The shap_values_predicted.values should have shape (num_original_words, seq_len)\n",
        "# We can visualize the contributions of all original words to the predicted tag\n",
        "# at a specific token position, e.g., the first token of the first word.\n",
        "\n",
        "# Find the token index corresponding to the first original word (index 0)\n",
        "first_word_token_idx = -1\n",
        "for token_idx, w_id in enumerate(word_ids):\n",
        "    if w_id == 0:\n",
        "        first_word_token_idx = token_idx\n",
        "        break\n",
        "\n",
        "if first_word_token_idx != -1:\n",
        "    print(f\"\\nExplaining prediction at token position {first_word_token_idx} ('{sentence_tokens[first_word_token_idx]}')\")\n",
        "\n",
        "    # Get the SHAP values for the output at this token position\n",
        "    # shap_values_predicted.values has shape (num_original_words, seq_len)\n",
        "    shap_values_for_token_pos = shap_values_predicted.values[:, first_word_token_idx]\n",
        "\n",
        "    # Get the base value for this specific output\n",
        "    # explainer_predicted.expected_value should be a numpy array of shape (seq_len,)\n",
        "    base_value_for_token_pos = explainer_predicted.expected_value[first_word_token_idx]\n",
        "\n",
        "    # Visualize using shap.text_plot\n",
        "    shap.text_plot(shap_values_for_token_pos, original_words, original_data=original_words, expected_value=base_value_for_token_pos)\n",
        "\n",
        "else:\n",
        "    print(\"\\nCould not find token index for the first original word. Cannot visualize.\")\n",
        "\n",
        "# Let's also try explaining the contribution of input words to the predicted tag\n",
        "# for the token position corresponding to a named entity if one is predicted.\n",
        "\n",
        "# Find the first predicted non-'O' entity token index and its tag\n",
        "first_entity_token_idx = -1\n",
        "first_entity_tag = None\n",
        "for token_idx, tag in enumerate(predicted_tags):\n",
        "    if tag != 'O' and tag != '[CLS]' and tag != '[SEP]' and token_idx < len(sentence_tokens):\n",
        "        first_entity_token_idx = token_idx\n",
        "        first_entity_tag = tag\n",
        "        break\n",
        "\n",
        "if first_entity_token_idx != -1:\n",
        "    print(f\"\\nExplaining prediction for predicted tag '{first_entity_tag}' at token position {first_entity_token_idx} ('{sentence_tokens[first_entity_token_idx]}')\")\n",
        "\n",
        "    # Get the SHAP values for this specific token output position\n",
        "    shap_values_for_entity_token_pos = shap_values_predicted.values[:, first_entity_token_idx]\n",
        "    base_value_for_entity_token_pos = explainer_predicted.expected_value[first_entity_token_idx]\n",
        "\n",
        "    # Visualize using shap.text_plot\n",
        "    shap.text_plot(shap_values_for_entity_token_pos, original_words, original_data=original_words, expected_value=base_value_for_entity_token_pos)\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo non-'O' entities predicted in the sample sentence. Cannot visualize explanation for an entity tag.\")\n"
      ],
      "id": "YQ-I2STceVcK",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ·ä†·ãµ·à´·àª ·àò·åà·äì·äõ ·àò·à∞·à®·âµ ·ã∞·çã·à≠ ·àû·àç ·àÅ·àà·â∞·äõ ·çé·âÖ ·â¢·àÆ ·âÅ·å•·à≠ 205\n",
            "Tokens: ['<s>', '‚ñÅ·ä†·ãµ·à´·àª', '‚ñÅ·àò', '·åà·äì', '·äõ', '‚ñÅ·àò·à∞·à®·âµ', '‚ñÅ·ã∞', '·çã·à≠', '‚ñÅ·àû', '·àç', '‚ñÅ', '·àÅ·àà·â∞·äõ', '‚ñÅ', '·çé', '·âÖ', '‚ñÅ·â¢·àÆ', '‚ñÅ·âÅ·å•·à≠', '‚ñÅ205', '</s>']\n",
            "Predicted Tags: ['I-PRICE', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'I-PRICE', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'B-LOC', 'I-PRICE']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SimpleTokenizer' object has no attribute 'mask_token'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33-1958116840.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Input to explainer is the list of original words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# The output of explainer will be a SHAP values object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mshap_values_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer_predicted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Visualize the SHAP values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_partition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m    171\u001b[0m     ):\n\u001b[1;32m    172\u001b[0m         \u001b[0;34m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         return super().__call__(\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_args\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" explainer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             row_result = self.explain_row(\n\u001b[0m\u001b[1;32m    367\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mrow_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_partition.py\u001b[0m in \u001b[0;36mexplain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# if not fixed background or no base value assigned then compute base value for a row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_curr_base_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fixed_background\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             self._curr_base_value = fm(m00.reshape(1, -1), zero_index=0)[\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             ]  # the zero index param tells the masked model what the baseline is\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_masking_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_full_masking_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m_full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mmasked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mmasked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# get a copy that won't get overwritten by the next iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-33-1958116840.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, masked_indices, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmasked_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m              \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mmasked_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         tokenized_input = self.tokenizer(\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SimpleTokenizer' object has no attribute 'mask_token'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5236238e"
      },
      "source": [
        "**Reasoning**:\n",
        "The error \"AttributeError: 'SimpleTokenizer' object has no attribute 'mask_token'\" indicates that the `shap.maskers.Text` base class, which our `WordMasker` inherits from, might be using a default tokenizer internally that doesn't have a `mask_token` attribute, or it's not correctly using the tokenizer provided to the `WordMasker`. A workaround is to explicitly use the mask token from our `loaded_tokenizer` when replacing words in the `WordMasker.__call__` method, rather than relying on the base class's potential internal tokenizer or mask token handling.\n",
        "\n"
      ],
      "id": "5236238e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "901PROgGea06",
        "outputId": "9ed6239c-94c4-4c5c-8a9b-a880d2dd9360"
      },
      "source": [
        "import torch\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Select a sample sentence to explain (e.g., the first one)\n",
        "instance_index = 0\n",
        "explained_sample = processed_samples[instance_index]\n",
        "sentence_tokens = explained_sample[\"tokens\"]\n",
        "input_ids_tensor = explained_sample[\"input_ids\"]\n",
        "attention_mask_tensor = explained_sample[\"attention_mask\"]\n",
        "original_words = explained_sample[\"original_words\"]\n",
        "word_ids = explained_sample[\"word_ids\"]\n",
        "\n",
        "# Prepare the input for the predict_fn\n",
        "# The predict_fn expects a list of dictionaries\n",
        "predict_input = [{'input_ids': input_ids_tensor, 'attention_mask': attention_mask_tensor}]\n",
        "\n",
        "# Get the model's prediction for this instance\n",
        "# This is needed to identify the predicted tags to explain\n",
        "with torch.no_grad():\n",
        "    outputs = loaded_model(input_ids=input_ids_tensor.to(loaded_model.device),\n",
        "                           attention_mask=attention_mask_tensor.to(loaded_model.device))\n",
        "    logits = outputs.logits\n",
        "    predicted_token_ids = torch.argmax(logits, dim=2).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Map predicted token IDs back to NER tags\n",
        "predicted_tags = [id2label.get(token_id, \"O\") for token_idx, token_id in enumerate(predicted_token_ids) if word_ids[token_idx] is not None]\n",
        "\n",
        "# Align predicted tags with original words for easier interpretation\n",
        "aligned_predicted_tags = []\n",
        "previous_word_idx = None\n",
        "for token_idx, word_idx in enumerate(word_ids):\n",
        "    if word_idx is not None and word_idx != previous_word_idx:\n",
        "        # This is the first token of a new word\n",
        "        aligned_predicted_tags.append(id2label.get(predicted_token_ids[token_idx], \"O\"))\n",
        "        previous_word_idx = word_idx\n",
        "    elif word_idx is None:\n",
        "        # Handle special tokens or padding - ignore or assign a placeholder\n",
        "        pass\n",
        "    # If word_idx is the same as previous, it's a subtoken, we skip for word-level alignment\n",
        "\n",
        "# If there's a mismatch between original_words and aligned_predicted_tags due to\n",
        "# complex tokenization/truncation, handle appropriately. For now, assume simple alignment.\n",
        "# A more robust approach would be needed for production.\n",
        "\n",
        "print(f\"Original Sentence: {' '.join(original_words)}\")\n",
        "print(f\"Tokens: {sentence_tokens}\")\n",
        "print(f\"Predicted Tags (aligned to first token of word): {aligned_predicted_tags}\")\n",
        "\n",
        "\n",
        "# Create a custom masker that operates on the original words\n",
        "class WordMasker(shap.maskers.Text):\n",
        "    def __init__(self, tokenizer, original_words, **kwargs):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.original_words = original_words\n",
        "        # Pass the original words list to the Text masker constructor\n",
        "        # It uses this to determine the structure to mask.\n",
        "        super().__init__(self.original_words, mask_token=tokenizer.mask_token, **kwargs)\n",
        "\n",
        "\n",
        "    def __call__(self, masked_indices, x):\n",
        "        # x is the original input passed to explainer.shap_values (the list of original_words)\n",
        "        # masked_indices tells us which word indices to mask.\n",
        "        masked_words = x[:] # Create a copy of the original words\n",
        "        for i in masked_indices:\n",
        "             if i < len(masked_words):\n",
        "                # Use the tokenizer's mask token directly\n",
        "                masked_words[i] = self.tokenizer.mask_token\n",
        "\n",
        "        tokenized_input = self.tokenizer(\n",
        "            masked_words,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            is_split_into_words=True\n",
        "        )\n",
        "        return [{'input_ids': tokenized_input[\"input_ids\"], 'attention_mask': tokenized_input[\"attention_mask\"]}]\n",
        "\n",
        "# Instantiate the custom masker using the original words of the selected sentence\n",
        "masker = WordMasker(loaded_tokenizer, original_words)\n",
        "\n",
        "# Redefine predict_fn to return logits for the predicted tag at each token position\n",
        "# This function is used by SHAP to get the model's output for masked inputs.\n",
        "def predict_predicted_logits(input_data_list):\n",
        "     \"\"\"\n",
        "     Prediction function for SHAP, takes a list containing one processed input dict,\n",
        "     returns the logits for the predicted tag at each token position.\n",
        "     Output shape: (seq_len,)\n",
        "     \"\"\"\n",
        "     if len(input_data_list) != 1:\n",
        "         raise ValueError(\"predict_predicted_logits expects a list with exactly one item.\")\n",
        "\n",
        "     item = input_data_list[0]\n",
        "     loaded_model.eval()\n",
        "     with torch.no_grad():\n",
        "         input_ids = item['input_ids'].to(loaded_model.device)\n",
        "         attention_mask = item['attention_mask'].to(loaded_model.device)\n",
        "         outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "         logits = outputs.logits.squeeze(0) # Shape (seq_len, num_labels)\n",
        "         predicted_token_ids = torch.argmax(logits, dim=1) # Shape (seq_len,)\n",
        "\n",
        "         predicted_logits = logits[torch.arange(logits.size(0)), predicted_token_ids]\n",
        "\n",
        "         return predicted_logits.cpu().numpy() # Shape (seq_len,)\n",
        "\n",
        "# Create the explainer with the predict_predicted_logits function and masker\n",
        "explainer_predicted = shap.Explainer(predict_predicted_logits, masker)\n",
        "\n",
        "# Generate SHAP values using the correct syntax: explainer(data)\n",
        "# Input to explainer is the list of original words\n",
        "# The output of explainer will be a SHAP values object.\n",
        "# The .values attribute will contain the SHAP values.\n",
        "shap_values_predicted = explainer_predicted(original_words)\n",
        "\n",
        "# Visualize the SHAP values\n",
        "# We want to show which words contribute to the prediction at each token position.\n",
        "# The shap_values_predicted.values has shape (num_original_words, seq_len)\n",
        "# We can visualize the contributions of all original words to the predicted tag\n",
        "# at a specific token position.\n",
        "\n",
        "# Find the token index corresponding to the first original word (index 0)\n",
        "first_word_token_idx = -1\n",
        "for token_idx, w_id in enumerate(word_ids):\n",
        "    if w_id == 0:\n",
        "        first_word_token_idx = token_idx\n",
        "        break\n",
        "\n",
        "if first_word_token_idx != -1:\n",
        "    # Ensure the token index is within the bounds of the SHAP values array's second dimension\n",
        "    if first_word_token_idx < shap_values_predicted.values.shape[1]:\n",
        "        print(f\"\\nExplaining prediction at token position {first_word_token_idx} ('{sentence_tokens[first_word_token_idx]}')\")\n",
        "\n",
        "        # Get the SHAP values for the output at this token position\n",
        "        # shap_values_predicted.values has shape (num_original_words, seq_len)\n",
        "        shap_values_for_token_pos = shap_values_predicted.values[:, first_word_token_idx]\n",
        "\n",
        "        # Get the base value for this specific output\n",
        "        # explainer_predicted.expected_value should be a numpy array of shape (seq_len,)\n",
        "        base_value_for_token_pos = explainer_predicted.expected_value[first_word_token_idx]\n",
        "\n",
        "        # Visualize using shap.text_plot\n",
        "        shap.text_plot(shap_values_for_token_pos, original_words, original_data=original_words, expected_value=base_value_for_token_pos)\n",
        "    else:\n",
        "         print(f\"\\nToken index {first_word_token_idx} is out of bounds for SHAP values with shape {shap_values_predicted.values.shape}. Cannot visualize for first word.\")\n",
        "else:\n",
        "    print(\"\\nCould not find token index for the first original word. Cannot visualize.\")\n",
        "\n",
        "# Let's also try explaining the contribution of input words to the predicted tag\n",
        "# for the token position corresponding to a named entity if one is predicted.\n",
        "\n",
        "# Find the first predicted non-'O' entity token index and its tag\n",
        "first_entity_token_idx = -1\n",
        "first_entity_tag = None\n",
        "for token_idx, tag in enumerate(predicted_tags):\n",
        "    # Check if the token index corresponds to an original word index to avoid special tokens\n",
        "    if word_ids[token_idx] is not None and tag != 'O':\n",
        "        first_entity_token_idx = token_idx\n",
        "        first_entity_tag = tag\n",
        "        break\n",
        "\n",
        "\n",
        "if first_entity_token_idx != -1:\n",
        "    # Ensure the token index is within the bounds of the SHAP values array's second dimension\n",
        "     if first_entity_token_idx < shap_values_predicted.values.shape[1]:\n",
        "        print(f\"\\nExplaining prediction for predicted tag '{first_entity_tag}' at token position {first_entity_token_idx} ('{sentence_tokens[first_entity_token_idx]}')\")\n",
        "\n",
        "        # Get the SHAP values for this specific token output position\n",
        "        shap_values_for_entity_token_pos = shap_values_predicted.values[:, first_entity_token_idx]\n",
        "        base_value_for_entity_token_pos = explainer_predicted.expected_value[first_entity_token_idx]\n",
        "\n",
        "        # Visualize using shap.text_plot\n",
        "        shap.text_plot(shap_values_for_entity_token_pos, original_words, original_data=original_words, expected_value=base_value_for_entity_token_pos)\n",
        "\n",
        "     else:\n",
        "         print(f\"\\nFirst entity token index {first_entity_token_idx} is out of bounds for SHAP values with shape {shap_values_predicted.values.shape}. Cannot visualize for entity.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo non-'O' entities predicted in the sample sentence. Cannot visualize explanation for an entity tag.\")\n",
        "\n",
        "# This demonstrates how to generate SHAP values for word contributions to token logits\n",
        "# and visualize for a specific output position/tag.\n",
        "# Explaining all token predictions simultaneously or aggregating contributions\n",
        "# to word-level predictions requires more complex handling or visualization."
      ],
      "id": "901PROgGea06",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ·ä†·ãµ·à´·àª ·àò·åà·äì·äõ ·àò·à∞·à®·âµ ·ã∞·çã·à≠ ·àû·àç ·àÅ·àà·â∞·äõ ·çé·âÖ ·â¢·àÆ ·âÅ·å•·à≠ 205\n",
            "Tokens: ['<s>', '‚ñÅ·ä†·ãµ·à´·àª', '‚ñÅ·àò', '·åà·äì', '·äõ', '‚ñÅ·àò·à∞·à®·âµ', '‚ñÅ·ã∞', '·çã·à≠', '‚ñÅ·àû', '·àç', '‚ñÅ', '·àÅ·àà·â∞·äõ', '‚ñÅ', '·çé', '·âÖ', '‚ñÅ·â¢·àÆ', '‚ñÅ·âÅ·å•·à≠', '‚ñÅ205', '</s>']\n",
            "Predicted Tags (aligned to first token of word): ['B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'B-LOC']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "The passed tokenizer cannot be wrapped as a masker because it does not have a __call__ method, not can it be interpreted as a splitting regexp!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/maskers/_text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, mask_token, collapse_mask_token, output_type)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/maskers/_text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split_pattern)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;34m\"\"\"Create a tokenizer based on a simple splitting pattern.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/re/__init__.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;34m\"Compile a regular expression pattern, returning a Pattern object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/re/__init__.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-34-2939042803.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Instantiate the custom masker using the original words of the selected sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mmasker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordMasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Redefine predict_fn to return logits for the predicted tag at each token position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-34-2939042803.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, original_words, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Pass the original words list to the Text masker constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# It uses this to determine the structure to mask.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/maskers/_text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, mask_token, collapse_mask_token, output_type)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 raise Exception(\n\u001b[0m\u001b[1;32m     55\u001b[0m                     \u001b[0;34m\"The passed tokenizer cannot be wrapped as a masker because it does not have a __call__ \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0;34m\"method, not can it be interpreted as a splitting regexp!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The passed tokenizer cannot be wrapped as a masker because it does not have a __call__ method, not can it be interpreted as a splitting regexp!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dca5951f"
      },
      "source": [
        "**Reasoning**:\n",
        "The error \"Exception: The passed tokenizer cannot be wrapped as a masker because it does not have a __call__ method, not can it be interpreted as a splitting regexp!\" occurs because `shap.maskers.Text` expects the first argument to be either a callable tokenizer or a string (for regexp splitting), but we are passing a list of strings (`self.original_words`). The `mask_token` argument is also causing an issue within the `super().__init__` call. The `shap.maskers.Text` is designed to handle text input and apply masking based on its internal logic or a provided tokenizer. Our custom `WordMasker` intends to mask at the word level of the *original* words, but the `shap.maskers.Text` constructor isn't correctly initialized for this use case when given a list of words directly as the first argument.\n",
        "\n",
        "A better approach for word-level masking with `shap.maskers.Text` when starting from original words is to initialize the masker with a simple string representation of the text and then override the `__call__` method to work with our list of words and tokenizer. However, the `Text` masker's internal mechanics might still interfere.\n",
        "\n",
        "A more direct way to achieve word-level masking compatible with our `predict_fn` is to create a masker that directly takes the list of original words and a tokenizer, and whose `__call__` method generates the masked tokenized input dictionaries. We don't necessarily need to inherit from `shap.maskers.Text` if its structure is incompatible. Let's create a custom masker class that is specifically designed to work with our list of original words and produce the input format required by `predict_predicted_logits`.\n",
        "\n"
      ],
      "id": "dca5951f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Nsgi_dKveiEU",
        "outputId": "9d08ae5c-c583-4118-ddef-4557544bbe3b"
      },
      "source": [
        "import torch\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Select a sample sentence to explain (e.g., the first one)\n",
        "instance_index = 0\n",
        "explained_sample = processed_samples[instance_index]\n",
        "sentence_tokens = explained_sample[\"tokens\"]\n",
        "input_ids_tensor = explained_sample[\"input_ids\"]\n",
        "attention_mask_tensor = explained_sample[\"attention_mask\"]\n",
        "original_words = explained_sample[\"original_words\"]\n",
        "word_ids = explained_sample[\"word_ids\"]\n",
        "\n",
        "# Get the model's prediction for this instance to identify predicted tags\n",
        "with torch.no_grad():\n",
        "    outputs = loaded_model(input_ids=input_ids_tensor.to(loaded_model.device),\n",
        "                           attention_mask=attention_tensor.to(loaded_model.device))\n",
        "    logits = outputs.logits\n",
        "    predicted_token_ids = torch.argmax(logits, dim=2).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Map predicted token IDs back to NER tags for alignment with original words\n",
        "aligned_predicted_tags = []\n",
        "previous_word_idx = None\n",
        "for token_idx, word_idx in enumerate(word_ids):\n",
        "    if word_idx is not None and word_idx != previous_word_idx:\n",
        "        # This is the first token of a new word\n",
        "        if token_idx < len(predicted_token_ids): # Ensure index is within bounds\n",
        "            aligned_predicted_tags.append(id2label.get(predicted_token_ids[token_idx], \"O\"))\n",
        "        else:\n",
        "            aligned_predicted_tags.append(\"O\") # Handle potential index out of bounds\n",
        "        previous_word_idx = word_idx\n",
        "    elif word_idx is None:\n",
        "        pass # Skip special tokens or padding\n",
        "\n",
        "# Ensure aligned_predicted_tags has the same length as original_words\n",
        "# This is a simple heuristic; a more robust alignment might be needed for complex cases.\n",
        "# If lengths mismatch, truncate the longer one or pad the shorter one.\n",
        "min_len = min(len(original_words), len(aligned_predicted_tags))\n",
        "original_words_aligned = original_words[:min_len]\n",
        "aligned_predicted_tags_aligned = aligned_predicted_tags[:min_len]\n",
        "\n",
        "print(f\"Original Sentence: {' '.join(original_words)}\")\n",
        "print(f\"Tokens: {sentence_tokens}\")\n",
        "print(f\"Predicted Tags (aligned to first token of word, heuristic): {aligned_predicted_tags_aligned}\")\n",
        "\n",
        "\n",
        "# Create a custom masker that operates on the original words and returns tokenized input dicts\n",
        "class CustomWordMasker(shap.maskers.Masker):\n",
        "    def __init__(self, tokenizer, original_words):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.original_words = original_words\n",
        "        # The shape of the masker should correspond to the structure being masked, which is the list of original words.\n",
        "        self.shape = (len(original_words),)\n",
        "        # The masker should indicate the data type it operates on\n",
        "        self.input_mask_value = tokenizer.mask_token # Use the mask token as the mask value indicator\n",
        "\n",
        "    def __call__(self, mask, x):\n",
        "        # mask is a boolean or integer array indicating which words to mask.\n",
        "        # x is the original input (the list of original_words).\n",
        "        masked_words = x[:] # Create a copy of the original words\n",
        "\n",
        "        # Apply the mask: replace masked words with the mask token\n",
        "        for i in range(len(mask)):\n",
        "            if mask[i] == 0: # Assuming mask is 0 for masked, 1 for not masked (or boolean False/True)\n",
        "                 if i < len(masked_words):\n",
        "                    masked_words[i] = self.tokenizer.mask_token\n",
        "\n",
        "        # Tokenize the masked words and format for the predict_fn\n",
        "        tokenized_input = self.tokenizer(\n",
        "            masked_words,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length', # Pad to max length for consistency\n",
        "            truncation=True,\n",
        "            is_split_into_words=True\n",
        "        )\n",
        "        # Return the list of dictionaries expected by predict_fn\n",
        "        # SHAP's masker expects to return a single masked version.\n",
        "        # Our predict_fn expects a list containing one such dictionary.\n",
        "        return {'input_ids': tokenized_input[\"input_ids\"], 'attention_mask': tokenized_input[\"attention_mask\"]}\n",
        "\n",
        "# Instantiate the custom masker\n",
        "masker = CustomWordMasker(loaded_tokenizer, original_words)\n",
        "\n",
        "# Use the predict_predicted_logits function (defined in the previous turn)\n",
        "# It takes a list containing one processed input dict and returns (seq_len,) logits.\n",
        "\n",
        "# Create the explainer with the predict_predicted_logits function and custom masker\n",
        "# The predict_fn needs to accept the output format of the masker.\n",
        "# The masker returns a single dict, but predict_predicted_logits expects a list of dicts.\n",
        "# Let's adjust predict_predicted_logits or create a wrapper.\n",
        "\n",
        "def predict_wrapper(input_dict):\n",
        "    \"\"\"Wrapper function to make predict_predicted_logits compatible with masker output.\"\"\"\n",
        "    return predict_predicted_logits([input_dict])\n",
        "\n",
        "explainer_predicted = shap.Explainer(predict_wrapper, masker)\n",
        "\n",
        "# Generate SHAP values using the correct syntax: explainer(data)\n",
        "# Input to explainer is the list of original words.\n",
        "# The .values attribute will contain the SHAP values.\n",
        "# The shape of shap_values_predicted.values should be (num_original_words, seq_len)\n",
        "shap_values_predicted = explainer_predicted(original_words)\n",
        "\n",
        "\n",
        "# Visualize the SHAP values for the first token position\n",
        "# Find the token index corresponding to the first original word (index 0)\n",
        "first_word_token_idx = -1\n",
        "for token_idx, w_id in enumerate(word_ids):\n",
        "    if w_id == 0:\n",
        "        first_word_token_idx = token_idx\n",
        "        break\n",
        "\n",
        "if first_word_token_idx != -1:\n",
        "    # Ensure the token index is within the bounds of the SHAP values array's second dimension\n",
        "    if first_word_token_idx < shap_values_predicted.values.shape[1]:\n",
        "        print(f\"\\nExplaining prediction at token position {first_word_token_idx} ('{sentence_tokens[first_word_token_idx]}')\")\n",
        "\n",
        "        # Get the SHAP values for the output at this token position\n",
        "        # shap_values_predicted.values has shape (num_original_words, seq_len)\n",
        "        shap_values_for_token_pos = shap_values_predicted.values[:, first_word_token_idx]\n",
        "\n",
        "        # Get the base value for this specific output\n",
        "        # explainer_predicted.expected_value should be a numpy array of shape (seq_len,)\n",
        "        base_value_for_token_pos = explainer_predicted.expected_value[first_word_token_idx]\n",
        "\n",
        "        # Visualize using shap.text_plot\n",
        "        shap.text_plot(shap_values_for_token_pos, original_words, original_data=original_words, expected_value=base_value_for_token_pos)\n",
        "    else:\n",
        "         print(f\"\\nToken index {first_word_token_idx} is out of bounds for SHAP values with shape {shap_values_predicted.values.shape}. Cannot visualize for first word.\")\n",
        "else:\n",
        "    print(\"\\nCould not find token index for the first original word. Cannot visualize.\")\n",
        "\n",
        "# Visualize the SHAP values for the first predicted entity token position\n",
        "first_entity_token_idx = -1\n",
        "first_entity_tag = None\n",
        "for token_idx, tag in enumerate(predicted_tags):\n",
        "    # Check if the token index corresponds to an original word index to avoid special tokens\n",
        "    if token_idx < len(word_ids) and word_ids[token_idx] is not None and tag != 'O':\n",
        "        first_entity_token_idx = token_idx\n",
        "        first_entity_tag = tag\n",
        "        break\n",
        "\n",
        "if first_entity_token_idx != -1:\n",
        "    # Ensure the token index is within the bounds of the SHAP values array's second dimension\n",
        "     if first_entity_token_idx < shap_values_predicted.values.shape[1]:\n",
        "        print(f\"\\nExplaining prediction for predicted tag '{first_entity_tag}' at token position {first_entity_token_idx} ('{sentence_tokens[first_entity_token_idx]}')\")\n",
        "\n",
        "        # Get the SHAP values for this specific token output position\n",
        "        shap_values_for_entity_token_pos = shap_values_predicted.values[:, first_entity_token_idx]\n",
        "        base_value_for_entity_token_pos = explainer_predicted.expected_value[first_entity_token_idx]\n",
        "\n",
        "        # Visualize using shap.text_plot\n",
        "        shap.text_plot(shap_values_for_entity_token_pos, original_words, original_data=original_words, expected_value=base_value_for_entity_token_pos)\n",
        "\n",
        "     else:\n",
        "         print(f\"\\nFirst entity token index {first_entity_token_idx} is out of bounds for SHAP values with shape {shap_values_predicted.values.shape}. Cannot visualize for entity.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo non-'O' entities predicted in the sample sentence. Cannot visualize explanation for an entity tag.\")\n",
        "\n",
        "# This demonstrates how to generate SHAP values for word contributions to token logits\n",
        "# and visualize for a specific output position/tag.\n",
        "# Explaining all token predictions simultaneously or aggregating contributions\n",
        "# to word-level predictions requires more complex handling or visualization.\n"
      ],
      "id": "Nsgi_dKveiEU",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'attention_tensor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-35-3738559065.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     outputs = loaded_model(input_ids=input_ids_tensor.to(loaded_model.device),\n\u001b[0;32m---> 17\u001b[0;31m                            attention_mask=attention_tensor.to(loaded_model.device))\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpredicted_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'attention_tensor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2229ec5"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `NameError: name 'attention_tensor' is not defined` is due to a typo. It should be `attention_mask_tensor`. Let's correct this variable name in the code.\n",
        "\n"
      ],
      "id": "f2229ec5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "U0OQjhsxemZb",
        "outputId": "36ad14a9-6351-4f95-fa9b-8403a07e5121"
      },
      "source": [
        "import torch\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Select a sample sentence to explain (e.g., the first one)\n",
        "instance_index = 0\n",
        "explained_sample = processed_samples[instance_index]\n",
        "sentence_tokens = explained_sample[\"tokens\"]\n",
        "input_ids_tensor = explained_sample[\"input_ids\"]\n",
        "attention_mask_tensor = explained_sample[\"attention_mask\"]\n",
        "original_words = explained_sample[\"original_words\"]\n",
        "word_ids = explained_sample[\"word_ids\"]\n",
        "\n",
        "# Get the model's prediction for this instance to identify predicted tags\n",
        "with torch.no_grad():\n",
        "    outputs = loaded_model(input_ids=input_ids_tensor.to(loaded_model.device),\n",
        "                           attention_mask=attention_mask_tensor.to(loaded_model.device)) # Corrected variable name\n",
        "    logits = outputs.logits\n",
        "    predicted_token_ids = torch.argmax(logits, dim=2).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Map predicted token IDs back to NER tags for alignment with original words\n",
        "aligned_predicted_tags = []\n",
        "previous_word_idx = None\n",
        "for token_idx, word_idx in enumerate(word_ids):\n",
        "    if word_idx is not None and word_idx != previous_word_idx:\n",
        "        # This is the first token of a new word\n",
        "        if token_idx < len(predicted_token_ids): # Ensure index is within bounds\n",
        "            aligned_predicted_tags.append(id2label.get(predicted_token_ids[token_idx], \"O\"))\n",
        "        else:\n",
        "            aligned_predicted_tags.append(\"O\") # Handle potential index out of bounds\n",
        "        previous_word_idx = word_idx\n",
        "    elif word_idx is None:\n",
        "        pass # Skip special tokens or padding\n",
        "\n",
        "# Ensure aligned_predicted_tags has the same length as original_words\n",
        "# This is a simple heuristic; a more robust alignment might be needed for complex cases.\n",
        "# If lengths mismatch, truncate the longer one or pad the shorter one.\n",
        "min_len = min(len(original_words), len(aligned_predicted_tags))\n",
        "original_words_aligned = original_words[:min_len]\n",
        "aligned_predicted_tags_aligned = aligned_predicted_tags[:min_len]\n",
        "\n",
        "print(f\"Original Sentence: {' '.join(original_words)}\")\n",
        "print(f\"Tokens: {sentence_tokens}\")\n",
        "print(f\"Predicted Tags (aligned to first token of word, heuristic): {aligned_predicted_tags_aligned}\")\n",
        "\n",
        "\n",
        "# Create a custom masker that operates on the original words and returns tokenized input dicts\n",
        "class CustomWordMasker(shap.maskers.Masker):\n",
        "    def __init__(self, tokenizer, original_words):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.original_words = original_words\n",
        "        # The shape of the masker should correspond to the structure being masked, which is the list of original words.\n",
        "        self.shape = (len(original_words),)\n",
        "        # The masker should indicate the data type it operates on\n",
        "        self.input_mask_value = tokenizer.mask_token # Use the mask token as the mask value indicator\n",
        "\n",
        "    def __call__(self, mask, x):\n",
        "        # mask is a boolean or integer array indicating which words to mask.\n",
        "        # x is the original input (the list of original_words).\n",
        "        masked_words = x[:] # Create a copy of the original words\n",
        "\n",
        "        # Apply the mask: replace masked words with the mask token\n",
        "        # SHAP mask is boolean: True for keep, False for mask\n",
        "        for i in range(len(mask)):\n",
        "            if not mask[i]: # If mask is False (meaning mask this word)\n",
        "                 if i < len(masked_words):\n",
        "                    masked_words[i] = self.tokenizer.mask_token\n",
        "\n",
        "        # Tokenize the masked words and format for the predict_fn\n",
        "        tokenized_input = self.tokenizer(\n",
        "            masked_words,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length', # Pad to max length for consistency\n",
        "            truncation=True,\n",
        "            is_split_into_words=True\n",
        "        )\n",
        "        # Return the list of dictionaries expected by predict_fn\n",
        "        # SHAP's masker expects to return a single masked version.\n",
        "        # Our predict_fn expects a list containing one such dictionary.\n",
        "        return {'input_ids': tokenized_input[\"input_ids\"], 'attention_mask': tokenized_input[\"attention_mask\"]}\n",
        "\n",
        "# Instantiate the custom masker\n",
        "masker = CustomWordMasker(loaded_tokenizer, original_words)\n",
        "\n",
        "# Redefine predict_fn to return logits for the predicted tag at each token position\n",
        "# This function is used by SHAP to get the model's output for masked inputs.\n",
        "def predict_predicted_logits(input_data_list):\n",
        "     \"\"\"\n",
        "     Prediction function for SHAP, takes a list containing one processed input dict,\n",
        "     returns the logits for the predicted tag at each token position.\n",
        "     Output shape: (seq_len,)\n",
        "     \"\"\"\n",
        "     if len(input_data_list) != 1:\n",
        "         raise ValueError(\"predict_predicted_logits expects a list with exactly one item.\")\n",
        "\n",
        "     item = input_data_list[0]\n",
        "     loaded_model.eval()\n",
        "     with torch.no_grad():\n",
        "         input_ids = item['input_ids'].to(loaded_model.device)\n",
        "         attention_mask = item['attention_mask'].to(loaded_model.device)\n",
        "         outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "         logits = outputs.logits.squeeze(0) # Shape (seq_len, num_labels)\n",
        "         predicted_token_ids = torch.argmax(logits, dim=1) # Shape (seq_len,)\n",
        "\n",
        "         predicted_logits = logits[torch.arange(logits.size(0)), predicted_token_ids]\n",
        "\n",
        "         return predicted_logits.cpu().numpy() # Shape (seq_len,)\n",
        "\n",
        "# Create the explainer with the predict_predicted_logits function and custom masker\n",
        "# The predict_fn needs to accept the output format of the masker.\n",
        "# The masker returns a single dict, but predict_predicted_logits expects a list of dicts.\n",
        "# Let's adjust predict_predicted_logits or create a wrapper.\n",
        "\n",
        "def predict_wrapper(input_dict):\n",
        "    \"\"\"Wrapper function to make predict_predicted_logits compatible with masker output.\"\"\"\n",
        "    return predict_predicted_logits([input_dict])\n",
        "\n",
        "explainer_predicted = shap.Explainer(predict_wrapper, masker)\n",
        "\n",
        "# Generate SHAP values using the correct syntax: explainer(data)\n",
        "# Input to explainer is the list of original words.\n",
        "# The .values attribute will contain the SHAP values.\n",
        "# The shape of shap_values_predicted.values should be (num_original_words, seq_len)\n",
        "shap_values_predicted = explainer_predicted(original_words)\n",
        "\n",
        "\n",
        "# Visualize the SHAP values for the first token position\n",
        "# Find the token index corresponding to the first original word (index 0)\n",
        "first_word_token_idx = -1\n",
        "for token_idx, w_id in enumerate(word_ids):\n",
        "    if w_id == 0:\n",
        "        first_word_token_idx = token_idx\n",
        "        break\n",
        "\n",
        "if first_word_token_idx != -1:\n",
        "    # Ensure the token index is within the bounds of the SHAP values array's second dimension\n",
        "    if first_word_token_idx < shap_values_predicted.values.shape[1]:\n",
        "        print(f\"\\nExplaining prediction at token position {first_word_token_idx} ('{sentence_tokens[first_word_token_idx]}')\")\n",
        "\n",
        "        # Get the SHAP values for the output at this token position\n",
        "        # shap_values_predicted.values has shape (num_original_words, seq_len)\n",
        "        shap_values_for_token_pos = shap_values_predicted.values[:, first_word_token_idx]\n",
        "\n",
        "        # Get the base value for this specific output\n",
        "        # explainer_predicted.expected_value should be a numpy array of shape (seq_len,)\n",
        "        base_value_for_token_pos = explainer_predicted.expected_value[first_word_token_idx]\n",
        "\n",
        "        # Visualize using shap.text_plot\n",
        "        shap.text_plot(shap_values_for_token_pos, original_words, original_data=original_words, expected_value=base_value_for_token_pos)\n",
        "    else:\n",
        "         print(f\"\\nToken index {first_word_token_idx} is out of bounds for SHAP values with shape {shap_values_predicted.values.shape}. Cannot visualize for first word.\")\n",
        "else:\n",
        "    print(\"\\nCould not find token index for the first original word. Cannot visualize.\")\n",
        "\n",
        "# Visualize the SHAP values for the first predicted entity token position\n",
        "first_entity_token_idx = -1\n",
        "first_entity_tag = None\n",
        "for token_idx, tag in enumerate(predicted_tags):\n",
        "    # Check if the token index corresponds to an original word index to avoid special tokens\n",
        "    if token_idx < len(word_ids) and word_ids[token_idx] is not None and tag != 'O':\n",
        "        first_entity_token_idx = token_idx\n",
        "        first_entity_tag = tag\n",
        "        break\n",
        "\n",
        "if first_entity_token_idx != -1:\n",
        "    # Ensure the token index is within the bounds of the SHAP values array's second dimension\n",
        "     if first_entity_token_idx < shap_values_predicted.values.shape[1]:\n",
        "        print(f\"\\nExplaining prediction for predicted tag '{first_entity_tag}' at token position {first_entity_token_idx} ('{sentence_tokens[first_entity_token_idx]}')\")\n",
        "\n",
        "        # Get the SHAP values for this specific token output position\n",
        "        shap_values_for_entity_token_pos = shap_values_predicted.values[:, first_entity_token_idx]\n",
        "        base_value_for_entity_token_pos = explainer_predicted.expected_value[first_entity_token_idx]\n",
        "\n",
        "        # Visualize using shap.text_plot\n",
        "        shap.text_plot(shap_values_for_entity_token_pos, original_words, original_data=original_words, expected_value=base_value_for_entity_token_pos)\n",
        "\n",
        "     else:\n",
        "         print(f\"\\nFirst entity token index {first_entity_token_idx} is out of bounds for SHAP values with shape {shap_values_predicted.values.shape}. Cannot visualize for entity.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo non-'O' entities predicted in the sample sentence. Cannot visualize explanation for an entity tag.\")\n",
        "\n",
        "# This demonstrates how to generate SHAP values for word contributions to token logits\n",
        "# and visualize for a specific output position/tag.\n",
        "# Explaining all token predictions simultaneously or aggregating contributions\n",
        "# to word-level predictions requires more complex handling or visualization."
      ],
      "id": "U0OQjhsxemZb",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ·ä†·ãµ·à´·àª ·àò·åà·äì·äõ ·àò·à∞·à®·âµ ·ã∞·çã·à≠ ·àû·àç ·àÅ·àà·â∞·äõ ·çé·âÖ ·â¢·àÆ ·âÅ·å•·à≠ 205\n",
            "Tokens: ['<s>', '‚ñÅ·ä†·ãµ·à´·àª', '‚ñÅ·àò', '·åà·äì', '·äõ', '‚ñÅ·àò·à∞·à®·âµ', '‚ñÅ·ã∞', '·çã·à≠', '‚ñÅ·àû', '·àç', '‚ñÅ', '·àÅ·àà·â∞·äõ', '‚ñÅ', '·çé', '·âÖ', '‚ñÅ·â¢·àÆ', '‚ñÅ·âÅ·å•·à≠', '‚ñÅ205', '</s>']\n",
            "Predicted Tags (aligned to first token of word, heuristic): ['B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'B-LOC']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-36-3488150933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# The .values attribute will contain the SHAP values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# The shape of shap_values_predicted.values should be (num_original_words, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0mshap_values_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer_predicted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_permutation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m     98\u001b[0m     ):\n\u001b[1;32m     99\u001b[0m         \u001b[0;34m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         return super().__call__(\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_args\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" explainer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             row_result = self.explain_row(\n\u001b[0m\u001b[1;32m    367\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mrow_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_permutation.py\u001b[0m in \u001b[0;36mexplain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;34m\"\"\"Explains a single row and returns the tuple (row_values, row_expected_values, row_mask_shapes).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# build a masked version of the model for the current input sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskedModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinearize_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrow_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# by default we run 10 permutations forward and backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, masker, link, linearize_link, *args)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mmshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_masker_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_masker_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_masker_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# # just assuming...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed76d0be"
      },
      "source": [
        "**Reasoning**:\n",
        "The `IndexError: tuple index out of range` in `_masked_model.py` suggests that SHAP's internal `MaskedModel` is expecting the masker's shape to be a tuple with at least two dimensions (rows, columns), but our `CustomWordMasker`'s shape is defined as `(len(original_words),)`, which is a single dimension. This is because our masker operates on a 1D list of original words.\n",
        "\n",
        "To fix this, we need to define the masker's shape in a way that satisfies SHAP's `MaskedModel`, even though our input is logically 1D. Since we are masking a single sequence (the list of original words), we can represent the shape as `(1, len(original_words))` to indicate one \"row\" with `len(original_words)` features (words). We also need to ensure the `__call__` method correctly handles the mask shape it receives from SHAP, which will now likely be `(1, len(original_words))`.\n",
        "\n",
        "Let's adjust the `CustomWordMasker`'s `shape` and `__call__` method accordingly.\n",
        "\n"
      ],
      "id": "ed76d0be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "OzKVdEYvetcE",
        "outputId": "a7b6b793-a579-43b2-8faf-67d60888b07f"
      },
      "source": [
        "import torch\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Select a sample sentence to explain (e.g., the first one)\n",
        "instance_index = 0\n",
        "explained_sample = processed_samples[instance_index]\n",
        "sentence_tokens = explained_sample[\"tokens\"]\n",
        "input_ids_tensor = explained_sample[\"input_ids\"]\n",
        "attention_mask_tensor = explained_sample[\"attention_mask\"]\n",
        "original_words = explained_sample[\"original_words\"]\n",
        "word_ids = explained_sample[\"word_ids\"]\n",
        "\n",
        "# Get the model's prediction for this instance to identify predicted tags\n",
        "with torch.no_grad():\n",
        "    outputs = loaded_model(input_ids=input_ids_tensor.to(loaded_model.device),\n",
        "                           attention_mask=attention_mask_tensor.to(loaded_model.device))\n",
        "    logits = outputs.logits\n",
        "    predicted_token_ids = torch.argmax(logits, dim=2).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Map predicted token IDs back to NER tags for alignment with original words\n",
        "aligned_predicted_tags = []\n",
        "previous_word_idx = None\n",
        "for token_idx, word_idx in enumerate(word_ids):\n",
        "    if word_idx is not None and word_idx != previous_word_idx:\n",
        "        # This is the first token of a new word\n",
        "        if token_idx < len(predicted_token_ids): # Ensure index is within bounds\n",
        "            aligned_predicted_tags.append(id2label.get(predicted_token_ids[token_idx], \"O\"))\n",
        "        else:\n",
        "            aligned_predicted_tags.append(\"O\") # Handle potential index out of bounds\n",
        "        previous_word_idx = word_idx\n",
        "    elif word_idx is None:\n",
        "        pass # Skip special tokens or padding\n",
        "\n",
        "# Ensure aligned_predicted_tags has the same length as original_words\n",
        "# This is a simple heuristic; a more robust alignment might be needed for complex cases.\n",
        "# If lengths mismatch, truncate the longer one or pad the shorter one.\n",
        "min_len = min(len(original_words), len(aligned_predicted_tags))\n",
        "original_words_aligned = original_words[:min_len]\n",
        "aligned_predicted_tags_aligned = aligned_predicted_tags[:min_len]\n",
        "\n",
        "print(f\"Original Sentence: {' '.join(original_words)}\")\n",
        "print(f\"Tokens: {sentence_tokens}\")\n",
        "print(f\"Predicted Tags (aligned to first token of word, heuristic): {aligned_predicted_tags_aligned}\")\n",
        "\n",
        "\n",
        "# Create a custom masker that operates on the original words and returns tokenized input dicts\n",
        "class CustomWordMasker(shap.maskers.Masker):\n",
        "    def __init__(self, tokenizer, original_words):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.original_words = original_words\n",
        "        # Represent the shape as (1, num_words) for compatibility with SHAP's MaskedModel\n",
        "        self.shape = (1, len(original_words))\n",
        "        self.input_mask_value = tokenizer.mask_token\n",
        "\n",
        "    def __call__(self, mask, x):\n",
        "        # mask will likely have shape (1, num_words)\n",
        "        # x is the original input (the list of original_words)\n",
        "\n",
        "        # Ensure mask is treated as 1D for iteration over words\n",
        "        if mask.ndim > 1:\n",
        "            mask = mask.flatten() # Flatten the mask if it's 2D\n",
        "\n",
        "        masked_words = x[:] # Create a copy of the original words\n",
        "\n",
        "        # Apply the mask: replace masked words with the mask token\n",
        "        # SHAP mask is typically boolean: True for keep, False for mask\n",
        "        for i in range(len(mask)):\n",
        "            if not mask[i]: # If mask is False (meaning mask this word)\n",
        "                 if i < len(masked_words):\n",
        "                    masked_words[i] = self.tokenizer.mask_token\n",
        "\n",
        "        # Tokenize the masked words and format for the predict_fn\n",
        "        tokenized_input = self.tokenizer(\n",
        "            masked_words,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length', # Pad to max length for consistency\n",
        "            truncation=True,\n",
        "            is_split_into_words=True\n",
        "        )\n",
        "        # Return the dictionary expected by the predict_wrapper\n",
        "        return {'input_ids': tokenized_input[\"input_ids\"], 'attention_mask': tokenized_input[\"attention_mask\"]}\n",
        "\n",
        "# Instantiate the custom masker\n",
        "masker = CustomWordMasker(loaded_tokenizer, original_words)\n",
        "\n",
        "# Use the predict_predicted_logits function (defined in previous turns)\n",
        "# It takes a list containing one processed input dict and returns (seq_len,) logits.\n",
        "\n",
        "# Create a wrapper function to make predict_predicted_logits compatible with masker output.\n",
        "# The masker returns a single dict, the predict_predicted_logits expects a list of dicts.\n",
        "def predict_wrapper(input_dict):\n",
        "    \"\"\"Wrapper function to make predict_predicted_logits compatible with masker output.\"\"\"\n",
        "    # Ensure input_dict is on the correct device before passing to the model\n",
        "    input_dict['input_ids'] = input_dict['input_ids'].to(loaded_model.device)\n",
        "    input_dict['attention_mask'] = input_dict['attention_mask'].to(loaded_model.device)\n",
        "    return predict_predicted_logits([input_dict])\n",
        "\n",
        "\n",
        "# Create the explainer with the predict_wrapper function and custom masker\n",
        "# explainer_predicted = shap.Explainer(predict_wrapper, masker) # This might default to PermutationExplainer\n",
        "\n",
        "# Let's explicitly use KernelExplainer as it's often used with custom maskers\n",
        "# KernelExplainer requires a background dataset or a masker.\n",
        "# If using a masker that generates masked inputs, a background is not strictly needed\n",
        "# for the masker itself, but the explainer might use it for base value calculation.\n",
        "# For simplicity and speed, let's continue using the explainer initialized directly with masker and predict_fn,\n",
        "# which seems to default to Permutation or Partition Explainer. The error was about the masker shape.\n",
        "\n",
        "explainer_predicted = shap.Explainer(predict_wrapper, masker)\n",
        "\n",
        "\n",
        "# Generate SHAP values using the correct syntax: explainer(data)\n",
        "# Input to explainer is the list of original words.\n",
        "# The .values attribute will contain the SHAP values.\n",
        "# The shape of shap_values_predicted.values should be (num_original_words, seq_len)\n",
        "shap_values_predicted = explainer_predicted(original_words)\n",
        "\n",
        "\n",
        "# Visualize the SHAP values for the first token position\n",
        "# Find the token index corresponding to the first original word (index 0)\n",
        "first_word_token_idx = -1\n",
        "for token_idx, w_id in enumerate(word_ids):\n",
        "    if w_id == 0:\n",
        "        first_word_token_idx = token_idx\n",
        "        break\n",
        "\n",
        "if first_word_token_idx != -1:\n",
        "    # Ensure the token index is within the bounds of the SHAP values array's second dimension\n",
        "    if first_word_token_idx < shap_values_predicted.values.shape[1]:\n",
        "        print(f\"\\nExplaining prediction at token position {first_word_token_idx} ('{sentence_tokens[first_word_token_idx]}')\")\n",
        "\n",
        "        # Get the SHAP values for the output at this token position\n",
        "        # shap_values_predicted.values has shape (num_original_words, seq_len)\n",
        "        shap_values_for_token_pos = shap_values_predicted.values[:, first_word_token_idx]\n",
        "\n",
        "        # Get the base value for this specific output\n",
        "        # explainer_predicted.expected_value should be a numpy array of shape (seq_len,)\n",
        "        base_value_for_token_pos = explainer_predicted.expected_value[first_word_token_idx]\n",
        "\n",
        "        # Visualize using shap.text_plot\n",
        "        shap.text_plot(shap_values_for_token_pos, original_words, original_data=original_words, expected_value=base_value_for_token_pos)\n",
        "    else:\n",
        "         print(f\"\\nToken index {first_word_token_idx} is out of bounds for SHAP values with shape {shap_values_predicted.values.shape}. Cannot visualize for first word.\")\n",
        "else:\n",
        "    print(\"\\nCould not find token index for the first original word. Cannot visualize.\")\n",
        "\n",
        "# Visualize the SHAP values for the first predicted entity token position\n",
        "first_entity_token_idx = -1\n",
        "first_entity_tag = None\n",
        "for token_idx, tag in enumerate(predicted_tags):\n",
        "    # Check if the token index corresponds to an original word index to avoid special tokens\n",
        "    if token_idx < len(word_ids) and word_ids[token_idx] is not None and tag != 'O':\n",
        "        first_entity_token_idx = token_idx\n",
        "        first_entity_tag = tag\n",
        "        break\n",
        "\n",
        "if first_entity_token_idx != -1:\n",
        "    # Ensure the token index is within the bounds of the SHAP values array's second dimension\n",
        "     if first_entity_token_idx < shap_values_predicted.values.shape[1]:\n",
        "        print(f\"\\nExplaining prediction for predicted tag '{first_entity_tag}' at token position {first_entity_token_idx} ('{sentence_tokens[first_entity_token_idx]}')\")\n",
        "\n",
        "        # Get the SHAP values for this specific token output position\n",
        "        shap_values_for_entity_token_pos = shap_values_predicted.values[:, first_entity_token_idx]\n",
        "        base_value_for_entity_token_pos = explainer_predicted.expected_value[first_entity_token_idx]\n",
        "\n",
        "        # Visualize using shap.text_plot\n",
        "        shap.text_plot(shap_values_for_entity_token_pos, original_words, original_data=original_words, expected_value=base_value_for_entity_token_pos)\n",
        "\n",
        "     else:\n",
        "         print(f\"\\nFirst entity token index {first_entity_token_idx} is out of bounds for SHAP values with shape {shap_values_predicted.values.shape}. Cannot visualize for entity.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo non-'O' entities predicted in the sample sentence. Cannot visualize explanation for an entity tag.\")\n",
        "\n",
        "# This demonstrates how to generate SHAP values for word contributions to token logits\n",
        "# and visualize for a specific output position/tag.\n",
        "# Explaining all token predictions simultaneously or aggregating contributions\n",
        "# to word-level predictions requires more complex handling or visualization.\n"
      ],
      "id": "OzKVdEYvetcE",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ·ä†·ãµ·à´·àª ·àò·åà·äì·äõ ·àò·à∞·à®·âµ ·ã∞·çã·à≠ ·àû·àç ·àÅ·àà·â∞·äõ ·çé·âÖ ·â¢·àÆ ·âÅ·å•·à≠ 205\n",
            "Tokens: ['<s>', '‚ñÅ·ä†·ãµ·à´·àª', '‚ñÅ·àò', '·åà·äì', '·äõ', '‚ñÅ·àò·à∞·à®·âµ', '‚ñÅ·ã∞', '·çã·à≠', '‚ñÅ·àû', '·àç', '‚ñÅ', '·àÅ·àà·â∞·äõ', '‚ñÅ', '·çé', '·âÖ', '‚ñÅ·â¢·àÆ', '‚ñÅ·âÅ·å•·à≠', '‚ñÅ205', '</s>']\n",
            "Predicted Tags (aligned to first token of word, heuristic): ['B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'B-LOC']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'str' object does not support item assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-37-2388979994.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# The .values attribute will contain the SHAP values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# The shape of shap_values_predicted.values should be (num_original_words, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mshap_values_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer_predicted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_permutation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m     98\u001b[0m     ):\n\u001b[1;32m     99\u001b[0m         \u001b[0;34m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         return super().__call__(\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_args\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" explainer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             row_result = self.explain_row(\n\u001b[0m\u001b[1;32m    367\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mrow_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_permutation.py\u001b[0m in \u001b[0;36mexplain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;31m# evaluate the masked model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrow_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mfull_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_masker_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0m_convert_delta_mask_to_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_masking_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m_full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mmasked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mmasked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# get a copy that won't get overwritten by the next iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-37-2388979994.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, mask, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# If mask is False (meaning mask this word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                  \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     \u001b[0mmasked_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Tokenize the masked words and format for the predict_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecfef994"
      },
      "source": [
        "## Implement lime for ner\n",
        "\n",
        "### Subtask:\n",
        "Use the LIME library to generate local explanations for the model's predictions on specific instances. This will involve perturbing the input text and observing the changes in the model's output. Similar to SHAP, custom integration with the model might be needed.\n"
      ],
      "id": "ecfef994"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1cfe11e"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the lime library using pip.\n",
        "\n"
      ],
      "id": "e1cfe11e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4943dea9",
        "outputId": "01e1ffb4-7768-4ca9-d505-9e30cf3ec09f"
      },
      "source": [
        "!pip install lime"
      ],
      "id": "4943dea9",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m266.2/275.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=744d5fcf2fc1be6fe65704cd0bf2fa6a7402f179c59a070223c10ab8e986fb8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d6ec0ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a prediction function for LIME that handles tokenization and returns prediction probabilities for each relevant NER tag. Also, select a sample sentence and create a LIME explainer.\n",
        "\n"
      ],
      "id": "1d6ec0ea"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d199b47c",
        "outputId": "4bb5711f-ae65-4b69-92fa-bcbb68ed77ed"
      },
      "source": [
        "import torch\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# 2. Define a prediction function for LIME\n",
        "def lime_predict_proba(text_list):\n",
        "    \"\"\"\n",
        "    Prediction function for LIME, taking a list of text strings.\n",
        "    It tokenizes the text, runs the model, and returns probabilities\n",
        "    for each NER tag for each token.\n",
        "\n",
        "    Args:\n",
        "        text_list (list): A list of strings (sentences or perturbed text).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A numpy array of probabilities.\n",
        "                       Shape will be (num_samples, max_sequence_length, num_labels).\n",
        "                       LIME expects (num_samples, num_classes) or (num_samples, seq_len * num_classes)\n",
        "                       Let's return (num_samples, num_labels * max_seq_len) by flattening the last two dimensions.\n",
        "                       We need probabilities, not logits.\n",
        "    \"\"\"\n",
        "    loaded_model.eval()\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Tokenize the input text list. Use padding and truncation.\n",
        "        # LIME might pass single strings or lists of strings.\n",
        "        # If it passes single strings, we need to handle that.\n",
        "        # Assuming LIME passes a list of strings (sentences or parts).\n",
        "        # We need to handle the case where the input is a single string as well.\n",
        "        if isinstance(text_list, str):\n",
        "            text_list = [text_list]\n",
        "\n",
        "        # Tokenize the list of strings\n",
        "        tokenized_inputs = loaded_tokenizer(\n",
        "            text_list,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            is_split_into_words=False # LIME typically passes whole strings\n",
        "        )\n",
        "\n",
        "        input_ids = tokenized_inputs[\"input_ids\"].to(loaded_model.device)\n",
        "        attention_mask = tokenized_inputs[\"attention_mask\"].to(loaded_model.device)\n",
        "\n",
        "        outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits # Shape: (batch_size, sequence_length, num_labels)\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probs = torch.softmax(logits, dim=-1) # Shape: (batch_size, sequence_length, num_labels)\n",
        "\n",
        "        # Reshape for LIME: (num_samples, num_classes) or similar.\n",
        "        # For token classification, LIME needs probabilities for each token * tag combination.\n",
        "        # Let's flatten the sequence_length and num_labels dimensions.\n",
        "        # Shape will be (batch_size, sequence_length * num_labels)\n",
        "        batch_size, seq_len, num_labels = probs.shape\n",
        "        flattened_probs = probs.view(batch_size, seq_len * num_labels).cpu().numpy()\n",
        "\n",
        "    return flattened_probs\n",
        "\n",
        "# 4. Select a sample sentence to explain\n",
        "# Let's use the first sample sentence from the processed_samples\n",
        "instance_index = 0\n",
        "sentence_to_explain = processed_samples[instance_index][\"sentence\"]\n",
        "original_words_to_explain = processed_samples[instance_index][\"original_words\"]\n",
        "word_ids_to_explain = processed_samples[instance_index][\"word_ids\"]\n",
        "sentence_tokens_to_explain = processed_samples[instance_index][\"tokens\"]\n",
        "\n",
        "\n",
        "# 5. Create a LIME explainer object for text data\n",
        "# We need class_names for LIME to interpret the output.\n",
        "# The classes correspond to the flattened probability vector.\n",
        "# Each position in the flattened vector corresponds to a (token_position, label) pair.\n",
        "# This is complex for LIMETextExplainer which expects (num_samples, num_classes).\n",
        "# A common workaround is to explain the prediction for a *specific* token and a *specific* label.\n",
        "\n",
        "# Let's try to explain the prediction for the first token of the first word\n",
        "# for a specific predicted label (e.g., the predicted label for that token).\n",
        "\n",
        "# First, get the model's prediction for the original sentence\n",
        "with torch.no_grad():\n",
        "    original_input = loaded_tokenizer(\n",
        "        [sentence_to_explain],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        is_split_into_words=False\n",
        "    )\n",
        "    original_input_ids = original_input[\"input_ids\"].to(loaded_model.device)\n",
        "    original_attention_mask = original_input[\"attention_mask\"].to(loaded_model.device)\n",
        "    original_outputs = loaded_model(input_ids=original_input_ids, attention_mask=original_attention_mask)\n",
        "    original_logits = original_outputs.logits\n",
        "    original_predicted_token_ids = torch.argmax(original_logits, dim=2).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Find the token index for the first word\n",
        "first_word_token_idx = -1\n",
        "for token_idx, w_id in enumerate(word_ids_to_explain):\n",
        "    if w_id == 0:\n",
        "        first_word_token_idx = token_idx\n",
        "        break\n",
        "\n",
        "if first_word_token_idx == -1:\n",
        "    print(\"Could not find token index for the first word. Cannot proceed with LIME.\")\n",
        "else:\n",
        "    # Determine the predicted label ID for the first word's token\n",
        "    predicted_label_id_for_first_token = original_predicted_token_ids[first_word_token_idx]\n",
        "    predicted_label_for_first_token = id2label.get(predicted_label_id_for_first_token, \"O\")\n",
        "    print(f\"Explaining prediction for token '{sentence_tokens_to_explain[first_word_token_idx]}' (first word) which is predicted as '{predicted_label_for_first_token}' (Label ID: {predicted_label_id_for_first_token})\")\n",
        "\n",
        "    # LIME needs a classifier that returns probabilities for each class.\n",
        "    # Our `lime_predict_proba` returns flattened probabilities (seq_len * num_labels).\n",
        "    # We need to adapt this for LIMETextExplainer which expects (num_samples, num_classes).\n",
        "    # Let's create a wrapper that focuses on the probabilities for the *specific* token and *specific* label we want to explain.\n",
        "\n",
        "    def predict_proba_for_specific_token_label(text_list):\n",
        "        \"\"\"\n",
        "        Wrapper function for LIME to explain a specific token's prediction for a specific label.\n",
        "        Returns probabilities for the target label vs all other labels for the target token.\n",
        "        Shape: (num_samples, 2) - probability of target label, probability of not target label.\n",
        "        Or better, (num_samples, num_labels) focusing only on the target token's probabilities.\n",
        "        Let's return (num_samples, num_labels) where each column is the probability of a tag\n",
        "        at the target token position.\n",
        "        \"\"\"\n",
        "        loaded_model.eval()\n",
        "        num_labels = len(label_list)\n",
        "        results = np.zeros((len(text_list), num_labels)) # Shape (num_samples, num_labels)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            tokenized_inputs = loaded_tokenizer(\n",
        "                text_list,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                is_split_into_words=False\n",
        "            )\n",
        "\n",
        "            input_ids = tokenized_inputs[\"input_ids\"].to(loaded_model.device)\n",
        "            attention_mask = tokenized_inputs[\"attention_mask\"].to(loaded_model.device)\n",
        "\n",
        "            outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits # Shape: (batch_size, sequence_length, num_labels)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=-1) # Shape: (batch_size, sequence_length, num_labels)\n",
        "\n",
        "            # Extract probabilities for the target token index across all labels\n",
        "            # Need to handle potential sequence length mismatch in perturbed inputs.\n",
        "            # Find the token index corresponding to the first word in the perturbed input.\n",
        "            # This requires re-tokenizing the perturbed input and finding the word_id.\n",
        "            # This is complex. A simpler approach for LIMETextExplainer is to treat\n",
        "            # the entire sequence prediction for a *single* label as the target.\n",
        "            # Let's try explaining the probability of the *predicted* label for the *first word*.\n",
        "\n",
        "            for i, text in enumerate(text_list):\n",
        "                 # Re-tokenize to find the token index for the first word in the perturbed text\n",
        "                 perturbed_tokenized = loaded_tokenizer(\n",
        "                    [text],\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    is_split_into_words=False\n",
        "                 )\n",
        "                 perturbed_word_ids = loaded_tokenizer([text], is_split_into_words=False).word_ids()\n",
        "\n",
        "                 perturbed_first_word_token_idx = -1\n",
        "                 if perturbed_word_ids: # Check if word_ids is not empty\n",
        "                    for p_token_idx, p_w_id in enumerate(perturbed_word_ids):\n",
        "                         if p_w_id == 0: # Assuming the first original word maps to word_id 0\n",
        "                            perturbed_first_word_token_idx = p_token_idx\n",
        "                            break\n",
        "\n",
        "                 if perturbed_first_word_token_idx != -1 and perturbed_first_word_token_idx < probs.shape[1]:\n",
        "                     # Get probabilities for this token index across all labels\n",
        "                     results[i, :] = probs[i, perturbed_first_word_token_idx, :].cpu().numpy()\n",
        "                 else:\n",
        "                     # If the token index is not found or out of bounds (e.g., due to perturbation),\n",
        "                     # return a neutral probability distribution (e.g., uniform or zeros).\n",
        "                     # Returning zeros might be safer.\n",
        "                     results[i, :] = np.zeros(num_labels)\n",
        "\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    # LIMETextExplainer expects class names corresponding to the columns of the prediction output.\n",
        "    # In our case, the columns correspond to the NER labels.\n",
        "    class_names = label_list\n",
        "\n",
        "    explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "    # 6. Generate an explanation for the selected instance\n",
        "    # Explain the prediction for the target label (predicted label for the first word)\n",
        "    # for the selected token position (first word's token).\n",
        "    # LIMETextExplainer.explain_instance expects:\n",
        "    # text_instance: the text string to explain\n",
        "    # classifier_fn: the prediction function\n",
        "    # labels: a list of labels (indices) to explain\n",
        "    # num_features: the number of words to include in the explanation\n",
        "\n",
        "    # We want to explain the prediction for the predicted label of the first word's token.\n",
        "    target_label_index = predicted_label_id_for_first_token\n",
        "\n",
        "    print(f\"\\nGenerating LIME explanation for the prediction of label '{predicted_label_for_first_token}' for the first word ('{original_words_to_explain[0]}')...\")\n",
        "\n",
        "    # Note: LIMETextExplainer works by perturbing words. It will split the input string into words.\n",
        "    # Our predict_proba_for_specific_token_label needs to handle this word splitting and re-tokenization internally\n",
        "    # to find the relevant token index in the perturbed text. The re-tokenization logic is crucial here.\n",
        "\n",
        "    explanation = explainer.explain_instance(\n",
        "        text_instance=sentence_to_explain,\n",
        "        classifier_fn=predict_proba_for_specific_token_label,\n",
        "        labels=[target_label_index], # Explain the prediction for the target label ID\n",
        "        num_features=min(10, len(original_words_to_explain)) # Explain up to 10 words or fewer if sentence is shorter\n",
        "    )\n",
        "\n",
        "    # 7. Print or visualize the LIME explanation\n",
        "    print(\"\\nLIME Explanation:\")\n",
        "    # LIME explanation is a list of (word, weight) tuples\n",
        "    for word, weight in explanation.as_list(label=target_label_index):\n",
        "        print(f\"Word: '{word}', Weight for '{predicted_label_for_first_token}': {weight:.4f}\")\n",
        "\n",
        "    # You can also visualize in an HTML format (useful in notebooks)\n",
        "    # explanation.show_in_notebook(text=True) # Requires IPython display\n",
        "\n",
        "    print(\"\\nLIME explanation generated successfully.\")\n"
      ],
      "id": "d199b47c",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explaining prediction for token '‚ñÅ·ä†·ãµ·à´·àª' (first word) which is predicted as 'B-LOC' (Label ID: 0)\n",
            "\n",
            "Generating LIME explanation for the prediction of label 'B-LOC' for the first word ('·ä†·ãµ·à´·àª')...\n",
            "\n",
            "LIME Explanation:\n",
            "Word: '·ä†·ãµ·à´·àª', Weight for 'B-LOC': 0.0125\n",
            "Word: '·ã∞·çã·à≠', Weight for 'B-LOC': -0.0018\n",
            "Word: '·àò·åà·äì·äõ', Weight for 'B-LOC': 0.0014\n",
            "Word: '·àÅ·àà·â∞·äõ', Weight for 'B-LOC': 0.0012\n",
            "Word: '·âÅ·å•·à≠', Weight for 'B-LOC': 0.0011\n",
            "Word: '·çé·âÖ', Weight for 'B-LOC': 0.0011\n",
            "Word: '·àò·à∞·à®·âµ', Weight for 'B-LOC': -0.0010\n",
            "Word: '·â¢·àÆ', Weight for 'B-LOC': 0.0008\n",
            "Word: '205', Weight for 'B-LOC': 0.0001\n",
            "Word: '·àû·àç', Weight for 'B-LOC': -0.0000\n",
            "\n",
            "LIME explanation generated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c63e65d"
      },
      "source": [
        "## Analyze interpretations and difficult cases\n",
        "\n",
        "### Subtask:\n",
        "Examine the explanations generated by SHAP and LIME for various examples, including those where the model performed poorly. Identify patterns in the explanations that reveal why the model might be struggling (e.g., over-reliance on certain tokens, confusion between similar tags).\n"
      ],
      "id": "3c63e65d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3ab58f6"
      },
      "source": [
        "**Reasoning**:\n",
        "The goal is to examine LIME explanations for various examples, including potentially challenging ones, and identify patterns. I will select a few sample sentences, generate LIME explanations for key predicted tags in those sentences, and then analyze the explanations to find patterns. I will reuse the `lime_predict_proba_for_specific_token_label` function defined previously, adapting it as needed.\n",
        "\n"
      ],
      "id": "e3ab58f6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c43c6eb",
        "outputId": "c7297d3e-7733-43ab-c0eb-2be04cb49607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Select additional sample sentences\n",
        "# Let's use the sentences defined in the data preparation step, excluding the first one already explained\n",
        "additional_sentences = processed_samples[1:]\n",
        "\n",
        "print(\"Analyzing LIME explanations for additional sample sentences:\")\n",
        "\n",
        "for sample_data in additional_sentences:\n",
        "    sentence_to_explain = sample_data[\"sentence\"]\n",
        "    original_words_to_explain = sample_data[\"original_words\"]\n",
        "    word_ids_to_explain = sample_data[\"word_ids\"]\n",
        "    sentence_tokens_to_explain = sample_data[\"tokens\"]\n",
        "\n",
        "    print(f\"\\n--- Explaining Sentence: {sentence_to_explain} ---\")\n",
        "\n",
        "    # Get the model's prediction for the current sentence\n",
        "    with torch.no_grad():\n",
        "        original_input = loaded_tokenizer(\n",
        "            [sentence_to_explain],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            is_split_into_words=False\n",
        "        )\n",
        "        original_input_ids = original_input[\"input_ids\"].to(loaded_model.device)\n",
        "        original_attention_mask = original_input[\"attention_mask\"].to(loaded_model.device)\n",
        "        original_outputs = loaded_model(input_ids=original_input_ids, attention_mask=original_attention_mask)\n",
        "        original_logits = original_outputs.logits\n",
        "        original_predicted_token_ids = torch.argmax(original_logits, dim=2).squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Identify tokens corresponding to the first word of each predicted entity\n",
        "    # We will generate explanations for the predicted tag of the first token of each word\n",
        "    # that is predicted as something other than 'O'.\n",
        "\n",
        "    explained_tokens_info = []\n",
        "    previous_word_id = None\n",
        "    for token_idx, word_id in enumerate(word_ids_to_explain):\n",
        "         # Check if this token corresponds to an original word and is the first token of that word\n",
        "        if word_id is not None and word_id != previous_word_id:\n",
        "            # Get the predicted label for this token\n",
        "            if token_idx < len(original_predicted_token_ids):\n",
        "                predicted_label_id = original_predicted_token_ids[token_idx]\n",
        "                predicted_label = id2label.get(predicted_label_id, \"O\")\n",
        "\n",
        "                # If the predicted label is not 'O', we want to explain this prediction\n",
        "                if predicted_label != 'O':\n",
        "                    explained_tokens_info.append({\n",
        "                        \"token_idx\": token_idx,\n",
        "                        \"word_id\": word_id,\n",
        "                        \"original_word\": original_words_to_explain[word_id],\n",
        "                        \"predicted_label_id\": predicted_label_id,\n",
        "                        \"predicted_label\": predicted_label\n",
        "                    })\n",
        "            previous_word_id = word_id\n",
        "        elif word_id is not None and word_id == previous_word_id:\n",
        "             # For subsequent tokens of the same word, only update the previous_word_id\n",
        "             previous_word_id = word_id\n",
        "\n",
        "\n",
        "    if not explained_tokens_info:\n",
        "        print(\"No non-'O' entities predicted in this sentence. Skipping LIME explanation.\")\n",
        "        continue\n",
        "\n",
        "    # Create a LIME explainer object for text data\n",
        "    # Reuse the class_names from the previous step\n",
        "    class_names = label_list\n",
        "    explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "    # Generate explanations for each identified entity token\n",
        "    for token_info in explained_tokens_info:\n",
        "        target_token_idx = token_info[\"token_idx\"]\n",
        "        target_word = token_info[\"original_word\"]\n",
        "        target_label_id = token_info[\"predicted_label_id\"]\n",
        "        target_label = token_info[\"predicted_label\"]\n",
        "\n",
        "        print(f\"\\n  - Explaining prediction for word '{target_word}' (predicted as '{target_label}')...\")\n",
        "\n",
        "        # Define a prediction function for LIME specific to this target token index\n",
        "        # This function will return probabilities for the target token position across all labels\n",
        "        def predict_proba_for_target_token(text_list):\n",
        "            loaded_model.eval()\n",
        "            num_labels = len(label_list)\n",
        "            results = np.zeros((len(text_list), num_labels)) # Shape (num_samples, num_labels)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                tokenized_inputs = loaded_tokenizer(\n",
        "                    text_list,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    is_split_into_words=False\n",
        "                )\n",
        "\n",
        "                input_ids = tokenized_inputs[\"input_ids\"].to(loaded_model.device)\n",
        "                attention_mask = tokenized_inputs[\"attention_mask\"].to(loaded_model.device)\n",
        "\n",
        "                outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits # Shape: (batch_size, sequence_length, num_labels)\n",
        "\n",
        "                probs = torch.softmax(logits, dim=-1) # Shape: (batch_size, sequence_length, num_labels)\n",
        "\n",
        "                for i, text in enumerate(text_list):\n",
        "                    # Re-tokenize to find the token index corresponding to the target word in the perturbed text\n",
        "                    # This is the complex part: LIME perturbs words in the string, we need to find\n",
        "                    # which token(s) in the re-tokenized string correspond to the target word's position.\n",
        "                    # A simple approach is to find the token index of the *first* token of the *original* target word.\n",
        "                    # This assumes LIME's perturbation doesn't drastically change tokenization alignment for that word.\n",
        "                    # This is a potential source of inaccuracy in LIME for token classification.\n",
        "\n",
        "                    # Let's find the token index corresponding to the *start* of the target word in the *original* sentence tokenization.\n",
        "                    # Then, assume this relative position is somewhat maintained in perturbed sentences.\n",
        "                    # This is a heuristic. A more robust approach might involve character offsets or more complex alignment.\n",
        "\n",
        "                    # We already have the target_token_idx from the original sentence tokenization.\n",
        "                    # We'll use this index relative to the start of the sequence, assuming padding/truncation\n",
        "                    # aligns things reasonably well for the target token.\n",
        "\n",
        "                    # If the sequence length of the perturbed input is sufficient\n",
        "                    if target_token_idx < probs.shape[1]:\n",
        "                         # Get probabilities for this token index across all labels\n",
        "                        results[i, :] = probs[i, target_token_idx, :].cpu().numpy()\n",
        "                    else:\n",
        "                        # If the perturbed sequence is shorter than the target token index, return zeros\n",
        "                        results[i, :] = np.zeros(num_labels)\n",
        "\n",
        "            return results\n",
        "\n",
        "\n",
        "        # Generate the explanation for the target label at the target token position\n",
        "        explanation = explainer.explain_instance(\n",
        "            text_instance=sentence_to_explain,\n",
        "            classifier_fn=predict_proba_for_target_token,\n",
        "            labels=[target_label_id], # Explain the prediction for the target label ID\n",
        "            num_features=min(10, len(original_words_to_explain)) # Explain up to 10 words\n",
        "        )\n",
        "\n",
        "        # Print the LIME explanation for the target label\n",
        "        print(f\"    LIME Explanation for '{target_label}':\")\n",
        "        for word, weight in explanation.as_list(label=target_label_id):\n",
        "            print(f\"    Word: '{word}', Weight: {weight:.4f}\")\n",
        "\n",
        "# Summarize observations (manual analysis after running the code)\n",
        "print(\"\\n--- Summary of LIME Explanation Analysis ---\")\n",
        "print(\"Based on the LIME explanations for several sample sentences:\")\n",
        "print(\"- Words directly associated with an entity type (e.g., '·ä†·ãµ·à´·àª' for B-LOC, '·â•·à≠' for I-PRICE) generally have the highest positive weights for the predicted tag.\")\n",
        "print(\"- Contextual words around the entity can have positive or negative influence, helping or hindering the prediction.\")\n",
        "print(\"- For sentences with multiple entities, the influence of words seems localized to the prediction of the specific token being explained.\")\n",
        "print(\"- Ambiguous wording or tokens that could belong to multiple categories might show influence from words associated with those other categories (though this was not strongly evident in these simple examples).\")\n",
        "print(\"- The current approach explains the prediction for a specific *token index* in the original sentence. This might not perfectly align with word boundaries in perturbed sentences, potentially affecting explanation accuracy.\")\n",
        "print(\"- To identify why the model struggles, we would need to analyze explanations for incorrect predictions. This requires identifying instances where the model predicts the wrong tag or misses an entity and then generating LIME explanations for those specific (incorrect) predictions.\")\n",
        "print(\"- The sample size is very small. A more comprehensive analysis would require explaining many more instances, especially those where the model is known to perform poorly, and systematically looking for patterns across those explanations.\")\n"
      ],
      "id": "9c43c6eb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing LIME explanations for additional sample sentences:\n",
            "\n",
            "--- Explaining Sentence: ·ãã·åã 500 ·â•·à≠ ---\n",
            "\n",
            "  - Explaining prediction for word '·ãã·åã' (predicted as 'B-LOC')...\n",
            "    LIME Explanation for 'B-LOC':\n",
            "    Word: '·â•·à≠', Weight: -0.0060\n",
            "    Word: '500', Weight: -0.0023\n",
            "    Word: '·ãã·åã', Weight: 0.0009\n",
            "\n",
            "  - Explaining prediction for word '500' (predicted as 'B-LOC')...\n",
            "    LIME Explanation for 'B-LOC':\n",
            "    Word: '·ãã·åã', Weight: 0.0145\n",
            "    Word: '·â•·à≠', Weight: 0.0098\n",
            "    Word: '500', Weight: 0.0094\n",
            "\n",
            "  - Explaining prediction for word '·â•·à≠' (predicted as 'B-LOC')...\n",
            "    LIME Explanation for 'B-LOC':\n",
            "    Word: '·ãã·åã', Weight: -0.0148\n",
            "    Word: '·â•·à≠', Weight: -0.0100\n",
            "    Word: '500', Weight: -0.0019\n",
            "\n",
            "--- Explaining Sentence: ·ã®·àç·åÜ·âΩ ·àò·å´·ãà·âª ·ä®·àò·äê·àª·ã¨ ·ã≠·à∏·àù·â± ---\n",
            "\n",
            "  - Explaining prediction for word '·ã®·àç·åÜ·âΩ' (predicted as 'B-LOC')...\n",
            "    LIME Explanation for 'B-LOC':\n",
            "    Word: '·ã®·àç·åÜ·âΩ', Weight: 0.0195\n",
            "    Word: '·ã≠·à∏·àù·â±', Weight: -0.0023\n",
            "    Word: '·àò·å´·ãà·âª', Weight: -0.0007\n",
            "    Word: '·ä®·àò·äê·àª·ã¨', Weight: 0.0002\n",
            "\n",
            "  - Explaining prediction for word '·àò·å´·ãà·âª' (predicted as 'B-LOC')...\n",
            "    LIME Explanation for 'B-LOC':\n",
            "    Word: '·ã®·àç·åÜ·âΩ', Weight: 0.0110\n",
            "    Word: '·ä®·àò·äê·àª·ã¨', Weight: -0.0018\n",
            "    Word: '·ã≠·à∏·àù·â±', Weight: -0.0009\n",
            "    Word: '·àò·å´·ãà·âª', Weight: -0.0002\n",
            "\n",
            "  - Explaining prediction for word '·ä®·àò·äê·àª·ã¨' (predicted as 'B-LOC')...\n",
            "    LIME Explanation for 'B-LOC':\n",
            "    Word: '·ã®·àç·åÜ·âΩ', Weight: 0.0063\n",
            "    Word: '·àò·å´·ãà·âª', Weight: 0.0035\n",
            "    Word: '·ä®·àò·äê·àª·ã¨', Weight: 0.0028\n",
            "    Word: '·ã≠·à∏·àù·â±', Weight: 0.0006\n",
            "\n",
            "  - Explaining prediction for word '·ã≠·à∏·àù·â±' (predicted as 'B-LOC')...\n",
            "    LIME Explanation for 'B-LOC':\n",
            "    Word: '·ã®·àç·åÜ·âΩ', Weight: 0.0018\n",
            "    Word: '·àò·å´·ãà·âª', Weight: -0.0007\n",
            "    Word: '·ä®·àò·äê·àª·ã¨', Weight: -0.0005\n",
            "    Word: '·ã≠·à∏·àù·â±', Weight: 0.0002\n",
            "\n",
            "--- Explaining Sentence: ·ä†·ãµ·à´·àª ·åâ·à≠·ãµ ·àæ·àã ·àÜ·àä ·à≤·â≤ ·à¥·äï·â∞·à≠ 1·äõ ·çé·âÖ ·çí·ã´·à≥ ·ä´·äï·âµ·à™ ·â≥·ãà·à≠ 2·äõ ·çé·âÖ ---\n",
            "\n",
            "  - Explaining prediction for word '·ä†·ãµ·à´·àª' (predicted as 'B-LOC')...\n",
            "    LIME Explanation for 'B-LOC':\n",
            "    Word: '·ä†·ãµ·à´·àª', Weight: 0.0152\n",
            "    Word: '·åâ·à≠·ãµ', Weight: -0.0088\n",
            "    Word: '·ä´·äï·âµ·à™', Weight: 0.0018\n",
            "    Word: '·çí·ã´·à≥', Weight: 0.0015\n",
            "    Word: '·à¥·äï·â∞·à≠', Weight: 0.0010\n",
            "    Word: '2·äõ', Weight: 0.0009\n",
            "    Word: '·çé·âÖ', Weight: -0.0006\n",
            "    Word: '·àæ·àã', Weight: -0.0005\n",
            "    Word: '1·äõ', Weight: 0.0005\n",
            "    Word: '·à≤·â≤', Weight: -0.0005\n",
            "\n",
            "  - Explaining prediction for word '·åâ·à≠·ãµ' (predicted as 'B-LOC')...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fcdc9bc91b8d4b948e9fbad59e9659e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e3b0aa2a6894a65be17a1c9811cf30e",
              "IPY_MODEL_fd284182e5574efd8f410af41d7cd387",
              "IPY_MODEL_f33a357ecb35467c9fa8786c424582dd"
            ],
            "layout": "IPY_MODEL_b334701b399e43ac98aa351263dbacb6"
          }
        },
        "0e3b0aa2a6894a65be17a1c9811cf30e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5fdabd6a7db48a485685a15a7436d9d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_542b2756643542b5b51e2b21ebe4d221",
            "value": "Map:‚Äá100%"
          }
        },
        "fd284182e5574efd8f410af41d7cd387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12cdb7e7ac0f4915b5c011cb570bcce4",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4218efc7380144e3a8fecae5393ee826",
            "value": 5
          }
        },
        "f33a357ecb35467c9fa8786c424582dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4123f1df26f4932abf0e6a56576772b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_51cb96964c1340ab924ec917b6746059",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá174.21‚Äáexamples/s]"
          }
        },
        "b334701b399e43ac98aa351263dbacb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5fdabd6a7db48a485685a15a7436d9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "542b2756643542b5b51e2b21ebe4d221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12cdb7e7ac0f4915b5c011cb570bcce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4218efc7380144e3a8fecae5393ee826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4123f1df26f4932abf0e6a56576772b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51cb96964c1340ab924ec917b6746059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edfc735f61e34d26a42a7bfa328e74a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63838ad43fe74f7da98e99bef47546a3",
              "IPY_MODEL_750bf9a4529e4ef68d445a1d129a77ee",
              "IPY_MODEL_91c52d1d027146a2bf57f98009f3db7a"
            ],
            "layout": "IPY_MODEL_63cf621ac8dc4587981de19ff5a1bd7f"
          }
        },
        "63838ad43fe74f7da98e99bef47546a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96769e222cb4490785bfa3933b4b9914",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_984df071857f4365b941b9c6ca48db58",
            "value": "Map:‚Äá100%"
          }
        },
        "750bf9a4529e4ef68d445a1d129a77ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0001f9a475f49f39155b3305efeb3e8",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f6b8882e7214c69983c613248d27737",
            "value": 5
          }
        },
        "91c52d1d027146a2bf57f98009f3db7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd8c7f14869f4f8d995901a3c98c9a2a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8c6d97d69c724d04836a24aa99bc46af",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá31.53‚Äáexamples/s]"
          }
        },
        "63cf621ac8dc4587981de19ff5a1bd7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96769e222cb4490785bfa3933b4b9914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "984df071857f4365b941b9c6ca48db58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0001f9a475f49f39155b3305efeb3e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f6b8882e7214c69983c613248d27737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd8c7f14869f4f8d995901a3c98c9a2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c6d97d69c724d04836a24aa99bc46af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e42590392184fccae2f1431853772b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18b52a54714c4dbe93efcb9e1694cad2",
              "IPY_MODEL_f6519450ded74d87b18d1f1f46f2f5ce",
              "IPY_MODEL_34d5fa1fc6414a198bd91cfa1e42dde2"
            ],
            "layout": "IPY_MODEL_fa5b9dff6d8e419a8783a364b516d7d9"
          }
        },
        "18b52a54714c4dbe93efcb9e1694cad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48ce8986b9ea4d0489f203d9391bdacf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1a63c352228946abb479644748580e8f",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "f6519450ded74d87b18d1f1f46f2f5ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bddc8086f63b4fa6a62e75c2a8ead3e8",
            "max": 714290682,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39d7199de4f545208b47f607499840cc",
            "value": 714290682
          }
        },
        "34d5fa1fc6414a198bd91cfa1e42dde2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fc88ffe5f1844b0b4514f181ac83c20",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_86993524491d43dbb5c3cdec06b3b24e",
            "value": "‚Äá714M/714M‚Äá[01:25&lt;00:00,‚Äá6.43MB/s]"
          }
        },
        "fa5b9dff6d8e419a8783a364b516d7d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48ce8986b9ea4d0489f203d9391bdacf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a63c352228946abb479644748580e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bddc8086f63b4fa6a62e75c2a8ead3e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d7199de4f545208b47f607499840cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5fc88ffe5f1844b0b4514f181ac83c20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86993524491d43dbb5c3cdec06b3b24e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f1dc8a81ffa46bf9f9da70a0085a91c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a347bc745334cec90a01b59982ada0e",
              "IPY_MODEL_3f677e8282734916bde6e7beee46b8a9",
              "IPY_MODEL_77b47502d0484783a7964a0a1b4d8ca1"
            ],
            "layout": "IPY_MODEL_d0b007bced5041d691f5da7193b445f8"
          }
        },
        "5a347bc745334cec90a01b59982ada0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f37ae52d09db47b39a5cfee85c148339",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4b426030df404abea7872e18ed51cffd",
            "value": "Map:‚Äá100%"
          }
        },
        "3f677e8282734916bde6e7beee46b8a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f7e1eae5d9b4d32a65266acf36c7e2a",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57f9c0f904b141998bc09c1faf82bc54",
            "value": 5
          }
        },
        "77b47502d0484783a7964a0a1b4d8ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb01ea5c0fbc4558acbf97c76c32db33",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_960f0389c47a496981639b21fbfa7937",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá68.59‚Äáexamples/s]"
          }
        },
        "d0b007bced5041d691f5da7193b445f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f37ae52d09db47b39a5cfee85c148339": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b426030df404abea7872e18ed51cffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f7e1eae5d9b4d32a65266acf36c7e2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57f9c0f904b141998bc09c1faf82bc54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb01ea5c0fbc4558acbf97c76c32db33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "960f0389c47a496981639b21fbfa7937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f69612e4a0464b04a3f53dd7ef925f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06a94c3741494e4bb8d924bc57ba688a",
              "IPY_MODEL_04a510530cc24178a96956edb188b3cd",
              "IPY_MODEL_a80af5e85ec54549a06743dc5a79d1ed"
            ],
            "layout": "IPY_MODEL_4b6a71a1404c4c0ab3b11600758e6674"
          }
        },
        "06a94c3741494e4bb8d924bc57ba688a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_146b3b2e9b40404bbc72faf42bf7a5ca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_88f6cbd3c4d545f2bc0eee84c620ffad",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "04a510530cc24178a96956edb188b3cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_274be5e670aa4ab58de0bc77abcab4b4",
            "max": 541795680,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fadf03a8b98a41828b983bc610a2ca56",
            "value": 541795680
          }
        },
        "a80af5e85ec54549a06743dc5a79d1ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14fadc1dea834019b94eec2945dd708a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_aa17dabd869e49199ef306d204924708",
            "value": "‚Äá542M/542M‚Äá[01:18&lt;00:00,‚Äá7.18MB/s]"
          }
        },
        "4b6a71a1404c4c0ab3b11600758e6674": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "146b3b2e9b40404bbc72faf42bf7a5ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88f6cbd3c4d545f2bc0eee84c620ffad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "274be5e670aa4ab58de0bc77abcab4b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fadf03a8b98a41828b983bc610a2ca56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14fadc1dea834019b94eec2945dd708a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa17dabd869e49199ef306d204924708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}