{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò Fine-Tune & Compare Multilingual Models for Amharic NER\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
    "                          TrainingArguments, Trainer)\n",
    "from sklearn.metrics import classification_report\n",
    "import evaluate\n",
    "\n",
    "# ‚úÖ Define models to compare\n",
    "model_names = [\n",
    "    \"xlm-roberta-base\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    \"distilbert-base-multilingual-cased\",\n",
    "    \"Davlan/afroxlmr-base\",\n",
    "    \"Davlan/bert-base-amharic\"\n",
    "]\n",
    "\n",
    "# ‚úÖ Load CoNLL-formatted dataset\n",
    "\n",
    "def load_conll_data(filepath):\n",
    "    sentences, labels = [], []\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        sentence, label = [], []\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 2:\n",
    "                    token, tag = parts\n",
    "                    sentence.append(token)\n",
    "                    label.append(tag)\n",
    "    if sentence:  # Append last sentence if file doesn't end with newline\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "sentences, tags = load_conll_data(\"amharic_ner_conll.txt\")\n",
    "\n",
    "label_list = sorted(list(set(tag for tag_seq in tags for tag in tag_seq)))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# ‚úÖ Tokenize and align labels\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, sentences, tags):\n",
    "    tokenized_inputs = tokenizer(sentences, is_split_into_words=True, truncation=True, padding=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return dict(tokenized_inputs)\n",
    "\n",
    "# ‚úÖ Convert to Hugging Face Dataset\n",
    "raw_dataset = Dataset.from_pandas(pd.DataFrame({\"tokens\": sentences, \"ner_tags\": tags}))\n",
    "\n",
    "# ‚úÖ Evaluation metric\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [[id2label[p] for (p, l) in zip(pred, lab) if l != -100] for pred, lab in zip(predictions, labels)]\n",
    "    true_labels = [[id2label[l] for (p, l) in zip(pred, lab) if l != -100] for pred, lab in zip(predictions, labels)]\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }\n",
    "\n",
    "# ‚úÖ Loop through models and train/evaluate\n",
    "results_summary = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n\\nüîÑ Fine-tuning model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenized_dataset = raw_dataset.map(lambda x: tokenize_and_align_labels(tokenizer, x[\"tokens\"], x[\"ner_tags\"]), batched=True)\n",
    "\n",
    "    if len(tokenized_dataset) == 0:\n",
    "        print(f\"‚ö†Ô∏è Skipping {model_name} ‚Äî dataset mapping returned 0 samples.\")\n",
    "        continue\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label_list),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{model_name.split('/')[-1]}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        eval_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    metrics = trainer.evaluate()\n",
    "    metrics[\"model\"] = model_name\n",
    "    metrics[\"train_time_sec\"] = training_time\n",
    "    results_summary.append(metrics)\n",
    "\n",
    "# ‚úÖ Results Summary Table\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "print(\"\\nüìä Model Comparison Summary:\")\n",
    "print(results_df.sort_values(\"f1\", ascending=False))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}